# -*- coding: utf-8 -*-
"""chapter05.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R_l5EKH9IX7A0pKAtje8MtTDmqcupYu9
"""

from huggingface_hub import notebook_login

notebook_login()

!pip install -U bitsandbytes

# Load model directly
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# https://huggingface.co/meta-llama/Meta-Llama-3-8B
# https://huggingface.co/meta-llama/Meta-Llama-3-8B/tree/main
# from_pretrainedの引数にモデル名を指定すると、モデルをダウンロードしてきてくれます。ダウンロードには3分ほどかかります。

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B")

#　メモリが足りない場合は, 量子化のために以下を有効にしてください
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=False,
)

model = AutoModelForCausalLM.from_pretrained(
            "meta-llama/Meta-Llama-3-8B",
            device_map="auto",
            quantization_config=bnb_config,
            torch_dtype=torch.bfloat16,
        )

# knock40
prompt = """
[問題]に対する[答え]を[選択肢]の中から選んでください。
[問題]:9世紀に活躍した人物に関係するできごとについて述べた次のア～ウを年代の古い順に正しく並べよ。
[選択肢]:[ア　藤原時平は，策謀を用いて菅原道真を政界から追放した。, イ　嵯峨天皇は，藤原冬嗣らを蔵人頭に任命した。, ウ　藤原良房は，承和の変後，藤原氏の中での北家の優位を確立した。]
[答え]:"""

for i in range(3):
  model_input = tokenizer(prompt, return_tensors="pt").to(model.device)
  input_ids = model_input["input_ids"]

  with torch.no_grad():
    result = model.generate(
                input_ids,
                max_new_tokens=300,
                # eos_token_id=terminators,
                do_sample=True,
                temperature=0.6,
                top_p=0.9,
            )
    result = result[0][input_ids.shape[-1]:]
    output = tokenizer.decode(result, skip_special_tokens=True)
    print("\n-----生成結果-----\n", output.split("[問題]")[0])

    del input_ids
    del model_input
    torch.cuda.empty_cache()

# knock41
prompt = """
[問題]に対する[答え]を[選択肢]の中から選んでください。
[問題]:日本の近代化に関連するできごとについて述べた次のア～ウを年代の古い順に正しく並べよ。
[選択肢]:[ア　府知事・県令からなる地方官会議が設置された。, イ　廃藩置県が実施され，中央から府知事・県令が派遣される体制になった。, ウ　すべての藩主が，天皇に領地と領民を返還した。]
[答え]:ウ→イ→ア
[問題]:江戸幕府の北方での対外的な緊張について述べた次の文ア～ウを年代の古い順に正しく並べよ。
[選択肢]:[ア　レザノフが長崎に来航したが，幕府が冷淡な対応をしたため，ロシア船が樺太や択捉島を攻撃した。, イ　ゴローウニンが国後島に上陸し，幕府の役人に捕らえられ抑留された。, ウ　ラクスマンが根室に来航し，漂流民を届けるとともに通商を求めた。]
[答え]:ウ→ア→イ
[問題]:中居屋重兵衛の生涯の期間におこったできごとについて述べた次のア～ウを，年代の古い順に正しく並べよ。
[選択肢]:[ア　アヘン戦争がおこり，清がイギリスに敗北した。, イ　異国船打払令が出され，外国船を撃退することが命じられた。, ウ　桜田門外の変がおこり，大老の井伊直弼が暗殺された。]
[答え]:イ→ア→ウ
[問題]:加藤高明が外務大臣として提言を行ってから、内閣総理大臣となり演説を行うまでの時期のできごとについて述べた次のア～ウを，年代の古い順に正しく並べよ。
[選択肢]:[ア　朝鮮半島において，独立を求める大衆運動である三・一独立運動が展開された。, イ　関東大震災後の混乱のなかで，朝鮮人や中国人に対する殺傷事件がおきた。, ウ　日本政府が，袁世凱政府に対して二十一カ条の要求を突き付けた。]
[答え]:ウ→ア→イ
[問題]:9世紀に活躍した人物に関係するできごとについて述べた次のア～ウを年代の古い順に正しく並べよ。
[選択肢]:[ア　藤原時平は，策謀を用いて菅原道真を政界から追放した。, イ　嵯峨天皇は，藤原冬嗣らを蔵人頭に任命した。, ウ　藤原良房は，承和の変後，藤原氏の中での北家の優位を確立した。]
[答え]:"""

for i in range(3):
  model_input = tokenizer(prompt, return_tensors="pt").to(model.device)
  input_ids = model_input["input_ids"]

  with torch.no_grad():
    result = model.generate(
                input_ids,
                max_new_tokens=300,
                # eos_token_id=terminators,
                do_sample=True,
                temperature=0.6,
                top_p=0.9,
            )
    result = result[0][input_ids.shape[-1]:]
    output = tokenizer.decode(result, skip_special_tokens=True)
    print("\n-----生成結果-----\n", output.split("[問題]")[0])

    del input_ids
    del model_input
    torch.cuda.empty_cache()

# knock42
import pandas as pd

# JMMLUの高校情報科学データを読み込む（CSVファイル）
df = pd.read_csv("jmmlu_high_school_computer_science.csv")

correct_count = 0

for _, row in df.iterrows():
    question = row["問題"]
    choices = [row["選択肢A"], row["選択肢B"], row["選択肢C"], row["選択肢D"]]
    correct_index = ["A", "B", "C", "D"].index(row["正解"])

    inputs = tokenizer([question] * len(choices), choices, return_tensors="pt", padding=True)

    with torch.no_grad():
        outputs = model(**inputs)

    predicted_index = torch.argmax(outputs.logits).item()

    if predicted_index == correct_index:
        correct_count += 1

accuracy = correct_count / len(df) * 100
print(f"高校情報科学の正解率: {accuracy:.2f}%")

# knock43
# JMMLUの高校情報科学データを読み込む
df = pd.read_csv("jmmlu_high_school_computer_science.csv")

# 実験: 正解をすべてDに統一
df["正解"] = "D"  # 本来の正解とは関係なく、Dを正解に設定

correct_count = 0

for _, row in df.iterrows():
    question = row["問題"]

    # 選択肢の順序を変更する（ランダムシャッフル）
    choices = [row["選択肢A"], row["選択肢B"], row["選択肢C"], row["選択肢D"]]
    random.shuffle(choices)

    # 温度パラメータを設定して推論（ランダム性を調整）
    inputs = tokenizer([question] * len(choices), choices, return_tensors="pt", padding=True)

    with torch.no_grad():
        outputs = model(**inputs)

    predicted_index = torch.argmax(outputs.logits).item()

    if predicted_index == choices.index(row["選択肢D"]):  # 変更後のDのインデックスと比較
        correct_count += 1

# 正解率を計算
accuracy = correct_count / len(df) * 100
print(f"実験結果（正解をDに統一）: 正解率 {accuracy:.2f}%")

# Load model directly
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct
# https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/tree/main
# from_pretrainedの引数にモデル名を指定すると、モデルをダウンロードしてきてくれます。ダウンロードには3分ほどかかります。

tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B-Instruct")

#　メモリが足りない場合は, 量子化のために以下を有効にしてください
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=False,
)

model = AutoModelForCausalLM.from_pretrained(
            "meta-llama/Meta-Llama-3-8B-Instruct",
            device_map="auto",
            quantization_config=bnb_config,
            torch_dtype=torch.bfloat16,
        )

# knock44
# 問題をプロンプトとして設計
prompt = """
つばめちゃんは渋谷駅から東急東横線に乗り、自由が丘駅で乗り換えました。
東急大井町線の大井町方面の急行に乗車してしまいました。
自由が丘の次の急行停車駅で降車し、反対方向の電車で一駅戻りました。
目的地の駅の名前を答えてください。
"""

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# 推論
with torch.no_grad():
    output = model.generate(
        inputs["input_ids"],
        max_new_tokens=50,  # 長すぎる回答を防ぐ
        do_sample=True,
        temperature=0.6  # 調整可能
    )

response = tokenizer.decode(output[0], skip_special_tokens=True)
print("推論結果:", response)

# kcnok45
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# モデルのロード
model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16
)

# プロンプトの設計
prompt = """
つばめちゃんは渋谷駅から東急東横線に乗り、自由が丘駅で乗り換えました。
しかし、間違えて大井町方面ではなく**逆方向の急行電車（溝の口方面）**に乗車してしまいました。
自由が丘の次の急行停車駅で降車し、反対方向の各駅停車に乗車しました。
目的地の駅に向かうためには、何駅先のどの駅で降りればよいでしょうか？
"""

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# 推論
with torch.no_grad():
    output = model.generate(
        inputs["input_ids"],
        max_new_tokens=50,  # 出力の長さを調整
        do_sample=True,
        temperature=0.3  # より確定的な回答を得るために低めに設定
    )

response = tokenizer.decode(output[0], skip_special_tokens=True)
print("推論結果:", response)

# knock46
# ランダムなお題の選択
topics = ["春の風", "猫の一日", "雨の日", "夏の夜", "電車通勤", "紅葉", "冬の朝", "海辺", "月夜", "読書"]
selected_topic = random.choice(topics)

# プロンプトの設計
prompt = f"""
お題: {selected_topic}
このお題に関する川柳を10個作成してください。
5-7-5の形式を守り、自然な日本語にしてください。
"""

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# 推論
with torch.no_grad():
    output = model.generate(
        inputs["input_ids"],
        max_new_tokens=100,  # 出力サイズを調整
        do_sample=True,
        temperature=0.6  # ランダム性を加える
    )

response = tokenizer.decode(output[0], skip_special_tokens=True)
print(f"お題: {selected_topic}\n川柳案:\n{response}")

# knock47
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# モデルのロード
model_name = "meta-llama/Meta-Llama-3-8B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.bfloat16
)

# 評価する川柳（前回の実行結果を入力）
senryu_list = [
    "春の風 ひらひら舞うは 花びらよ",
    "猫の影 そっと忍び寄る 夜の声",
    "雨の日や 傘の向こうに 見る世界",
    "夏の夜 星の瞬き 夢のあと",
    "電車揺れ 窓の景色と 眠る人",
    "紅葉舞う 一歩先には 秋の風",
    "冬の朝 白き息さえ 旅の友",
    "海辺には 波のささやき 砂の跡",
    "月夜には 影を追いかけ 夢の道",
    "読書する 時を忘れる 物語"
]

# 評価プロンプトの設計
prompt = "以下の川柳をユーモア・独創性・流れの美しさの観点から、10段階で評価してください。\n"

for i, senryu in enumerate(senryu_list, 1):
    prompt += f"{i}. {senryu}\n"

prompt += "\n各川柳に対し、10点満点の評価を出し、それぞれ短い理由を述べてください。"

# モデルに評価させる
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    output = model.generate(
        inputs["input_ids"],
        max_new_tokens=200,  # 回答の長さを調整
        do_sample=True,
        temperature=0.6  # ランダム性を適度に
    )

response = tokenizer.decode(output[0], skip_special_tokens=True)
print("評価結果:\n", response)

# knock48
import numpy as np

# 評価する川柳（前々回の実行結果を入力）
senryu_list = [
    "春の風 ひらひら舞うは 花びらよ",
    "猫の影 そっと忍び寄る 夜の声",
    "雨の日や 傘の向こうに 見る世界"
]

# 評価プロンプト
def get_prompt(senryu):
    return f"この川柳をユーモア・独創性・流れの美しさの観点から、10点満点で評価してください。\n川柳: {senryu}\n評価: "

# 複数回評価を実施
num_trials = 10
scores = []

for senryu in senryu_list:
    trial_scores = []
    for _ in range(num_trials):
        inputs = tokenizer(get_prompt(senryu), return_tensors="pt").to(model.device)
        with torch.no_grad():
            output = model.generate(inputs["input_ids"], max_new_tokens=5, do_sample=True, temperature=0.6)
        score = tokenizer.decode(output[0], skip_special_tokens=True).strip()
        trial_scores.append(float(score))  # 数値に変換
    scores.append(trial_scores)

# 分散の計算
for i, senryu_scores in enumerate(scores):
    variance = np.var(senryu_scores)
    print(f"川柳 {i+1} のスコア分散: {variance:.2f}")

# knock49
from transformers import AutoTokenizer

# トークナイザーのロード
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Meta-Llama-3-8B")

# 夏目漱石『吾輩は猫である』の冒頭部分
text = """吾輩は猫である。名前はまだ無い。

どこで生れたかとんと見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。しかもあとで聞くとそれは書生という人間中で一番獰悪な種族であったそうだ。この書生というのは時々我々を捕えて煮て食うという話である。しかしその当時は何という考もなかったから別段恐しいとも思わなかった。ただ彼の掌に載せられてスーと持ち上げられた時何だかフワフワした感じがあったばかりである。掌の上で少し落ちついて書生の顔を見たのがいわゆる人間というものの見始であろう。この時妙なものだと思った感じが今でも残っている。第一毛をもって装飾されべきはずの顔がつるつるしてまるで薬缶だ。その後猫にもだいぶ逢ったがこんな片輪には一度も出会わした事がない。のみならず顔の真中があまりに突起している。そうしてその穴の中から時々ぷうぷうと煙を吹く。どうも咽せぽくて実に弱った。これが人間の飲む煙草というものである事はようやくこの頃知った。"""

# トークン数を計測
tokens = tokenizer(text)
print(f"トークン数: {len(tokens['input_ids'])}")