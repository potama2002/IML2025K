{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tFLta_VkUcj",
        "outputId": "326fa504-3d78-4b49-8590-5381e98661f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQ7H3Lx2nLZS",
        "outputId": "ca1ff605-29df-4269-d81a-b45e18d09953"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘/content/drive/MyDrive/Colab Notebooks’: File exists\n",
            "/content/drive/MyDrive/Colab Notebooks\n",
            "\u001b[0m\u001b[01;34mfairseq\u001b[0m/  \u001b[01;34mkftt-data-1.0\u001b[0m/  \u001b[01;34mresult\u001b[0m/  \u001b[01;34mtokenized_data\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "%mkdir /content/drive/MyDrive/Colab\\ Notebooks\n",
        "%cd /content/drive/MyDrive/Colab Notebooks\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bcgYlX_7i7q7",
        "outputId": "24eaab75-d160-4695-899c-c39390a29bee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'fairseq' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/Colab Notebooks/fairseq\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/drive/MyDrive/Colab%20Notebooks/fairseq\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (2022.6.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.12.0+cu113)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-2.6.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (235 kB)\n",
            "\u001b[K     |████████████████████████████████| 235 kB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.29.30)\n",
            "Collecting hydra-core<1.1,>=1.0.7\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 59.1 MB/s \n",
            "\u001b[?25hCollecting sacrebleu>=1.4.12\n",
            "  Downloading sacrebleu-2.1.0-py3-none-any.whl (92 kB)\n",
            "\u001b[K     |████████████████████████████████| 92 kB 10.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.15.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (4.64.0)\n",
            "Requirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (0.12.0+cu113)\n",
            "Collecting omegaconf<2.1\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fairseq==0.12.2) (1.21.6)\n",
            "Collecting antlr4-python3-runtime==4.8\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 69.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (5.8.0)\n",
            "Collecting PyYAML>=5.1.*\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 62.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from omegaconf<2.1->fairseq==0.12.2) (4.1.1)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.5.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu>=1.4.12->fairseq==0.12.2) (0.8.10)\n",
            "Collecting colorama\n",
            "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi->fairseq==0.12.2) (2.21)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->hydra-core<1.1,>=1.0.7->fairseq==0.12.2) (3.8.0)\n",
            "Building wheels for collected packages: antlr4-python3-runtime\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=759af3ed5a5aff7545f1379440ae792cf42f72fe35ec111ded4022deaa9bc81e\n",
            "  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n",
            "Successfully built antlr4-python3-runtime\n",
            "Installing collected packages: PyYAML, portalocker, omegaconf, colorama, antlr4-python3-runtime, sacrebleu, hydra-core, bitarray, fairseq\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Running setup.py develop for fairseq\n",
            "Successfully installed PyYAML-6.0 antlr4-python3-runtime-4.8 bitarray-2.6.0 colorama-0.4.5 fairseq hydra-core-1.0.7 omegaconf-2.0.6 portalocker-2.5.1 sacrebleu-2.1.0\n",
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pytorch/fairseq\n",
        "%cd fairseq \n",
        "!pip install --editable ./\n",
        "%cd ..\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkPbOR75HhF7",
        "outputId": "75b07282-c58b-4ad3-cc12-c877edc310b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/env/python\n",
            "/env/python:/content/drive/MyDrive/Colab Notebooks/fairseq/\n"
          ]
        }
      ],
      "source": [
        "!echo $PYTHONPATH\n",
        "\n",
        "import os\n",
        "os.environ['PYTHONPATH'] += \":/content/drive/MyDrive/Colab Notebooks/fairseq/\"\n",
        "\n",
        "!echo $PYTHONPATH"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN1qi4qu5uHL"
      },
      "source": [
        "# 90"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5LPPQOIAaPv",
        "outputId": "d8132442-0e86-4a16-ec5d-19ba0c161511"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-07-19 10:22:47 | INFO | fairseq.tasks.text_to_speech | Please install tensorboardX: pip install tensorboardX\n",
            "2022-07-19 10:24:13 | INFO | fairseq_cli.preprocess | Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='result/preprocessing', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, simul_type=None, source_lang='ja', srcdict=None, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='./tokenized_data/test', tgtdict=None, threshold_loss_scale=None, thresholdsrc=3, thresholdtgt=3, tokenizer='space', tpu=False, trainpref='./tokenized_data/train', use_plasma_view=False, user_dir=None, validpref='./tokenized_data/dev', wandb_project=None, workers=20)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/bin/fairseq-preprocess\", line 33, in <module>\n",
            "    sys.exit(load_entry_point('fairseq', 'console_scripts', 'fairseq-preprocess')())\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/fairseq/fairseq_cli/preprocess.py\", line 389, in cli_main\n",
            "    main(args)\n",
            "  File \"/content/drive/MyDrive/Colab Notebooks/fairseq/fairseq_cli/preprocess.py\", line 299, in main\n",
            "    raise FileExistsError(_dict_path(args.source_lang, args.destdir))\n",
            "FileExistsError: result/preprocessing/dict.ja.txt\n"
          ]
        }
      ],
      "source": [
        "# !fairseq-preprocess \\\n",
        "#     --trainpref ./tokenized_data/train \\\n",
        "#     --validpref ./tokenized_data/dev \\\n",
        "#     --testpref  ./tokenized_data/test \\\n",
        "#     --source-lang ja \\\n",
        "#     --target-lang en \\\n",
        "#     --tokenizer space \\\n",
        "#     --workers 10 \\\n",
        "#     --thresholdsrc 5 \\\n",
        "#     --thresholdtgt 5 \\\n",
        "#     --destdir result/preprocessing \n",
        "\n",
        "  # --thresholdsrc 5 \\\n",
        "    # --thresholdtgt 5 \\\n",
        "\n",
        "!fairseq-preprocess \\\n",
        "    --trainpref ./tokenized_data/train \\\n",
        "    --validpref ./tokenized_data/dev \\\n",
        "    --testpref  ./tokenized_data/test \\\n",
        "    --source-lang ja \\\n",
        "    --target-lang en \\\n",
        "    --tokenizer space \\\n",
        "    --workers 20 \\\n",
        "    --thresholdsrc 3\\\n",
        "    --thresholdtgt 3\\\n",
        "    --task translation \\\n",
        "    --destdir result/preprocessing "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2At7vj6k5rFs"
      },
      "source": [
        "# 91"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjRwWU44_Jp7",
        "outputId": "e07b2892-0533-42cc-d90d-9f57dd946225"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mストリーミング出力は最後の 5000 行に切り捨てられました。\u001b[0m\n",
            "|       from large pool |     261    |     263    |    7339 K  |    7339 K  |\n",
            "|       from small pool |     344    |     496    |    5399 K  |    5399 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |      86    |   55208    |   55139    |\n",
            "|       from large pool |      45    |      45    |    4004    |    3959    |\n",
            "|       from small pool |      24    |      41    |   51204    |   51180    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      57    |      58    |    7172 K  |    7172 K  |\n",
            "|       from large pool |      44    |      45    |    4175 K  |    4175 K  |\n",
            "|       from small pool |      13    |      30    |    2996 K  |    2996 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:12:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  62% 1062/1720 [05:37<03:21,  3.26it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:12:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.36 GiB (GPU 0; 14.76 GiB total capacity; 12.30 GiB already allocated; 909.75 MiB free; 13.06 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:12:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1655         |        cudaMalloc retries: 2698      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9161 MB |   12597 MB |  105553 GB |  105545 GB |\n",
            "|       from large pool |    9118 MB |   12554 MB |  104760 GB |  104751 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     793 GB |     793 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9161 MB |   12597 MB |  105553 GB |  105545 GB |\n",
            "|       from large pool |    9118 MB |   12554 MB |  104760 GB |  104751 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     793 GB |     793 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13378 MB |   13378 MB |    6579 GB |    6566 GB |\n",
            "|       from large pool |   13334 MB |   13334 MB |    6479 GB |    6466 GB |\n",
            "|       from small pool |      44 MB |     184 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     780 MB |    2052 MB |  119142 GB |  119141 GB |\n",
            "|       from large pool |     779 MB |    2052 MB |  118152 GB |  118151 GB |\n",
            "|       from small pool |       1 MB |       7 MB |     990 GB |     990 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12744 K  |   12743 K  |\n",
            "|       from large pool |     258    |     263    |    7342 K  |    7342 K  |\n",
            "|       from small pool |     349    |     496    |    5401 K  |    5401 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12744 K  |   12743 K  |\n",
            "|       from large pool |     258    |     263    |    7342 K  |    7342 K  |\n",
            "|       from small pool |     349    |     496    |    5401 K  |    5401 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      68    |     138    |   55279    |   55211    |\n",
            "|       from large pool |      46    |      46    |    4007    |    3961    |\n",
            "|       from small pool |      22    |      92    |   51272    |   51250    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      53    |      57    |    7175 K  |    7175 K  |\n",
            "|       from large pool |      43    |      44    |    4177 K  |    4177 K  |\n",
            "|       from small pool |      10    |      24    |    2998 K  |    2998 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:12:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  62% 1064/1720 [05:38<03:53,  2.80it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:12:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.39 GiB (GPU 0; 14.76 GiB total capacity; 9.52 GiB already allocated; 1.86 GiB free; 12.10 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:12:36 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1656         |        cudaMalloc retries: 2699      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7307 MB |    9753 MB |  105576 GB |  105569 GB |\n",
            "|       from large pool |    7265 MB |    9711 MB |  104782 GB |  104775 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     793 GB |     793 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7307 MB |    9753 MB |  105576 GB |  105569 GB |\n",
            "|       from large pool |    7265 MB |    9711 MB |  104782 GB |  104775 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     793 GB |     793 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12388 MB |   12422 MB |    6582 GB |    6570 GB |\n",
            "|       from large pool |   12344 MB |   12344 MB |    6482 GB |    6470 GB |\n",
            "|       from small pool |      44 MB |      78 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2632 MB |    2634 MB |  119167 GB |  119164 GB |\n",
            "|       from large pool |    2632 MB |    2632 MB |  118176 GB |  118174 GB |\n",
            "|       from small pool |       0 MB |       3 MB |     990 GB |     990 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12746 K  |   12745 K  |\n",
            "|       from large pool |     258    |     262    |    7343 K  |    7343 K  |\n",
            "|       from small pool |     349    |     496    |    5402 K  |    5402 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12746 K  |   12745 K  |\n",
            "|       from large pool |     258    |     262    |    7343 K  |    7343 K  |\n",
            "|       from small pool |     349    |     496    |    5402 K  |    5402 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      68    |      85    |   55297    |   55229    |\n",
            "|       from large pool |      46    |      46    |    4008    |    3962    |\n",
            "|       from small pool |      22    |      39    |   51289    |   51267    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      46    |      48    |    7176 K  |    7176 K  |\n",
            "|       from large pool |      37    |      37    |    4177 K  |    4177 K  |\n",
            "|       from small pool |       9    |      21    |    2998 K  |    2998 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:12:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  62% 1065/1720 [05:38<03:22,  3.23it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:12:36 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.28 GiB (GPU 0; 14.76 GiB total capacity; 9.67 GiB already allocated; 1.97 GiB free; 11.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:12:36 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1657         |        cudaMalloc retries: 2700      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7572 MB |    9901 MB |  105586 GB |  105579 GB |\n",
            "|       from large pool |    7529 MB |    9859 MB |  104792 GB |  104785 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     793 GB |     793 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7572 MB |    9901 MB |  105586 GB |  105579 GB |\n",
            "|       from large pool |    7529 MB |    9859 MB |  104792 GB |  104785 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     793 GB |     793 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12272 MB |   12422 MB |    6584 GB |    6572 GB |\n",
            "|       from large pool |   12228 MB |   12344 MB |    6484 GB |    6472 GB |\n",
            "|       from small pool |      44 MB |      78 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1263 MB |    3535 MB |  119176 GB |  119175 GB |\n",
            "|       from large pool |    1262 MB |    3533 MB |  118186 GB |  118185 GB |\n",
            "|       from small pool |       1 MB |       3 MB |     990 GB |     990 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   12746 K  |   12746 K  |\n",
            "|       from large pool |     258    |     263    |    7343 K  |    7343 K  |\n",
            "|       from small pool |     350    |     496    |    5402 K  |    5402 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   12746 K  |   12746 K  |\n",
            "|       from large pool |     258    |     263    |    7343 K  |    7343 K  |\n",
            "|       from small pool |     350    |     496    |    5402 K  |    5402 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      68    |      85    |   55299    |   55231    |\n",
            "|       from large pool |      46    |      46    |    4009    |    3963    |\n",
            "|       from small pool |      22    |      39    |   51290    |   51268    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      49    |      50    |    7176 K  |    7176 K  |\n",
            "|       from large pool |      39    |      40    |    4178 K  |    4178 K  |\n",
            "|       from small pool |      10    |      21    |    2998 K  |    2998 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:12:36 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  63% 1081/1720 [05:43<03:15,  3.26it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:12:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.02 GiB (GPU 0; 14.76 GiB total capacity; 11.28 GiB already allocated; 607.75 MiB free; 13.36 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:12:41 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1658         |        cudaMalloc retries: 2704      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8464 MB |   11550 MB |  105770 GB |  105762 GB |\n",
            "|       from large pool |    8422 MB |   11508 MB |  104974 GB |  104966 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     796 GB |     796 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8464 MB |   11550 MB |  105770 GB |  105762 GB |\n",
            "|       from large pool |    8422 MB |   11508 MB |  104974 GB |  104966 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     796 GB |     796 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13680 MB |   13700 MB |    6604 GB |    6591 GB |\n",
            "|       from large pool |   13636 MB |   13636 MB |    6503 GB |    6490 GB |\n",
            "|       from small pool |      44 MB |     184 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2127 MB |    3097 MB |  119372 GB |  119370 GB |\n",
            "|       from large pool |    2125 MB |    3097 MB |  118379 GB |  118377 GB |\n",
            "|       from small pool |       1 MB |      46 MB |     993 GB |     993 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12769 K  |   12768 K  |\n",
            "|       from large pool |     258    |     263    |    7355 K  |    7355 K  |\n",
            "|       from small pool |     349    |     496    |    5413 K  |    5413 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12769 K  |   12768 K  |\n",
            "|       from large pool |     258    |     263    |    7355 K  |    7355 K  |\n",
            "|       from small pool |     349    |     496    |    5413 K  |    5413 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      67    |     137    |   55409    |   55342    |\n",
            "|       from large pool |      45    |      45    |    4017    |    3972    |\n",
            "|       from small pool |      22    |      92    |   51392    |   51370    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      49    |      55    |    7189 K  |    7189 K  |\n",
            "|       from large pool |      41    |      42    |    4184 K  |    4184 K  |\n",
            "|       from small pool |       8    |      47    |    3005 K  |    3005 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:12:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  63% 1088/1720 [05:46<03:59,  2.64it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:12:44 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.18 GiB (GPU 0; 14.76 GiB total capacity; 9.83 GiB already allocated; 3.86 GiB free; 10.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:12:44 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1659         |        cudaMalloc retries: 2707      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10069 MB |   10101 MB |  105856 GB |  105847 GB |\n",
            "|       from large pool |   10025 MB |   10057 MB |  105060 GB |  105050 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     796 GB |     796 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10069 MB |   10101 MB |  105856 GB |  105847 GB |\n",
            "|       from large pool |   10025 MB |   10057 MB |  105060 GB |  105050 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     796 GB |     796 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10334 MB |   10366 MB |    6612 GB |    6602 GB |\n",
            "|       from large pool |   10288 MB |   10288 MB |    6512 GB |    6502 GB |\n",
            "|       from small pool |      46 MB |      78 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  270555 KB |    2304 MB |  119466 GB |  119466 GB |\n",
            "|       from large pool |  269064 KB |    2301 MB |  118472 GB |  118472 GB |\n",
            "|       from small pool |    1491 KB |       4 MB |     993 GB |     993 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     610    |   12778 K  |   12778 K  |\n",
            "|       from large pool |     261    |     263    |    7361 K  |    7361 K  |\n",
            "|       from small pool |     345    |     496    |    5417 K  |    5417 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     610    |   12778 K  |   12778 K  |\n",
            "|       from large pool |     261    |     263    |    7361 K  |    7361 K  |\n",
            "|       from small pool |     345    |     496    |    5417 K  |    5417 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     123    |     139    |   55514    |   55391    |\n",
            "|       from large pool |     100    |     100    |    4085    |    3985    |\n",
            "|       from small pool |      23    |      39    |   51429    |   51406    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      80    |      81    |    7195 K  |    7195 K  |\n",
            "|       from large pool |      67    |      67    |    4187 K  |    4187 K  |\n",
            "|       from small pool |      13    |      21    |    3007 K  |    3007 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:12:44 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  63% 1092/1720 [05:47<04:40,  2.24it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:12:46 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.10 GiB (GPU 0; 14.76 GiB total capacity; 10.00 GiB already allocated; 2.43 GiB free; 11.52 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:12:46 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1660         |        cudaMalloc retries: 2710      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10237 MB |   12371 MB |  105914 GB |  105904 GB |\n",
            "|       from large pool |   10194 MB |   12328 MB |  105117 GB |  105107 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     796 GB |     796 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10237 MB |   12371 MB |  105914 GB |  105904 GB |\n",
            "|       from large pool |   10194 MB |   12328 MB |  105117 GB |  105107 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     796 GB |     796 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11800 MB |   14096 MB |    6625 GB |    6613 GB |\n",
            "|       from large pool |   11756 MB |   14052 MB |    6524 GB |    6513 GB |\n",
            "|       from small pool |      44 MB |      78 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1562 MB |    2829 MB |  119510 GB |  119509 GB |\n",
            "|       from large pool |    1561 MB |    2826 MB |  118516 GB |  118514 GB |\n",
            "|       from small pool |       0 MB |       5 MB |     994 GB |     994 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12783 K  |   12783 K  |\n",
            "|       from large pool |     258    |     263    |    7364 K  |    7364 K  |\n",
            "|       from small pool |     349    |     496    |    5419 K  |    5419 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12783 K  |   12783 K  |\n",
            "|       from large pool |     258    |     263    |    7364 K  |    7364 K  |\n",
            "|       from small pool |     349    |     496    |    5419 K  |    5419 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     111    |     131    |   55554    |   55443    |\n",
            "|       from large pool |      89    |      92    |    4090    |    4001    |\n",
            "|       from small pool |      22    |      39    |   51464    |   51442    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      93    |      98    |    7198 K  |    7197 K  |\n",
            "|       from large pool |      83    |      85    |    4189 K  |    4189 K  |\n",
            "|       from small pool |      10    |      19    |    3008 K  |    3008 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:12:46 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  64% 1093/1720 [05:48<04:41,  2.23it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:12:46 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.38 GiB (GPU 0; 14.76 GiB total capacity; 8.71 GiB already allocated; 2.45 GiB free; 11.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:12:46 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1661         |        cudaMalloc retries: 2711      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8923 MB |   12371 MB |  105923 GB |  105914 GB |\n",
            "|       from large pool |    8879 MB |   12328 MB |  105126 GB |  105117 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     796 GB |     796 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8923 MB |   12371 MB |  105923 GB |  105914 GB |\n",
            "|       from large pool |    8879 MB |   12328 MB |  105126 GB |  105117 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     796 GB |     796 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11784 MB |   14096 MB |    6625 GB |    6613 GB |\n",
            "|       from large pool |   11740 MB |   14052 MB |    6524 GB |    6513 GB |\n",
            "|       from small pool |      44 MB |      78 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2860 MB |    2860 MB |  119520 GB |  119517 GB |\n",
            "|       from large pool |    2860 MB |    2860 MB |  118526 GB |  118523 GB |\n",
            "|       from small pool |       0 MB |       5 MB |     994 GB |     994 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     613    |   12784 K  |   12783 K  |\n",
            "|       from large pool |     261    |     263    |    7364 K  |    7364 K  |\n",
            "|       from small pool |     345    |     496    |    5419 K  |    5419 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     613    |   12784 K  |   12783 K  |\n",
            "|       from large pool |     261    |     263    |    7364 K  |    7364 K  |\n",
            "|       from small pool |     345    |     496    |    5419 K  |    5419 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     110    |     131    |   55554    |   55444    |\n",
            "|       from large pool |      88    |      92    |    4090    |    4002    |\n",
            "|       from small pool |      22    |      39    |   51464    |   51442    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      83    |      98    |    7198 K  |    7198 K  |\n",
            "|       from large pool |      70    |      85    |    4190 K  |    4189 K  |\n",
            "|       from small pool |      13    |      19    |    3008 K  |    3008 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:12:46 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  64% 1100/1720 [05:50<03:30,  2.95it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:12:48 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.67 GiB (GPU 0; 14.76 GiB total capacity; 7.71 GiB already allocated; 2.45 GiB free; 11.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:12:48 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1662         |        cudaMalloc retries: 2712      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7899 MB |    9292 MB |  106001 GB |  105993 GB |\n",
            "|       from large pool |    7857 MB |    9249 MB |  105204 GB |  105196 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     797 GB |     797 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7899 MB |    9292 MB |  106001 GB |  105993 GB |\n",
            "|       from large pool |    7857 MB |    9249 MB |  105204 GB |  105196 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     797 GB |     797 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11784 MB |   11822 MB |    6625 GB |    6613 GB |\n",
            "|       from large pool |   11740 MB |   11740 MB |    6524 GB |    6513 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3884 MB |    3884 MB |  119621 GB |  119617 GB |\n",
            "|       from large pool |    3882 MB |    3882 MB |  118626 GB |  118623 GB |\n",
            "|       from small pool |       1 MB |       4 MB |     994 GB |     994 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12793 K  |   12793 K  |\n",
            "|       from large pool |     258    |     262    |    7370 K  |    7370 K  |\n",
            "|       from small pool |     349    |     496    |    5423 K  |    5422 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12793 K  |   12793 K  |\n",
            "|       from large pool |     258    |     262    |    7370 K  |    7370 K  |\n",
            "|       from small pool |     349    |     496    |    5423 K  |    5422 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     110    |     129    |   55573    |   55463    |\n",
            "|       from large pool |      88    |      88    |    4090    |    4002    |\n",
            "|       from small pool |      22    |      41    |   51483    |   51461    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      83    |      86    |    7204 K  |    7203 K  |\n",
            "|       from large pool |      71    |      71    |    4193 K  |    4193 K  |\n",
            "|       from small pool |      12    |      23    |    3010 K  |    3010 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:12:48 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  64% 1104/1720 [05:51<03:38,  2.82it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:12:49 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.36 GiB (GPU 0; 14.76 GiB total capacity; 7.36 GiB already allocated; 2.45 GiB free; 11.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:12:49 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1663         |        cudaMalloc retries: 2713      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7539 MB |    7563 MB |  106051 GB |  106043 GB |\n",
            "|       from large pool |    7496 MB |    7520 MB |  105253 GB |  105246 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     797 GB |     797 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7539 MB |    7563 MB |  106051 GB |  106043 GB |\n",
            "|       from large pool |    7496 MB |    7520 MB |  105253 GB |  105246 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     797 GB |     797 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11784 MB |   11818 MB |    6625 GB |    6613 GB |\n",
            "|       from large pool |   11740 MB |   11740 MB |    6524 GB |    6513 GB |\n",
            "|       from small pool |      44 MB |      78 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    4244 MB |    4244 MB |  119680 GB |  119676 GB |\n",
            "|       from large pool |    4243 MB |    4243 MB |  118685 GB |  118681 GB |\n",
            "|       from small pool |       1 MB |       3 MB |     994 GB |     994 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     610    |   12798 K  |   12798 K  |\n",
            "|       from large pool |     261    |     263    |    7373 K  |    7373 K  |\n",
            "|       from small pool |     345    |     496    |    5425 K  |    5424 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     610    |   12798 K  |   12798 K  |\n",
            "|       from large pool |     261    |     263    |    7373 K  |    7373 K  |\n",
            "|       from small pool |     345    |     496    |    5425 K  |    5424 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     110    |     127    |   55590    |   55480    |\n",
            "|       from large pool |      88    |      88    |    4090    |    4002    |\n",
            "|       from small pool |      22    |      39    |   51500    |   51478    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      78    |      78    |    7207 K  |    7207 K  |\n",
            "|       from large pool |      67    |      67    |    4195 K  |    4195 K  |\n",
            "|       from small pool |      11    |      18    |    3011 K  |    3011 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:12:49 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  64% 1109/1720 [05:53<02:32,  4.01it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:12:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.62 GiB (GPU 0; 14.76 GiB total capacity; 7.93 GiB already allocated; 2.45 GiB free; 11.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:12:50 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1664         |        cudaMalloc retries: 2714      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8119 MB |    9483 MB |  106091 GB |  106083 GB |\n",
            "|       from large pool |    8076 MB |    9441 MB |  105294 GB |  105286 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     797 GB |     797 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8119 MB |    9483 MB |  106091 GB |  106083 GB |\n",
            "|       from large pool |    8076 MB |    9441 MB |  105294 GB |  105286 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     797 GB |     797 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11784 MB |   11822 MB |    6625 GB |    6613 GB |\n",
            "|       from large pool |   11740 MB |   11740 MB |    6524 GB |    6513 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3664 MB |    3664 MB |  119740 GB |  119736 GB |\n",
            "|       from large pool |    3663 MB |    3663 MB |  118744 GB |  118741 GB |\n",
            "|       from small pool |       1 MB |       5 MB |     995 GB |     995 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12805 K  |   12804 K  |\n",
            "|       from large pool |     258    |     263    |    7377 K  |    7377 K  |\n",
            "|       from small pool |     349    |     496    |    5427 K  |    5427 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12805 K  |   12804 K  |\n",
            "|       from large pool |     258    |     263    |    7377 K  |    7377 K  |\n",
            "|       from small pool |     349    |     496    |    5427 K  |    5427 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     110    |     129    |   55609    |   55499    |\n",
            "|       from large pool |      88    |      88    |    4090    |    4002    |\n",
            "|       from small pool |      22    |      41    |   51519    |   51497    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      76    |      77    |    7211 K  |    7211 K  |\n",
            "|       from large pool |      65    |      65    |    4198 K  |    4198 K  |\n",
            "|       from small pool |      11    |      18    |    3012 K  |    3012 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:12:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  65% 1117/1720 [05:55<02:59,  3.35it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:12:53 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.13 GiB (GPU 0; 14.76 GiB total capacity; 8.33 GiB already allocated; 2.44 GiB free; 11.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:12:53 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1665         |        cudaMalloc retries: 2715      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8501 MB |    8565 MB |  106162 GB |  106154 GB |\n",
            "|       from large pool |    8455 MB |    8519 MB |  105364 GB |  105356 GB |\n",
            "|       from small pool |      45 MB |      75 MB |     798 GB |     798 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8501 MB |    8565 MB |  106162 GB |  106154 GB |\n",
            "|       from large pool |    8455 MB |    8519 MB |  105364 GB |  105356 GB |\n",
            "|       from small pool |      45 MB |      75 MB |     798 GB |     798 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11786 MB |   11872 MB |    6625 GB |    6613 GB |\n",
            "|       from large pool |   11740 MB |   11740 MB |    6524 GB |    6513 GB |\n",
            "|       from small pool |      46 MB |     132 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3284 MB |    4657 MB |  119836 GB |  119833 GB |\n",
            "|       from large pool |    3284 MB |    4655 MB |  118840 GB |  118837 GB |\n",
            "|       from small pool |       0 MB |       4 MB |     996 GB |     996 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     611    |   12816 K  |   12815 K  |\n",
            "|       from large pool |     261    |     263    |    7383 K  |    7383 K  |\n",
            "|       from small pool |     347    |     496    |    5432 K  |    5432 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     611    |   12816 K  |   12815 K  |\n",
            "|       from large pool |     261    |     263    |    7383 K  |    7383 K  |\n",
            "|       from small pool |     347    |     496    |    5432 K  |    5432 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     111    |     154    |   55653    |   55542    |\n",
            "|       from large pool |      88    |      88    |    4090    |    4002    |\n",
            "|       from small pool |      23    |      66    |   51563    |   51540    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |     102    |     103    |    7217 K  |    7217 K  |\n",
            "|       from large pool |      88    |      89    |    4202 K  |    4202 K  |\n",
            "|       from small pool |      14    |      19    |    3015 K  |    3015 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:12:53 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  65% 1118/1720 [05:55<02:57,  3.40it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:12:53 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.36 GiB (GPU 0; 14.76 GiB total capacity; 8.33 GiB already allocated; 2.44 GiB free; 11.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:12:53 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1666         |        cudaMalloc retries: 2716      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8498 MB |    8568 MB |  106173 GB |  106164 GB |\n",
            "|       from large pool |    8452 MB |    8522 MB |  105374 GB |  105366 GB |\n",
            "|       from small pool |      45 MB |      75 MB |     798 GB |     798 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8498 MB |    8568 MB |  106173 GB |  106164 GB |\n",
            "|       from large pool |    8452 MB |    8522 MB |  105374 GB |  105366 GB |\n",
            "|       from small pool |      45 MB |      75 MB |     798 GB |     798 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11786 MB |   11872 MB |    6625 GB |    6613 GB |\n",
            "|       from large pool |   11740 MB |   11740 MB |    6524 GB |    6513 GB |\n",
            "|       from small pool |      46 MB |     132 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3245 MB |    4728 MB |  119851 GB |  119848 GB |\n",
            "|       from large pool |    3245 MB |    4727 MB |  118855 GB |  118852 GB |\n",
            "|       from small pool |       0 MB |       4 MB |     996 GB |     996 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     611    |   12816 K  |   12815 K  |\n",
            "|       from large pool |     261    |     263    |    7383 K  |    7383 K  |\n",
            "|       from small pool |     347    |     496    |    5432 K  |    5432 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     611    |   12816 K  |   12815 K  |\n",
            "|       from large pool |     261    |     263    |    7383 K  |    7383 K  |\n",
            "|       from small pool |     347    |     496    |    5432 K  |    5432 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     111    |     154    |   55653    |   55542    |\n",
            "|       from large pool |      88    |      88    |    4090    |    4002    |\n",
            "|       from small pool |      23    |      66    |   51563    |   51540    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      86    |     105    |    7218 K  |    7218 K  |\n",
            "|       from large pool |      72    |      90    |    4202 K  |    4202 K  |\n",
            "|       from small pool |      14    |      19    |    3015 K  |    3015 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:12:53 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  65% 1122/1720 [05:56<03:32,  2.82it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:12:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.24 GiB (GPU 0; 14.76 GiB total capacity; 9.09 GiB already allocated; 2.60 GiB free; 11.35 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:12:54 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1667         |        cudaMalloc retries: 2717      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9305 MB |   10995 MB |  106228 GB |  106219 GB |\n",
            "|       from large pool |    9262 MB |   10952 MB |  105430 GB |  105421 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     798 GB |     798 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9305 MB |   10995 MB |  106228 GB |  106219 GB |\n",
            "|       from large pool |    9262 MB |   10952 MB |  105430 GB |  105421 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     798 GB |     798 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11626 MB |   13934 MB |    6627 GB |    6616 GB |\n",
            "|       from large pool |   11582 MB |   13854 MB |    6526 GB |    6515 GB |\n",
            "|       from small pool |      44 MB |      80 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2320 MB |    2932 MB |  119910 GB |  119908 GB |\n",
            "|       from large pool |    2319 MB |    2931 MB |  118914 GB |  118911 GB |\n",
            "|       from small pool |       0 MB |       5 MB |     996 GB |     996 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12821 K  |   12820 K  |\n",
            "|       from large pool |     258    |     263    |    7387 K  |    7386 K  |\n",
            "|       from small pool |     349    |     496    |    5434 K  |    5434 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12821 K  |   12820 K  |\n",
            "|       from large pool |     258    |     263    |    7387 K  |    7386 K  |\n",
            "|       from small pool |     349    |     496    |    5434 K  |    5434 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     109    |     128    |   55671    |   55562    |\n",
            "|       from large pool |      87    |      88    |    4091    |    4004    |\n",
            "|       from small pool |      22    |      40    |   51580    |   51558    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      82    |      85    |    7221 K  |    7221 K  |\n",
            "|       from large pool |      73    |      74    |    4204 K  |    4204 K  |\n",
            "|       from small pool |       9    |      19    |    3016 K  |    3016 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:12:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  66% 1127/1720 [05:58<03:01,  3.28it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:12:56 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.22 GiB (GPU 0; 14.76 GiB total capacity; 9.19 GiB already allocated; 483.75 MiB free; 13.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:12:56 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1668         |        cudaMalloc retries: 2720      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7141 MB |    9412 MB |  106287 GB |  106280 GB |\n",
            "|       from large pool |    7099 MB |    9370 MB |  105488 GB |  105481 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     798 GB |     798 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7141 MB |    9412 MB |  106287 GB |  106280 GB |\n",
            "|       from large pool |    7099 MB |    9370 MB |  105488 GB |  105481 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     798 GB |     798 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13804 MB |   13804 MB |    6634 GB |    6620 GB |\n",
            "|       from large pool |   13760 MB |   13760 MB |    6533 GB |    6519 GB |\n",
            "|       from small pool |      44 MB |      80 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    4390 MB |    4391 MB |  119977 GB |  119973 GB |\n",
            "|       from large pool |    4388 MB |    4389 MB |  118980 GB |  118976 GB |\n",
            "|       from small pool |       2 MB |       8 MB |     996 GB |     996 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12828 K  |   12827 K  |\n",
            "|       from large pool |     258    |     262    |    7391 K  |    7390 K  |\n",
            "|       from small pool |     349    |     496    |    5436 K  |    5436 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12828 K  |   12827 K  |\n",
            "|       from large pool |     258    |     262    |    7391 K  |    7390 K  |\n",
            "|       from small pool |     349    |     496    |    5436 K  |    5436 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     108    |     126    |   55692    |   55584    |\n",
            "|       from large pool |      86    |      86    |    4094    |    4008    |\n",
            "|       from small pool |      22    |      40    |   51598    |   51576    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      94    |      97    |    7225 K  |    7225 K  |\n",
            "|       from large pool |      83    |      84    |    4207 K  |    4207 K  |\n",
            "|       from small pool |      11    |      33    |    3017 K  |    3017 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:12:56 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  66% 1141/1720 [06:02<02:49,  3.42it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:13:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.29 GiB (GPU 0; 14.76 GiB total capacity; 9.32 GiB already allocated; 2.58 GiB free; 11.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:01 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1669         |        cudaMalloc retries: 2722      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9539 MB |    9561 MB |  106440 GB |  106431 GB |\n",
            "|       from large pool |    9495 MB |    9517 MB |  105640 GB |  105631 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     800 GB |     799 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9539 MB |    9561 MB |  106440 GB |  106431 GB |\n",
            "|       from large pool |    9495 MB |    9517 MB |  105640 GB |  105631 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     800 GB |     799 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11648 MB |   14104 MB |    6638 GB |    6627 GB |\n",
            "|       from large pool |   11602 MB |   13990 MB |    6538 GB |    6526 GB |\n",
            "|       from small pool |      46 MB |     114 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2108 MB |    2778 MB |  120178 GB |  120176 GB |\n",
            "|       from large pool |    2106 MB |    2777 MB |  119179 GB |  119177 GB |\n",
            "|       from small pool |       1 MB |       9 MB |     998 GB |     998 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     611    |   12847 K  |   12847 K  |\n",
            "|       from large pool |     261    |     263    |    7402 K  |    7402 K  |\n",
            "|       from small pool |     346    |     496    |    5445 K  |    5445 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     611    |   12847 K  |   12847 K  |\n",
            "|       from large pool |     261    |     263    |    7402 K  |    7402 K  |\n",
            "|       from small pool |     346    |     496    |    5445 K  |    5445 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     107    |     142    |   55776    |   55669    |\n",
            "|       from large pool |      84    |      85    |    4096    |    4012    |\n",
            "|       from small pool |      23    |      57    |   51680    |   51657    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      88    |      89    |    7237 K  |    7237 K  |\n",
            "|       from large pool |      76    |      77    |    4214 K  |    4214 K  |\n",
            "|       from small pool |      12    |      28    |    3022 K  |    3022 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  66% 1142/1720 [06:03<03:01,  3.19it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:13:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.92 GiB (GPU 0; 14.76 GiB total capacity; 8.70 GiB already allocated; 2.62 GiB free; 11.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:01 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1670         |        cudaMalloc retries: 2723      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8911 MB |    9561 MB |  106449 GB |  106440 GB |\n",
            "|       from large pool |    8867 MB |    9517 MB |  105649 GB |  105640 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     800 GB |     799 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8911 MB |    9561 MB |  106449 GB |  106440 GB |\n",
            "|       from large pool |    8867 MB |    9517 MB |  105649 GB |  105640 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     800 GB |     799 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11600 MB |   14104 MB |    6638 GB |    6627 GB |\n",
            "|       from large pool |   11554 MB |   13990 MB |    6538 GB |    6526 GB |\n",
            "|       from small pool |      46 MB |     114 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2688 MB |    4310 MB |  120188 GB |  120186 GB |\n",
            "|       from large pool |    2686 MB |    4310 MB |  119190 GB |  119187 GB |\n",
            "|       from small pool |       2 MB |       9 MB |     998 GB |     998 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     611    |   12848 K  |   12847 K  |\n",
            "|       from large pool |     261    |     263    |    7402 K  |    7402 K  |\n",
            "|       from small pool |     345    |     496    |    5445 K  |    5445 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     611    |   12848 K  |   12847 K  |\n",
            "|       from large pool |     261    |     263    |    7402 K  |    7402 K  |\n",
            "|       from small pool |     345    |     496    |    5445 K  |    5445 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     105    |     142    |   55776    |   55671    |\n",
            "|       from large pool |      82    |      85    |    4096    |    4014    |\n",
            "|       from small pool |      23    |      57    |   51680    |   51657    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      96    |      98    |    7237 K  |    7237 K  |\n",
            "|       from large pool |      84    |      87    |    4214 K  |    4214 K  |\n",
            "|       from small pool |      12    |      28    |    3022 K  |    3022 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  67% 1148/1720 [06:05<03:16,  2.90it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:13:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.67 GiB (GPU 0; 14.76 GiB total capacity; 7.79 GiB already allocated; 2.63 GiB free; 11.33 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:03 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1671         |        cudaMalloc retries: 2724      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7981 MB |    9375 MB |  106517 GB |  106509 GB |\n",
            "|       from large pool |    7939 MB |    9333 MB |  105717 GB |  105709 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     800 GB |     800 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7981 MB |    9375 MB |  106517 GB |  106509 GB |\n",
            "|       from large pool |    7939 MB |    9333 MB |  105717 GB |  105709 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     800 GB |     800 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11598 MB |   11634 MB |    6639 GB |    6627 GB |\n",
            "|       from large pool |   11554 MB |   11554 MB |    6538 GB |    6526 GB |\n",
            "|       from small pool |      44 MB |      80 MB |     100 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3616 MB |    3616 MB |  120285 GB |  120282 GB |\n",
            "|       from large pool |    3614 MB |    3614 MB |  119286 GB |  119283 GB |\n",
            "|       from small pool |       1 MB |       3 MB |     998 GB |     998 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12856 K  |   12855 K  |\n",
            "|       from large pool |     258    |     263    |    7407 K  |    7407 K  |\n",
            "|       from small pool |     349    |     496    |    5448 K  |    5448 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12856 K  |   12855 K  |\n",
            "|       from large pool |     258    |     263    |    7407 K  |    7407 K  |\n",
            "|       from small pool |     349    |     496    |    5448 K  |    5448 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     104    |     122    |   55793    |   55689    |\n",
            "|       from large pool |      82    |      82    |    4096    |    4014    |\n",
            "|       from small pool |      22    |      40    |   51697    |   51675    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      78    |      80    |    7242 K  |    7242 K  |\n",
            "|       from large pool |      65    |      65    |    4217 K  |    4217 K  |\n",
            "|       from small pool |      13    |      19    |    3024 K  |    3024 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  67% 1157/1720 [06:08<03:02,  3.08it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:13:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.67 GiB (GPU 0; 14.76 GiB total capacity; 9.84 GiB already allocated; 2.47 GiB free; 11.49 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:06 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1672         |        cudaMalloc retries: 2726      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7340 MB |   10076 MB |  106637 GB |  106630 GB |\n",
            "|       from large pool |    7298 MB |   10035 MB |  105836 GB |  105829 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     800 GB |     800 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7340 MB |   10076 MB |  106637 GB |  106630 GB |\n",
            "|       from large pool |    7298 MB |   10035 MB |  105836 GB |  105829 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     800 GB |     800 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11762 MB |   11762 MB |    6641 GB |    6630 GB |\n",
            "|       from large pool |   11720 MB |   11720 MB |    6540 GB |    6529 GB |\n",
            "|       from small pool |      42 MB |      82 MB |     101 GB |     100 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1683 MB |    2703 MB |  120438 GB |  120436 GB |\n",
            "|       from large pool |    1683 MB |    2703 MB |  119438 GB |  119437 GB |\n",
            "|       from small pool |       0 MB |       7 MB |     999 GB |     999 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12869 K  |   12868 K  |\n",
            "|       from large pool |     258    |     262    |    7415 K  |    7415 K  |\n",
            "|       from small pool |     349    |     496    |    5453 K  |    5453 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12869 K  |   12868 K  |\n",
            "|       from large pool |     258    |     262    |    7415 K  |    7415 K  |\n",
            "|       from small pool |     349    |     496    |    5453 K  |    5453 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      99    |     123    |   55813    |   55714    |\n",
            "|       from large pool |      78    |      82    |    4097    |    4019    |\n",
            "|       from small pool |      21    |      41    |   51716    |   51695    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      47    |      51    |    7249 K  |    7249 K  |\n",
            "|       from large pool |      38    |      39    |    4222 K  |    4222 K  |\n",
            "|       from small pool |       9    |      29    |    3026 K  |    3026 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  68% 1175/1720 [06:14<02:07,  4.27it/s, loss=4.528, nll_loss=4.528, ppl=23.07, wps=11505.7, ups=2.5, wpb=4593.3, bsz=256, num_updates=8000, lr=0.0001, gnorm=1.702, loss_scale=4, train_wall=30, gb_free=9.6, wall=3122]2022-07-19 07:13:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.33 GiB (GPU 0; 14.76 GiB total capacity; 9.57 GiB already allocated; 1.61 GiB free; 12.34 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1673         |        cudaMalloc retries: 2728      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7415 MB |    9803 MB |  106850 GB |  106843 GB |\n",
            "|       from large pool |    7372 MB |    9760 MB |  106048 GB |  106041 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     801 GB |     801 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7415 MB |    9803 MB |  106850 GB |  106843 GB |\n",
            "|       from large pool |    7372 MB |    9760 MB |  106048 GB |  106041 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     801 GB |     801 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12638 MB |   12638 MB |    6647 GB |    6634 GB |\n",
            "|       from large pool |   12594 MB |   12594 MB |    6546 GB |    6533 GB |\n",
            "|       from small pool |      44 MB |      84 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2834 MB |    3563 MB |  120664 GB |  120662 GB |\n",
            "|       from large pool |    2833 MB |    3561 MB |  119663 GB |  119661 GB |\n",
            "|       from small pool |       1 MB |      15 MB |    1000 GB |    1000 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   12895 K  |   12894 K  |\n",
            "|       from large pool |     258    |     262    |    7431 K  |    7430 K  |\n",
            "|       from small pool |     350    |     496    |    5463 K  |    5463 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   12895 K  |   12894 K  |\n",
            "|       from large pool |     258    |     262    |    7431 K  |    7430 K  |\n",
            "|       from small pool |     350    |     496    |    5463 K  |    5463 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     101    |     121    |   55837    |   55736    |\n",
            "|       from large pool |      79    |      79    |    4100    |    4021    |\n",
            "|       from small pool |      22    |      42    |   51737    |   51715    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      79    |      81    |    7265 K  |    7265 K  |\n",
            "|       from large pool |      68    |      69    |    4232 K  |    4232 K  |\n",
            "|       from small pool |      11    |      38    |    3032 K  |    3032 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  69% 1179/1720 [06:15<02:32,  3.54it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.16 GiB (GPU 0; 14.76 GiB total capacity; 9.11 GiB already allocated; 1.78 GiB free; 12.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1674         |        cudaMalloc retries: 2729      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7115 MB |    9328 MB |  106890 GB |  106883 GB |\n",
            "|       from large pool |    7073 MB |    9286 MB |  106088 GB |  106081 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     802 GB |     802 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7115 MB |    9328 MB |  106890 GB |  106883 GB |\n",
            "|       from large pool |    7073 MB |    9286 MB |  106088 GB |  106081 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     802 GB |     802 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12464 MB |   14134 MB |    6650 GB |    6638 GB |\n",
            "|       from large pool |   12420 MB |   14052 MB |    6549 GB |    6537 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3134 MB |    3659 MB |  120712 GB |  120709 GB |\n",
            "|       from large pool |    3132 MB |    3657 MB |  119711 GB |  119708 GB |\n",
            "|       from small pool |       2 MB |       3 MB |    1001 GB |    1001 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12900 K  |   12899 K  |\n",
            "|       from large pool |     258    |     262    |    7434 K  |    7433 K  |\n",
            "|       from small pool |     349    |     495    |    5465 K  |    5465 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12900 K  |   12899 K  |\n",
            "|       from large pool |     258    |     262    |    7434 K  |    7433 K  |\n",
            "|       from small pool |     349    |     495    |    5465 K  |    5465 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     101    |     121    |   55858    |   55757    |\n",
            "|       from large pool |      79    |      80    |    4102    |    4023    |\n",
            "|       from small pool |      22    |      41    |   51756    |   51734    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      66    |      70    |    7268 K  |    7268 K  |\n",
            "|       from large pool |      56    |      57    |    4234 K  |    4234 K  |\n",
            "|       from small pool |      10    |      20    |    3033 K  |    3033 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  69% 1180/1720 [06:15<02:31,  3.57it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.09 GiB (GPU 0; 14.76 GiB total capacity; 10.52 GiB already allocated; 919.75 MiB free; 13.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1675         |        cudaMalloc retries: 2730      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10776 MB |   10803 MB |  106902 GB |  106892 GB |\n",
            "|       from large pool |   10731 MB |   10758 MB |  106100 GB |  106089 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     802 GB |     802 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10776 MB |   10803 MB |  106902 GB |  106892 GB |\n",
            "|       from large pool |   10731 MB |   10758 MB |  106100 GB |  106089 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     802 GB |     802 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13368 MB |   14134 MB |    6654 GB |    6640 GB |\n",
            "|       from large pool |   13322 MB |   14052 MB |    6552 GB |    6539 GB |\n",
            "|       from small pool |      46 MB |      82 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2591 MB |    4658 MB |  120723 GB |  120720 GB |\n",
            "|       from large pool |    2590 MB |    4657 MB |  119721 GB |  119719 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1001 GB |    1001 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12900 K  |   12899 K  |\n",
            "|       from large pool |     261    |     263    |    7434 K  |    7434 K  |\n",
            "|       from small pool |     346    |     495    |    5465 K  |    5465 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12900 K  |   12899 K  |\n",
            "|       from large pool |     261    |     263    |    7434 K  |    7434 K  |\n",
            "|       from small pool |     346    |     495    |    5465 K  |    5465 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     102    |     121    |   55860    |   55758    |\n",
            "|       from large pool |      79    |      80    |    4103    |    4024    |\n",
            "|       from small pool |      23    |      41    |   51757    |   51734    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      93    |      93    |    7268 K  |    7268 K  |\n",
            "|       from large pool |      79    |      80    |    4234 K  |    4234 K  |\n",
            "|       from small pool |      14    |      20    |    3033 K  |    3033 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "2022-07-19 07:13:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.62 GiB (GPU 0; 14.76 GiB total capacity; 10.14 GiB already allocated; 921.75 MiB free; 13.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1676         |        cudaMalloc retries: 2731      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7705 MB |   10803 MB |  106913 GB |  106905 GB |\n",
            "|       from large pool |    7663 MB |   10758 MB |  106110 GB |  106103 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     802 GB |     802 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7705 MB |   10803 MB |  106913 GB |  106905 GB |\n",
            "|       from large pool |    7663 MB |   10758 MB |  106110 GB |  106103 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     802 GB |     802 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13366 MB |   14134 MB |    6654 GB |    6640 GB |\n",
            "|       from large pool |   13322 MB |   14052 MB |    6552 GB |    6539 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2544 MB |    4775 MB |  120734 GB |  120731 GB |\n",
            "|       from large pool |    2542 MB |    4774 MB |  119732 GB |  119730 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1001 GB |    1001 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12901 K  |   12900 K  |\n",
            "|       from large pool |     258    |     263    |    7434 K  |    7434 K  |\n",
            "|       from small pool |     349    |     495    |    5466 K  |    5465 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12901 K  |   12900 K  |\n",
            "|       from large pool |     258    |     263    |    7434 K  |    7434 K  |\n",
            "|       from small pool |     349    |     495    |    5466 K  |    5465 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     101    |     121    |   55860    |   55759    |\n",
            "|       from large pool |      79    |      80    |    4103    |    4024    |\n",
            "|       from small pool |      22    |      41    |   51757    |   51735    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      55    |      96    |    7268 K  |    7268 K  |\n",
            "|       from large pool |      45    |      82    |    4235 K  |    4235 K  |\n",
            "|       from small pool |      10    |      20    |    3033 K  |    3033 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  69% 1183/1720 [06:16<02:16,  3.95it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.28 GiB (GPU 0; 14.76 GiB total capacity; 9.45 GiB already allocated; 1.67 GiB free; 12.29 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:14 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1677         |        cudaMalloc retries: 2732      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7343 MB |    9673 MB |  106926 GB |  106918 GB |\n",
            "|       from large pool |    7301 MB |    9631 MB |  106123 GB |  106116 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     802 GB |     802 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7343 MB |    9673 MB |  106926 GB |  106918 GB |\n",
            "|       from large pool |    7301 MB |    9631 MB |  106123 GB |  106116 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     802 GB |     802 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12580 MB |   12674 MB |    6656 GB |    6644 GB |\n",
            "|       from large pool |   12536 MB |   12536 MB |    6555 GB |    6542 GB |\n",
            "|       from small pool |      44 MB |     138 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2906 MB |    4049 MB |  120748 GB |  120745 GB |\n",
            "|       from large pool |    2904 MB |    4048 MB |  119746 GB |  119744 GB |\n",
            "|       from small pool |       1 MB |      20 MB |    1001 GB |    1001 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12902 K  |   12902 K  |\n",
            "|       from large pool |     258    |     262    |    7435 K  |    7435 K  |\n",
            "|       from small pool |     349    |     496    |    5467 K  |    5466 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12902 K  |   12902 K  |\n",
            "|       from large pool |     258    |     262    |    7435 K  |    7435 K  |\n",
            "|       from small pool |     349    |     496    |    5467 K  |    5466 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     101    |     148    |   55908    |   55807    |\n",
            "|       from large pool |      79    |      79    |    4104    |    4025    |\n",
            "|       from small pool |      22    |      69    |   51804    |   51782    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      79    |      81    |    7269 K  |    7269 K  |\n",
            "|       from large pool |      70    |      70    |    4235 K  |    4235 K  |\n",
            "|       from small pool |       9    |      32    |    3034 K  |    3034 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  69% 1185/1720 [06:16<02:20,  3.80it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.07 GiB (GPU 0; 14.76 GiB total capacity; 11.03 GiB already allocated; 891.75 MiB free; 13.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:14 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1678         |        cudaMalloc retries: 2733      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8150 MB |   11294 MB |  106950 GB |  106942 GB |\n",
            "|       from large pool |    8108 MB |   11252 MB |  106147 GB |  106139 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     802 GB |     802 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8150 MB |   11294 MB |  106950 GB |  106942 GB |\n",
            "|       from large pool |    8108 MB |   11252 MB |  106147 GB |  106139 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     802 GB |     802 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13396 MB |   13430 MB |    6659 GB |    6646 GB |\n",
            "|       from large pool |   13352 MB |   13352 MB |    6558 GB |    6545 GB |\n",
            "|       from small pool |      44 MB |      78 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1051 MB |    3673 MB |  120771 GB |  120770 GB |\n",
            "|       from large pool |    1049 MB |    3671 MB |  119770 GB |  119769 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1001 GB |    1001 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12904 K  |   12904 K  |\n",
            "|       from large pool |     258    |     262    |    7437 K  |    7436 K  |\n",
            "|       from small pool |     349    |     496    |    5467 K  |    5467 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12904 K  |   12904 K  |\n",
            "|       from large pool |     258    |     262    |    7437 K  |    7436 K  |\n",
            "|       from small pool |     349    |     496    |    5467 K  |    5467 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     101    |     118    |   55926    |   55825    |\n",
            "|       from large pool |      79    |      79    |    4105    |    4026    |\n",
            "|       from small pool |      22    |      39    |   51821    |   51799    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      83    |      85    |    7271 K  |    7271 K  |\n",
            "|       from large pool |      72    |      73    |    4236 K  |    4236 K  |\n",
            "|       from small pool |      11    |      20    |    3034 K  |    3034 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  69% 1195/1720 [06:20<02:47,  3.13it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.96 GiB (GPU 0; 14.76 GiB total capacity; 11.62 GiB already allocated; 1.42 GiB free; 12.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1679         |        cudaMalloc retries: 2737      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8872 MB |   11900 MB |  107060 GB |  107051 GB |\n",
            "|       from large pool |    8829 MB |   11857 MB |  106256 GB |  106247 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     803 GB |     803 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8872 MB |   11900 MB |  107060 GB |  107051 GB |\n",
            "|       from large pool |    8829 MB |   11857 MB |  106256 GB |  106247 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     803 GB |     803 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12836 MB |   14274 MB |    6672 GB |    6659 GB |\n",
            "|       from large pool |   12792 MB |   14192 MB |    6570 GB |    6558 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     935 MB |    2116 MB |  120878 GB |  120877 GB |\n",
            "|       from large pool |     934 MB |    2114 MB |  119875 GB |  119874 GB |\n",
            "|       from small pool |       1 MB |       9 MB |    1003 GB |    1003 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12918 K  |   12918 K  |\n",
            "|       from large pool |     258    |     263    |    7444 K  |    7444 K  |\n",
            "|       from small pool |     349    |     496    |    5474 K  |    5473 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12918 K  |   12918 K  |\n",
            "|       from large pool |     258    |     263    |    7444 K  |    7444 K  |\n",
            "|       from small pool |     349    |     496    |    5474 K  |    5473 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     100    |     120    |   56041    |   55941    |\n",
            "|       from large pool |      78    |      79    |    4111    |    4033    |\n",
            "|       from small pool |      22    |      41    |   51930    |   51908    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      85    |      88    |    7279 K  |    7279 K  |\n",
            "|       from large pool |      74    |      75    |    4241 K  |    4241 K  |\n",
            "|       from small pool |      11    |      32    |    3038 K  |    3038 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  70% 1202/1720 [06:22<02:28,  3.48it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.27 GiB (GPU 0; 14.76 GiB total capacity; 10.47 GiB already allocated; 135.75 MiB free; 13.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:20 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1680         |        cudaMalloc retries: 2738      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10718 MB |   12941 MB |  107125 GB |  107114 GB |\n",
            "|       from large pool |   10674 MB |   12897 MB |  106320 GB |  106310 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     804 GB |     804 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10718 MB |   12941 MB |  107125 GB |  107114 GB |\n",
            "|       from large pool |   10674 MB |   12897 MB |  106320 GB |  106310 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     804 GB |     804 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14152 MB |   14272 MB |    6676 GB |    6662 GB |\n",
            "|       from large pool |   14108 MB |   14132 MB |    6575 GB |    6561 GB |\n",
            "|       from small pool |      44 MB |     140 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3433 MB |    3488 MB |  120957 GB |  120954 GB |\n",
            "|       from large pool |    3433 MB |    3485 MB |  119953 GB |  119950 GB |\n",
            "|       from small pool |       0 MB |       3 MB |    1003 GB |    1003 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12928 K  |   12927 K  |\n",
            "|       from large pool |     258    |     263    |    7449 K  |    7449 K  |\n",
            "|       from small pool |     349    |     496    |    5478 K  |    5477 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12928 K  |   12927 K  |\n",
            "|       from large pool |     258    |     263    |    7449 K  |    7449 K  |\n",
            "|       from small pool |     349    |     496    |    5478 K  |    5477 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      99    |     148    |   56090    |   55991    |\n",
            "|       from large pool |      77    |      78    |    4112    |    4035    |\n",
            "|       from small pool |      22    |      70    |   51978    |   51956    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      94    |      96    |    7284 K  |    7284 K  |\n",
            "|       from large pool |      85    |      85    |    4244 K  |    4244 K  |\n",
            "|       from small pool |       9    |      18    |    3040 K  |    3040 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  71% 1214/1720 [06:25<02:07,  3.97it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.33 GiB (GPU 0; 14.76 GiB total capacity; 9.00 GiB already allocated; 2.12 GiB free; 11.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:23 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1681         |        cudaMalloc retries: 2739      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6824 MB |    9211 MB |  107247 GB |  107241 GB |\n",
            "|       from large pool |    6782 MB |    9170 MB |  106442 GB |  106436 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     805 GB |     805 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6824 MB |    9211 MB |  107247 GB |  107241 GB |\n",
            "|       from large pool |    6782 MB |    9170 MB |  106442 GB |  106436 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     805 GB |     805 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12118 MB |   14190 MB |    6676 GB |    6664 GB |\n",
            "|       from large pool |   12074 MB |   14108 MB |    6575 GB |    6563 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     925 MB |    2906 MB |  121108 GB |  121107 GB |\n",
            "|       from large pool |     923 MB |    2903 MB |  120103 GB |  120103 GB |\n",
            "|       from small pool |       2 MB |       7 MB |    1004 GB |    1004 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12945 K  |   12944 K  |\n",
            "|       from large pool |     258    |     262    |    7460 K  |    7460 K  |\n",
            "|       from small pool |     349    |     496    |    5484 K  |    5484 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12945 K  |   12944 K  |\n",
            "|       from large pool |     258    |     262    |    7460 K  |    7460 K  |\n",
            "|       from small pool |     349    |     496    |    5484 K  |    5484 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      96    |     118    |   56109    |   56013    |\n",
            "|       from large pool |      74    |      77    |    4112    |    4038    |\n",
            "|       from small pool |      22    |      41    |   51997    |   51975    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      69    |      74    |    7295 K  |    7295 K  |\n",
            "|       from large pool |      58    |      60    |    4251 K  |    4250 K  |\n",
            "|       from small pool |      11    |      22    |    3044 K  |    3044 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  71% 1216/1720 [06:26<02:25,  3.45it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.75 GiB (GPU 0; 14.76 GiB total capacity; 9.56 GiB already allocated; 3.42 GiB free; 10.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:24 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1682         |        cudaMalloc retries: 2740      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9753 MB |    9830 MB |  107274 GB |  107265 GB |\n",
            "|       from large pool |    9706 MB |    9783 MB |  106469 GB |  106460 GB |\n",
            "|       from small pool |      46 MB |      75 MB |     805 GB |     805 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9753 MB |    9830 MB |  107274 GB |  107265 GB |\n",
            "|       from large pool |    9706 MB |    9783 MB |  106469 GB |  106460 GB |\n",
            "|       from small pool |      46 MB |      75 MB |     805 GB |     805 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10782 MB |   10812 MB |    6679 GB |    6669 GB |\n",
            "|       from large pool |   10734 MB |   10734 MB |    6578 GB |    6567 GB |\n",
            "|       from small pool |      48 MB |      78 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1028 MB |    3549 MB |  121135 GB |  121134 GB |\n",
            "|       from large pool |    1027 MB |    3547 MB |  120130 GB |  120129 GB |\n",
            "|       from small pool |       1 MB |       4 MB |    1005 GB |    1005 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     611    |   12947 K  |   12946 K  |\n",
            "|       from large pool |     261    |     263    |    7461 K  |    7461 K  |\n",
            "|       from small pool |     347    |     496    |    5485 K  |    5485 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     611    |   12947 K  |   12946 K  |\n",
            "|       from large pool |     261    |     263    |    7461 K  |    7461 K  |\n",
            "|       from small pool |     347    |     496    |    5485 K  |    5485 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      99    |     114    |   56128    |   56029    |\n",
            "|       from large pool |      75    |      75    |    4114    |    4039    |\n",
            "|       from small pool |      24    |      39    |   52014    |   51990    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      93    |      94    |    7296 K  |    7296 K  |\n",
            "|       from large pool |      81    |      82    |    4251 K  |    4251 K  |\n",
            "|       from small pool |      12    |      20    |    3044 K  |    3044 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  71% 1218/1720 [06:26<02:50,  2.95it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:24 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.73 GiB (GPU 0; 14.76 GiB total capacity; 10.55 GiB already allocated; 2.18 GiB free; 11.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:24 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1683         |        cudaMalloc retries: 2742      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8012 MB |   10807 MB |  107301 GB |  107293 GB |\n",
            "|       from large pool |    7969 MB |   10764 MB |  106496 GB |  106488 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     805 GB |     805 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8012 MB |   10807 MB |  107301 GB |  107293 GB |\n",
            "|       from large pool |    7969 MB |   10764 MB |  106496 GB |  106488 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     805 GB |     805 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12060 MB |   12386 MB |    6683 GB |    6672 GB |\n",
            "|       from large pool |   12016 MB |   12308 MB |    6582 GB |    6570 GB |\n",
            "|       from small pool |      44 MB |      78 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1251 MB |    1855 MB |  121159 GB |  121158 GB |\n",
            "|       from large pool |    1250 MB |    1853 MB |  120154 GB |  120153 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1005 GB |    1005 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   12949 K  |   12948 K  |\n",
            "|       from large pool |     258    |     262    |    7462 K  |    7462 K  |\n",
            "|       from small pool |     350    |     496    |    5486 K  |    5485 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   12949 K  |   12948 K  |\n",
            "|       from large pool |     258    |     262    |    7462 K  |    7462 K  |\n",
            "|       from small pool |     350    |     496    |    5486 K  |    5485 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      97    |     115    |   56145    |   56048    |\n",
            "|       from large pool |      75    |      76    |    4116    |    4041    |\n",
            "|       from small pool |      22    |      39    |   52029    |   52007    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      77    |      78    |    7297 K  |    7297 K  |\n",
            "|       from large pool |      68    |      69    |    4252 K  |    4252 K  |\n",
            "|       from small pool |       9    |      18    |    3044 K  |    3044 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:24 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  71% 1220/1720 [06:27<02:54,  2.86it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.66 GiB (GPU 0; 14.76 GiB total capacity; 12.48 GiB already allocated; 799.75 MiB free; 13.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1684         |        cudaMalloc retries: 2743      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   12727 MB |   12848 MB |  107334 GB |  107322 GB |\n",
            "|       from large pool |   12677 MB |   12798 MB |  106529 GB |  106516 GB |\n",
            "|       from small pool |      49 MB |      75 MB |     805 GB |     805 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   12727 MB |   12848 MB |  107334 GB |  107322 GB |\n",
            "|       from large pool |   12677 MB |   12798 MB |  106529 GB |  106516 GB |\n",
            "|       from small pool |      49 MB |      75 MB |     805 GB |     805 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13488 MB |   13516 MB |    6688 GB |    6674 GB |\n",
            "|       from large pool |   13438 MB |   13438 MB |    6586 GB |    6573 GB |\n",
            "|       from small pool |      50 MB |      78 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  778780 KB |    3740 MB |  121188 GB |  121187 GB |\n",
            "|       from large pool |  778268 KB |    3740 MB |  120183 GB |  120182 GB |\n",
            "|       from small pool |     512 KB |       3 MB |    1005 GB |    1005 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     611    |   12951 K  |   12950 K  |\n",
            "|       from large pool |     261    |     263    |    7464 K  |    7463 K  |\n",
            "|       from small pool |     347    |     496    |    5487 K  |    5486 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     611    |   12951 K  |   12950 K  |\n",
            "|       from large pool |     261    |     263    |    7464 K  |    7463 K  |\n",
            "|       from small pool |     347    |     496    |    5487 K  |    5486 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     108    |     122    |   56171    |   56063    |\n",
            "|       from large pool |      83    |      83    |    4125    |    4042    |\n",
            "|       from small pool |      25    |      39    |   52046    |   52021    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      98    |     100    |    7298 K  |    7298 K  |\n",
            "|       from large pool |      85    |      87    |    4253 K  |    4253 K  |\n",
            "|       from small pool |      13    |      16    |    3045 K  |    3045 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  71% 1224/1720 [06:28<02:42,  3.04it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.02 GiB (GPU 0; 14.76 GiB total capacity; 11.32 GiB already allocated; 981.75 MiB free; 12.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1685         |        cudaMalloc retries: 2745      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8508 MB |   11594 MB |  107373 GB |  107365 GB |\n",
            "|       from large pool |    8466 MB |   11552 MB |  106567 GB |  106559 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     805 GB |     805 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8508 MB |   11594 MB |  107373 GB |  107365 GB |\n",
            "|       from large pool |    8466 MB |   11552 MB |  106567 GB |  106559 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     805 GB |     805 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13306 MB |   13578 MB |    6691 GB |    6678 GB |\n",
            "|       from large pool |   13262 MB |   13438 MB |    6589 GB |    6576 GB |\n",
            "|       from small pool |      44 MB |     140 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1709 MB |    1798 MB |  121226 GB |  121225 GB |\n",
            "|       from large pool |    1707 MB |    1796 MB |  120221 GB |  120219 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1005 GB |    1005 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   12956 K  |   12955 K  |\n",
            "|       from large pool |     258    |     263    |    7466 K  |    7466 K  |\n",
            "|       from small pool |     350    |     496    |    5489 K  |    5488 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   12956 K  |   12955 K  |\n",
            "|       from large pool |     258    |     263    |    7466 K  |    7466 K  |\n",
            "|       from small pool |     350    |     496    |    5489 K  |    5488 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     104    |     153    |   56217    |   56113    |\n",
            "|       from large pool |      82    |      83    |    4126    |    4044    |\n",
            "|       from small pool |      22    |      70    |   52091    |   52069    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      72    |      73    |    7301 K  |    7301 K  |\n",
            "|       from large pool |      62    |      63    |    4255 K  |    4255 K  |\n",
            "|       from small pool |      10    |      18    |    3046 K  |    3046 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  71% 1225/1720 [06:29<02:48,  2.94it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:27 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.55 GiB (GPU 0; 14.76 GiB total capacity; 10.59 GiB already allocated; 2.76 GiB free; 11.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:27 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1686         |        cudaMalloc retries: 2746      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10802 MB |   11594 MB |  107387 GB |  107377 GB |\n",
            "|       from large pool |   10755 MB |   11552 MB |  106582 GB |  106571 GB |\n",
            "|       from small pool |      47 MB |      75 MB |     805 GB |     805 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10802 MB |   11594 MB |  107387 GB |  107377 GB |\n",
            "|       from large pool |   10755 MB |   11552 MB |  106582 GB |  106571 GB |\n",
            "|       from small pool |      47 MB |      75 MB |     805 GB |     805 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11458 MB |   13578 MB |    6692 GB |    6681 GB |\n",
            "|       from large pool |   11408 MB |   13438 MB |    6590 GB |    6579 GB |\n",
            "|       from small pool |      50 MB |     140 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  625811 KB |    3485 MB |  121239 GB |  121238 GB |\n",
            "|       from large pool |  623552 KB |    3485 MB |  120233 GB |  120232 GB |\n",
            "|       from small pool |    2259 KB |       3 MB |    1005 GB |    1005 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   12956 K  |   12956 K  |\n",
            "|       from large pool |     261    |     263    |    7467 K  |    7466 K  |\n",
            "|       from small pool |     347    |     496    |    5489 K  |    5489 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   12956 K  |   12956 K  |\n",
            "|       from large pool |     261    |     263    |    7467 K  |    7466 K  |\n",
            "|       from small pool |     347    |     496    |    5489 K  |    5489 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     118    |     153    |   56232    |   56114    |\n",
            "|       from large pool |      93    |      93    |    4138    |    4045    |\n",
            "|       from small pool |      25    |      70    |   52094    |   52069    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      93    |      94    |    7301 K  |    7301 K  |\n",
            "|       from large pool |      79    |      81    |    4255 K  |    4255 K  |\n",
            "|       from small pool |      14    |      18    |    3046 K  |    3046 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:27 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  72% 1239/1720 [06:33<02:29,  3.22it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.14 GiB (GPU 0; 14.76 GiB total capacity; 10.04 GiB already allocated; 1.28 GiB free; 12.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1687         |        cudaMalloc retries: 2749      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10285 MB |   10303 MB |  107532 GB |  107522 GB |\n",
            "|       from large pool |   10240 MB |   10259 MB |  106724 GB |  106714 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     807 GB |     807 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10285 MB |   10303 MB |  107532 GB |  107522 GB |\n",
            "|       from large pool |   10240 MB |   10259 MB |  106724 GB |  106714 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     807 GB |     807 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12972 MB |   13006 MB |    6700 GB |    6688 GB |\n",
            "|       from large pool |   12926 MB |   12926 MB |    6598 GB |    6586 GB |\n",
            "|       from small pool |      46 MB |      80 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2686 MB |    2735 MB |  121409 GB |  121407 GB |\n",
            "|       from large pool |    2685 MB |    2734 MB |  120401 GB |  120398 GB |\n",
            "|       from small pool |       1 MB |       5 MB |    1008 GB |    1008 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     611    |   12976 K  |   12975 K  |\n",
            "|       from large pool |     261    |     263    |    7476 K  |    7476 K  |\n",
            "|       from small pool |     346    |     496    |    5499 K  |    5498 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     611    |   12976 K  |   12975 K  |\n",
            "|       from large pool |     261    |     263    |    7476 K  |    7476 K  |\n",
            "|       from small pool |     346    |     496    |    5499 K  |    5498 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     105    |     122    |   56344    |   56239    |\n",
            "|       from large pool |      82    |      82    |    4142    |    4060    |\n",
            "|       from small pool |      23    |      40    |   52202    |   52179    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      74    |      74    |    7313 K  |    7313 K  |\n",
            "|       from large pool |      59    |      59    |    4261 K  |    4261 K  |\n",
            "|       from small pool |      15    |      22    |    3052 K  |    3052 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  72% 1240/1720 [06:34<02:29,  3.20it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.45 GiB (GPU 0; 14.76 GiB total capacity; 10.28 GiB already allocated; 1.17 GiB free; 12.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:32 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1688         |        cudaMalloc retries: 2751      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8022 MB |   10526 MB |  107543 GB |  107535 GB |\n",
            "|       from large pool |    7980 MB |   10484 MB |  106735 GB |  106727 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     807 GB |     807 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8022 MB |   10526 MB |  107543 GB |  107535 GB |\n",
            "|       from large pool |    7980 MB |   10484 MB |  106735 GB |  106727 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     807 GB |     807 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13086 MB |   13086 MB |    6703 GB |    6690 GB |\n",
            "|       from large pool |   13042 MB |   13042 MB |    6601 GB |    6588 GB |\n",
            "|       from small pool |      44 MB |      80 MB |     101 GB |     101 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2559 MB |    2776 MB |  121419 GB |  121417 GB |\n",
            "|       from large pool |    2557 MB |    2774 MB |  120411 GB |  120408 GB |\n",
            "|       from small pool |       1 MB |       5 MB |    1008 GB |    1008 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   12976 K  |   12976 K  |\n",
            "|       from large pool |     258    |     263    |    7477 K  |    7477 K  |\n",
            "|       from small pool |     349    |     496    |    5499 K  |    5498 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   12976 K  |   12976 K  |\n",
            "|       from large pool |     258    |     263    |    7477 K  |    7477 K  |\n",
            "|       from small pool |     349    |     496    |    5499 K  |    5498 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     104    |     122    |   56345    |   56241    |\n",
            "|       from large pool |      82    |      82    |    4143    |    4061    |\n",
            "|       from small pool |      22    |      40    |   52202    |   52180    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      70    |      75    |    7314 K  |    7314 K  |\n",
            "|       from large pool |      60    |      60    |    4261 K  |    4261 K  |\n",
            "|       from small pool |      10    |      22    |    3052 K  |    3052 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  72% 1245/1720 [06:35<02:43,  2.91it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.65 GiB (GPU 0; 14.76 GiB total capacity; 12.08 GiB already allocated; 1.21 GiB free; 12.74 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:34 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1689         |        cudaMalloc retries: 2754      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   12373 MB |   12413 MB |  107605 GB |  107593 GB |\n",
            "|       from large pool |   12327 MB |   12367 MB |  106796 GB |  106784 GB |\n",
            "|       from small pool |      46 MB |      75 MB |     808 GB |     808 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   12373 MB |   12413 MB |  107605 GB |  107593 GB |\n",
            "|       from large pool |   12327 MB |   12367 MB |  106796 GB |  106784 GB |\n",
            "|       from small pool |      46 MB |      75 MB |     808 GB |     808 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13048 MB |   13096 MB |    6713 GB |    6700 GB |\n",
            "|       from large pool |   13000 MB |   13048 MB |    6611 GB |    6598 GB |\n",
            "|       from small pool |      48 MB |     200 MB |     102 GB |     102 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  690720 KB |    2907 MB |  121481 GB |  121480 GB |\n",
            "|       from large pool |  688708 KB |    2907 MB |  120471 GB |  120471 GB |\n",
            "|       from small pool |    2012 KB |      14 MB |    1009 GB |    1009 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     611    |   12983 K  |   12982 K  |\n",
            "|       from large pool |     261    |     263    |    7480 K  |    7480 K  |\n",
            "|       from small pool |     346    |     496    |    5502 K  |    5502 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     611    |   12983 K  |   12982 K  |\n",
            "|       from large pool |     261    |     263    |    7480 K  |    7480 K  |\n",
            "|       from small pool |     346    |     496    |    5502 K  |    5502 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |     101    |     179    |   56446    |   56345    |\n",
            "|       from large pool |      77    |      79    |    4147    |    4070    |\n",
            "|       from small pool |      24    |     100    |   52299    |   52275    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |     102    |     105    |    7318 K  |    7318 K  |\n",
            "|       from large pool |      85    |      88    |    4263 K  |    4263 K  |\n",
            "|       from small pool |      17    |      33    |    3054 K  |    3054 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  73% 1261/1720 [06:41<02:53,  2.65it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.45 GiB (GPU 0; 14.76 GiB total capacity; 9.81 GiB already allocated; 1.16 GiB free; 12.79 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:39 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1690         |        cudaMalloc retries: 2756      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7538 MB |   10042 MB |  107811 GB |  107804 GB |\n",
            "|       from large pool |    7496 MB |   10000 MB |  107001 GB |  106994 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     809 GB |     809 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7538 MB |   10042 MB |  107811 GB |  107804 GB |\n",
            "|       from large pool |    7496 MB |   10000 MB |  107001 GB |  106994 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     809 GB |     809 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13102 MB |   13102 MB |    6715 GB |    6703 GB |\n",
            "|       from large pool |   13058 MB |   13058 MB |    6613 GB |    6600 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     102 GB |     102 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3059 MB |    3059 MB |  121722 GB |  121719 GB |\n",
            "|       from large pool |    3057 MB |    3057 MB |  120711 GB |  120708 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1010 GB |    1010 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13006 K  |   13005 K  |\n",
            "|       from large pool |     258    |     262    |    7494 K  |    7494 K  |\n",
            "|       from small pool |     349    |     496    |    5511 K  |    5511 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13006 K  |   13005 K  |\n",
            "|       from large pool |     258    |     262    |    7494 K  |    7494 K  |\n",
            "|       from small pool |     349    |     496    |    5511 K  |    5511 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      99    |     118    |   56464    |   56365    |\n",
            "|       from large pool |      77    |      77    |    4148    |    4071    |\n",
            "|       from small pool |      22    |      41    |   52316    |   52294    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      77    |      79    |    7331 K  |    7331 K  |\n",
            "|       from large pool |      67    |      67    |    4272 K  |    4272 K  |\n",
            "|       from small pool |      10    |      18    |    3059 K  |    3059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  73% 1262/1720 [06:42<02:41,  2.83it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.73 GiB (GPU 0; 14.76 GiB total capacity; 10.59 GiB already allocated; 893.75 MiB free; 13.08 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:39 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1691         |        cudaMalloc retries: 2757      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8048 MB |   10843 MB |  107822 GB |  107815 GB |\n",
            "|       from large pool |    8006 MB |   10800 MB |  107013 GB |  107005 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     809 GB |     809 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8048 MB |   10843 MB |  107822 GB |  107815 GB |\n",
            "|       from large pool |    8006 MB |   10800 MB |  107013 GB |  107005 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     809 GB |     809 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13394 MB |   13394 MB |    6718 GB |    6705 GB |\n",
            "|       from large pool |   13350 MB |   13350 MB |    6616 GB |    6603 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     102 GB |     102 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2549 MB |    3059 MB |  121730 GB |  121728 GB |\n",
            "|       from large pool |    2547 MB |    3057 MB |  120720 GB |  120717 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1010 GB |    1010 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13006 K  |   13006 K  |\n",
            "|       from large pool |     258    |     263    |    7495 K  |    7494 K  |\n",
            "|       from small pool |     349    |     496    |    5511 K  |    5511 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13006 K  |   13006 K  |\n",
            "|       from large pool |     258    |     263    |    7495 K  |    7494 K  |\n",
            "|       from small pool |     349    |     496    |    5511 K  |    5511 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      99    |     118    |   56465    |   56366    |\n",
            "|       from large pool |      77    |      77    |    4149    |    4072    |\n",
            "|       from small pool |      22    |      41    |   52316    |   52294    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      79    |    7332 K  |    7331 K  |\n",
            "|       from large pool |      58    |      67    |    4272 K  |    4272 K  |\n",
            "|       from small pool |      10    |      18    |    3059 K  |    3059 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  74% 1270/1720 [06:44<01:59,  3.77it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.58 GiB (GPU 0; 14.76 GiB total capacity; 12.09 GiB already allocated; 397.75 MiB free; 13.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:42 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1692         |        cudaMalloc retries: 2760      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8708 MB |   12377 MB |  107900 GB |  107892 GB |\n",
            "|       from large pool |    8666 MB |   12334 MB |  107090 GB |  107081 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     810 GB |     810 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8708 MB |   12377 MB |  107900 GB |  107892 GB |\n",
            "|       from large pool |    8666 MB |   12334 MB |  107090 GB |  107081 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     810 GB |     810 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13890 MB |   13890 MB |    6727 GB |    6714 GB |\n",
            "|       from large pool |   13846 MB |   13846 MB |    6625 GB |    6611 GB |\n",
            "|       from small pool |      44 MB |     200 MB |     102 GB |     102 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1511 MB |    2123 MB |  121817 GB |  121815 GB |\n",
            "|       from large pool |    1509 MB |    2121 MB |  120805 GB |  120804 GB |\n",
            "|       from small pool |       1 MB |      12 MB |    1011 GB |    1011 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     614    |   13017 K  |   13017 K  |\n",
            "|       from large pool |     258    |     262    |    7500 K  |    7500 K  |\n",
            "|       from small pool |     350    |     496    |    5516 K  |    5516 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     614    |   13017 K  |   13017 K  |\n",
            "|       from large pool |     258    |     262    |    7500 K  |    7500 K  |\n",
            "|       from small pool |     350    |     496    |    5516 K  |    5516 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      99    |     177    |   56546    |   56447    |\n",
            "|       from large pool |      77    |      77    |    4152    |    4075    |\n",
            "|       from small pool |      22    |     100    |   52394    |   52372    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      53    |    7338 K  |    7338 K  |\n",
            "|       from large pool |      43    |      44    |    4276 K  |    4276 K  |\n",
            "|       from small pool |       7    |      25    |    3062 K  |    3062 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  74% 1273/1720 [06:45<01:49,  4.10it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.05 GiB (GPU 0; 14.76 GiB total capacity; 8.69 GiB already allocated; 1.93 GiB free; 12.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:42 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1693         |        cudaMalloc retries: 2761      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6806 MB |    8903 MB |  107917 GB |  107910 GB |\n",
            "|       from large pool |    6765 MB |    8861 MB |  107105 GB |  107099 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     811 GB |     811 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6806 MB |    8903 MB |  107917 GB |  107910 GB |\n",
            "|       from large pool |    6765 MB |    8861 MB |  107105 GB |  107099 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     811 GB |     811 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12316 MB |   12474 MB |    6729 GB |    6717 GB |\n",
            "|       from large pool |   12274 MB |   12274 MB |    6627 GB |    6615 GB |\n",
            "|       from small pool |      42 MB |     200 MB |     102 GB |     102 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3411 MB |    3412 MB |  121836 GB |  121833 GB |\n",
            "|       from large pool |    3410 MB |    3412 MB |  120824 GB |  120821 GB |\n",
            "|       from small pool |       0 MB |      16 MB |    1012 GB |    1012 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   13021 K  |   13020 K  |\n",
            "|       from large pool |     258    |     262    |    7502 K  |    7501 K  |\n",
            "|       from small pool |     350    |     496    |    5518 K  |    5518 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   13021 K  |   13020 K  |\n",
            "|       from large pool |     258    |     262    |    7502 K  |    7501 K  |\n",
            "|       from small pool |     350    |     496    |    5518 K  |    5518 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      98    |     177    |   56625    |   56527    |\n",
            "|       from large pool |      77    |      77    |    4153    |    4076    |\n",
            "|       from small pool |      21    |     100    |   52472    |   52451    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      45    |      47    |    7341 K  |    7341 K  |\n",
            "|       from large pool |      36    |      37    |    4277 K  |    4277 K  |\n",
            "|       from small pool |       9    |      33    |    3063 K  |    3063 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  74% 1275/1720 [06:45<01:59,  3.72it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.56 GiB (GPU 0; 14.76 GiB total capacity; 10.12 GiB already allocated; 1.41 GiB free; 12.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:43 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1694         |        cudaMalloc retries: 2762      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7743 MB |   10363 MB |  107941 GB |  107933 GB |\n",
            "|       from large pool |    7701 MB |   10321 MB |  107129 GB |  107122 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     811 GB |     811 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7743 MB |   10363 MB |  107941 GB |  107933 GB |\n",
            "|       from large pool |    7701 MB |   10321 MB |  107129 GB |  107122 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     811 GB |     811 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12842 MB |   12876 MB |    6732 GB |    6719 GB |\n",
            "|       from large pool |   12798 MB |   12798 MB |    6629 GB |    6617 GB |\n",
            "|       from small pool |      44 MB |      78 MB |     102 GB |     102 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1428 MB |    3788 MB |  121863 GB |  121862 GB |\n",
            "|       from large pool |    1426 MB |    3786 MB |  120851 GB |  120849 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1012 GB |    1012 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13023 K  |   13022 K  |\n",
            "|       from large pool |     258    |     262    |    7503 K  |    7503 K  |\n",
            "|       from small pool |     349    |     496    |    5519 K  |    5519 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13023 K  |   13022 K  |\n",
            "|       from large pool |     258    |     262    |    7503 K  |    7503 K  |\n",
            "|       from small pool |     349    |     496    |    5519 K  |    5519 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      99    |     116    |   56644    |   56545    |\n",
            "|       from large pool |      77    |      77    |    4154    |    4077    |\n",
            "|       from small pool |      22    |      39    |   52490    |   52468    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      72    |      75    |    7342 K  |    7342 K  |\n",
            "|       from large pool |      64    |      65    |    4278 K  |    4278 K  |\n",
            "|       from small pool |       8    |      17    |    3063 K  |    3063 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  75% 1284/1720 [06:48<02:33,  2.85it/s, loss=4.639, nll_loss=4.639, ppl=24.92, wps=13037.1, ups=2.57, wpb=5077.7, bsz=256, num_updates=8100, lr=0.0001, gnorm=1.722, loss_scale=4, train_wall=32, gb_free=9.2, wall=3161]2022-07-19 07:13:46 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.09 GiB (GPU 0; 14.76 GiB total capacity; 11.03 GiB already allocated; 255.75 MiB free; 13.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:46 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1695         |        cudaMalloc retries: 2764      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   11290 MB |   11324 MB |  108049 GB |  108038 GB |\n",
            "|       from large pool |   11244 MB |   11279 MB |  107237 GB |  107226 GB |\n",
            "|       from small pool |      45 MB |      75 MB |     812 GB |     811 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   11290 MB |   11324 MB |  108049 GB |  108038 GB |\n",
            "|       from large pool |   11244 MB |   11279 MB |  107237 GB |  107226 GB |\n",
            "|       from small pool |      45 MB |      75 MB |     812 GB |     811 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14032 MB |   14114 MB |    6742 GB |    6728 GB |\n",
            "|       from large pool |   13986 MB |   14034 MB |    6639 GB |    6626 GB |\n",
            "|       from small pool |      46 MB |      80 MB |     102 GB |     102 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2741 MB |    3027 MB |  121964 GB |  121962 GB |\n",
            "|       from large pool |    2741 MB |    3026 MB |  120951 GB |  120949 GB |\n",
            "|       from small pool |       0 MB |       9 MB |    1012 GB |    1012 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     611    |   13035 K  |   13034 K  |\n",
            "|       from large pool |     261    |     263    |    7511 K  |    7510 K  |\n",
            "|       from small pool |     346    |     496    |    5524 K  |    5524 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     611    |   13035 K  |   13034 K  |\n",
            "|       from large pool |     261    |     263    |    7511 K  |    7510 K  |\n",
            "|       from small pool |     346    |     496    |    5524 K  |    5524 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      92    |     111    |   56688    |   56596    |\n",
            "|       from large pool |      69    |      71    |    4159    |    4090    |\n",
            "|       from small pool |      23    |      40    |   52529    |   52506    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      81    |      84    |    7349 K  |    7349 K  |\n",
            "|       from large pool |      67    |      71    |    4283 K  |    4283 K  |\n",
            "|       from small pool |      14    |      24    |    3066 K  |    3066 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:46 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  76% 1300/1720 [06:53<02:22,  2.94it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200] 2022-07-19 07:13:51 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.13 GiB (GPU 0; 14.76 GiB total capacity; 9.21 GiB already allocated; 3.09 GiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:51 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1696         |        cudaMalloc retries: 2767      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9425 MB |   11056 MB |  108227 GB |  108218 GB |\n",
            "|       from large pool |    9382 MB |   11013 MB |  107414 GB |  107405 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     813 GB |     813 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9425 MB |   11056 MB |  108227 GB |  108218 GB |\n",
            "|       from large pool |    9382 MB |   11013 MB |  107414 GB |  107405 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     813 GB |     813 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11122 MB |   13744 MB |    6749 GB |    6738 GB |\n",
            "|       from large pool |   11078 MB |   13700 MB |    6646 GB |    6636 GB |\n",
            "|       from small pool |      44 MB |      80 MB |     102 GB |     102 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1696 MB |    2716 MB |  122171 GB |  122170 GB |\n",
            "|       from large pool |    1695 MB |    2716 MB |  121156 GB |  121154 GB |\n",
            "|       from small pool |       0 MB |       3 MB |    1015 GB |    1015 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     604    |     610    |   13058 K  |   13057 K  |\n",
            "|       from large pool |     258    |     263    |    7523 K  |    7523 K  |\n",
            "|       from small pool |     346    |     496    |    5534 K  |    5534 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     604    |     610    |   13058 K  |   13057 K  |\n",
            "|       from large pool |     258    |     263    |    7523 K  |    7523 K  |\n",
            "|       from small pool |     346    |     496    |    5534 K  |    5534 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      82    |     101    |   56775    |   56693    |\n",
            "|       from large pool |      60    |      61    |    4162    |    4102    |\n",
            "|       from small pool |      22    |      40    |   52613    |   52591    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      40    |      53    |    7363 K  |    7363 K  |\n",
            "|       from large pool |      32    |      45    |    4291 K  |    4291 K  |\n",
            "|       from small pool |       8    |      17    |    3072 K  |    3072 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:51 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  76% 1309/1720 [06:56<01:37,  4.20it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:13:54 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.75 GiB (GPU 0; 14.76 GiB total capacity; 8.36 GiB already allocated; 3.14 GiB free; 10.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:54 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1697         |        cudaMalloc retries: 2768      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8564 MB |    8598 MB |  108307 GB |  108299 GB |\n",
            "|       from large pool |    8520 MB |    8554 MB |  107493 GB |  107485 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     814 GB |     814 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8564 MB |    8598 MB |  108307 GB |  108299 GB |\n",
            "|       from large pool |    8520 MB |    8554 MB |  107493 GB |  107485 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     814 GB |     814 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11074 MB |   11162 MB |    6749 GB |    6738 GB |\n",
            "|       from large pool |   11030 MB |   11078 MB |    6646 GB |    6636 GB |\n",
            "|       from small pool |      44 MB |      84 MB |     102 GB |     102 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2509 MB |    2523 MB |  122269 GB |  122266 GB |\n",
            "|       from large pool |    2509 MB |    2523 MB |  121253 GB |  121250 GB |\n",
            "|       from small pool |       0 MB |       9 MB |    1015 GB |    1015 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     610    |   13070 K  |   13070 K  |\n",
            "|       from large pool |     261    |     263    |    7530 K  |    7530 K  |\n",
            "|       from small pool |     345    |     496    |    5539 K  |    5539 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     610    |   13070 K  |   13070 K  |\n",
            "|       from large pool |     261    |     263    |    7530 K  |    7530 K  |\n",
            "|       from small pool |     345    |     496    |    5539 K  |    5539 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      80    |     102    |   56795    |   56715    |\n",
            "|       from large pool |      58    |      60    |    4162    |    4104    |\n",
            "|       from small pool |      22    |      42    |   52633    |   52611    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      61    |      63    |    7371 K  |    7371 K  |\n",
            "|       from large pool |      48    |      51    |    4296 K  |    4295 K  |\n",
            "|       from small pool |      13    |      30    |    3075 K  |    3075 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:54 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  77% 1316/1720 [06:58<02:13,  3.02it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:13:56 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.43 GiB (GPU 0; 14.76 GiB total capacity; 10.49 GiB already allocated; 1.06 GiB free; 12.89 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:56 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1698         |        cudaMalloc retries: 2770      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10737 MB |   10757 MB |  108396 GB |  108385 GB |\n",
            "|       from large pool |   10692 MB |   10713 MB |  107581 GB |  107571 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     814 GB |     814 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10737 MB |   10757 MB |  108396 GB |  108385 GB |\n",
            "|       from large pool |   10692 MB |   10713 MB |  107581 GB |  107571 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     814 GB |     814 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13200 MB |   13200 MB |    6754 GB |    6742 GB |\n",
            "|       from large pool |   13154 MB |   13154 MB |    6652 GB |    6639 GB |\n",
            "|       from small pool |      46 MB |      80 MB |     102 GB |     102 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2462 MB |    2515 MB |  122375 GB |  122372 GB |\n",
            "|       from large pool |    2461 MB |    2514 MB |  121358 GB |  121356 GB |\n",
            "|       from small pool |       1 MB |       9 MB |    1016 GB |    1016 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     611    |   13080 K  |   13079 K  |\n",
            "|       from large pool |     261    |     263    |    7536 K  |    7536 K  |\n",
            "|       from small pool |     346    |     496    |    5543 K  |    5543 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     611    |   13080 K  |   13079 K  |\n",
            "|       from large pool |     261    |     263    |    7536 K  |    7536 K  |\n",
            "|       from small pool |     346    |     496    |    5543 K  |    5543 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      82    |      99    |   56815    |   56733    |\n",
            "|       from large pool |      59    |      59    |    4164    |    4105    |\n",
            "|       from small pool |      23    |      40    |   52651    |   52628    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      75    |      75    |    7377 K  |    7376 K  |\n",
            "|       from large pool |      64    |      64    |    4299 K  |    4299 K  |\n",
            "|       from small pool |      11    |      26    |    3077 K  |    3077 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:56 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  77% 1325/1720 [07:00<01:38,  3.99it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:13:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB (GPU 0; 14.76 GiB total capacity; 11.08 GiB already allocated; 391.75 MiB free; 13.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:13:58 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1699         |        cudaMalloc retries: 2772      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8488 MB |   11342 MB |  108470 GB |  108462 GB |\n",
            "|       from large pool |    8445 MB |   11299 MB |  107655 GB |  107646 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     815 GB |     815 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8488 MB |   11342 MB |  108470 GB |  108462 GB |\n",
            "|       from large pool |    8445 MB |   11299 MB |  107655 GB |  107646 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     815 GB |     815 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13896 MB |   13896 MB |    6757 GB |    6744 GB |\n",
            "|       from large pool |   13852 MB |   13852 MB |    6654 GB |    6641 GB |\n",
            "|       from small pool |      44 MB |     120 MB |     102 GB |     102 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2553 MB |    2553 MB |  122465 GB |  122463 GB |\n",
            "|       from large pool |    2552 MB |    2552 MB |  121448 GB |  121445 GB |\n",
            "|       from small pool |       1 MB |      21 MB |    1017 GB |    1017 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13092 K  |   13091 K  |\n",
            "|       from large pool |     258    |     263    |    7543 K  |    7543 K  |\n",
            "|       from small pool |     349    |     496    |    5549 K  |    5548 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13092 K  |   13091 K  |\n",
            "|       from large pool |     258    |     263    |    7543 K  |    7543 K  |\n",
            "|       from small pool |     349    |     496    |    5549 K  |    5548 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      81    |     119    |   56853    |   56772    |\n",
            "|       from large pool |      59    |      59    |    4165    |    4106    |\n",
            "|       from small pool |      22    |      60    |   52688    |   52666    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      52    |    7384 K  |    7384 K  |\n",
            "|       from large pool |      40    |      43    |    4303 K  |    4303 K  |\n",
            "|       from small pool |      10    |      33    |    3080 K  |    3080 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:13:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  78% 1335/1720 [07:03<02:09,  2.98it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:14:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.81 GiB (GPU 0; 14.76 GiB total capacity; 12.50 GiB already allocated; 765.75 MiB free; 13.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:02 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1700         |        cudaMalloc retries: 2776      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8900 MB |   12802 MB |  108577 GB |  108569 GB |\n",
            "|       from large pool |    8853 MB |   12755 MB |  107761 GB |  107752 GB |\n",
            "|       from small pool |      46 MB |      75 MB |     816 GB |     816 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8900 MB |   12802 MB |  108577 GB |  108569 GB |\n",
            "|       from large pool |    8853 MB |   12755 MB |  107761 GB |  107752 GB |\n",
            "|       from small pool |      46 MB |      75 MB |     816 GB |     816 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13522 MB |   13932 MB |    6770 GB |    6757 GB |\n",
            "|       from large pool |   13474 MB |   13852 MB |    6667 GB |    6654 GB |\n",
            "|       from small pool |      48 MB |      80 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  737257 KB |    2036 MB |  122583 GB |  122583 GB |\n",
            "|       from large pool |  735484 KB |    2032 MB |  121565 GB |  121564 GB |\n",
            "|       from small pool |    1773 KB |       9 MB |    1018 GB |    1018 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     614    |   13106 K  |   13105 K  |\n",
            "|       from large pool |     252    |     256    |    7551 K  |    7550 K  |\n",
            "|       from small pool |     356    |     496    |    5555 K  |    5554 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     614    |   13106 K  |   13105 K  |\n",
            "|       from large pool |     252    |     256    |    7551 K  |    7550 K  |\n",
            "|       from small pool |     356    |     496    |    5555 K  |    5554 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      82    |      99    |   56945    |   56863    |\n",
            "|       from large pool |      58    |      59    |    4169    |    4111    |\n",
            "|       from small pool |      24    |      40    |   52776    |   52752    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      38    |      54    |    7392 K  |    7392 K  |\n",
            "|       from large pool |      23    |      35    |    4308 K  |    4308 K  |\n",
            "|       from small pool |      15    |      30    |    3083 K  |    3083 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  78% 1338/1720 [07:05<02:05,  3.05it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:14:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.03 GiB (GPU 0; 14.76 GiB total capacity; 9.14 GiB already allocated; 707.75 MiB free; 13.26 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:03 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1701         |        cudaMalloc retries: 2777      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9315 MB |    9422 MB |  108609 GB |  108600 GB |\n",
            "|       from large pool |    9268 MB |    9376 MB |  107792 GB |  107783 GB |\n",
            "|       from small pool |      46 MB |      75 MB |     817 GB |     817 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9315 MB |    9422 MB |  108609 GB |  108600 GB |\n",
            "|       from large pool |    9268 MB |    9376 MB |  107792 GB |  107783 GB |\n",
            "|       from small pool |      46 MB |      75 MB |     817 GB |     817 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13580 MB |   13722 MB |    6774 GB |    6761 GB |\n",
            "|       from large pool |   13532 MB |   13532 MB |    6671 GB |    6657 GB |\n",
            "|       from small pool |      48 MB |     190 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    4264 MB |    4334 MB |  122619 GB |  122615 GB |\n",
            "|       from large pool |    4263 MB |    4332 MB |  121600 GB |  121596 GB |\n",
            "|       from small pool |       1 MB |      54 MB |    1019 GB |    1019 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     611    |   13109 K  |   13109 K  |\n",
            "|       from large pool |     261    |     263    |    7552 K  |    7552 K  |\n",
            "|       from small pool |     347    |     496    |    5557 K  |    5556 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     611    |   13109 K  |   13109 K  |\n",
            "|       from large pool |     261    |     263    |    7552 K  |    7552 K  |\n",
            "|       from small pool |     347    |     496    |    5557 K  |    5556 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      83    |     154    |   57018    |   56935    |\n",
            "|       from large pool |      59    |      59    |    4171    |    4112    |\n",
            "|       from small pool |      24    |      95    |   52847    |   52823    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      76    |      77    |    7394 K  |    7394 K  |\n",
            "|       from large pool |      60    |      62    |    4309 K  |    4309 K  |\n",
            "|       from small pool |      16    |      52    |    3085 K  |    3085 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  78% 1339/1720 [07:05<02:12,  2.89it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:14:03 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 8.25 GiB (GPU 0; 14.76 GiB total capacity; 11.99 GiB already allocated; 389.75 MiB free; 13.57 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:03 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1702         |        cudaMalloc retries: 2779      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   12273 MB |   12286 MB |  108623 GB |  108611 GB |\n",
            "|       from large pool |   12228 MB |   12240 MB |  107806 GB |  107794 GB |\n",
            "|       from small pool |      45 MB |      75 MB |     817 GB |     817 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   12273 MB |   12286 MB |  108623 GB |  108611 GB |\n",
            "|       from large pool |   12228 MB |   12240 MB |  107806 GB |  107794 GB |\n",
            "|       from small pool |      45 MB |      75 MB |     817 GB |     817 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13898 MB |   13898 MB |    6778 GB |    6764 GB |\n",
            "|       from large pool |   13852 MB |   13852 MB |    6675 GB |    6661 GB |\n",
            "|       from small pool |      46 MB |     190 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1624 MB |    4334 MB |  122636 GB |  122635 GB |\n",
            "|       from large pool |    1623 MB |    4332 MB |  121617 GB |  121616 GB |\n",
            "|       from small pool |       0 MB |      54 MB |    1019 GB |    1019 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     611    |   13110 K  |   13109 K  |\n",
            "|       from large pool |     261    |     263    |    7552 K  |    7552 K  |\n",
            "|       from small pool |     346    |     496    |    5557 K  |    5556 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     611    |   13110 K  |   13109 K  |\n",
            "|       from large pool |     261    |     263    |    7552 K  |    7552 K  |\n",
            "|       from small pool |     346    |     496    |    5557 K  |    5556 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      82    |     154    |   57019    |   56937    |\n",
            "|       from large pool |      59    |      59    |    4172    |    4113    |\n",
            "|       from small pool |      23    |      95    |   52847    |   52824    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      79    |    7394 K  |    7394 K  |\n",
            "|       from large pool |      53    |      62    |    4309 K  |    4309 K  |\n",
            "|       from small pool |      15    |      52    |    3085 K  |    3085 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:03 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  78% 1344/1720 [07:07<01:57,  3.20it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:14:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.41 GiB (GPU 0; 14.76 GiB total capacity; 8.95 GiB already allocated; 2.32 GiB free; 11.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1703         |        cudaMalloc retries: 2780      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9160 MB |   10939 MB |  108676 GB |  108667 GB |\n",
            "|       from large pool |    9118 MB |   10896 MB |  107859 GB |  107850 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     817 GB |     817 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9160 MB |   10939 MB |  108676 GB |  108667 GB |\n",
            "|       from large pool |    9118 MB |   10896 MB |  107859 GB |  107850 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     817 GB |     817 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11916 MB |   13934 MB |    6778 GB |    6766 GB |\n",
            "|       from large pool |   11872 MB |   13852 MB |    6675 GB |    6663 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2755 MB |    2755 MB |  122695 GB |  122693 GB |\n",
            "|       from large pool |    2753 MB |    2753 MB |  121676 GB |  121673 GB |\n",
            "|       from small pool |       1 MB |      11 MB |    1019 GB |    1019 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13116 K  |   13116 K  |\n",
            "|       from large pool |     258    |     263    |    7556 K  |    7556 K  |\n",
            "|       from small pool |     349    |     496    |    5559 K  |    5559 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13116 K  |   13116 K  |\n",
            "|       from large pool |     258    |     263    |    7556 K  |    7556 K  |\n",
            "|       from small pool |     349    |     496    |    5559 K  |    5559 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      80    |     100    |   57037    |   56957    |\n",
            "|       from large pool |      58    |      59    |    4172    |    4114    |\n",
            "|       from small pool |      22    |      41    |   52865    |   52843    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      73    |      74    |    7398 K  |    7398 K  |\n",
            "|       from large pool |      64    |      64    |    4312 K  |    4312 K  |\n",
            "|       from small pool |       9    |      29    |    3086 K  |    3086 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  78% 1345/1720 [07:07<01:57,  3.19it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:14:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.53 GiB (GPU 0; 14.76 GiB total capacity; 8.60 GiB already allocated; 2.31 GiB free; 11.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1704         |        cudaMalloc retries: 2781      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8778 MB |   10939 MB |  108687 GB |  108678 GB |\n",
            "|       from large pool |    8732 MB |   10896 MB |  107869 GB |  107861 GB |\n",
            "|       from small pool |      45 MB |      75 MB |     817 GB |     817 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8778 MB |   10939 MB |  108687 GB |  108678 GB |\n",
            "|       from large pool |    8732 MB |   10896 MB |  107869 GB |  107861 GB |\n",
            "|       from small pool |      45 MB |      75 MB |     817 GB |     817 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11920 MB |   13934 MB |    6778 GB |    6766 GB |\n",
            "|       from large pool |   11872 MB |   13852 MB |    6675 GB |    6663 GB |\n",
            "|       from small pool |      48 MB |      82 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3141 MB |    4662 MB |  122710 GB |  122707 GB |\n",
            "|       from large pool |    3139 MB |    4660 MB |  121691 GB |  121688 GB |\n",
            "|       from small pool |       2 MB |      11 MB |    1019 GB |    1019 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   13117 K  |   13116 K  |\n",
            "|       from large pool |     261    |     263    |    7557 K  |    7557 K  |\n",
            "|       from small pool |     347    |     496    |    5559 K  |    5559 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   13117 K  |   13116 K  |\n",
            "|       from large pool |     261    |     263    |    7557 K  |    7557 K  |\n",
            "|       from small pool |     347    |     496    |    5559 K  |    5559 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      82    |     100    |   57039    |   56957    |\n",
            "|       from large pool |      58    |      59    |    4172    |    4114    |\n",
            "|       from small pool |      24    |      41    |   52867    |   52843    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      76    |      76    |    7399 K  |    7399 K  |\n",
            "|       from large pool |      64    |      64    |    4312 K  |    4312 K  |\n",
            "|       from small pool |      12    |      29    |    3086 K  |    3086 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  79% 1361/1720 [07:12<01:33,  3.85it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:14:10 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.84 GiB (GPU 0; 14.76 GiB total capacity; 9.63 GiB already allocated; 2.31 GiB free; 11.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:10 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1705         |        cudaMalloc retries: 2782      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9830 MB |    9908 MB |  108879 GB |  108870 GB |\n",
            "|       from large pool |    9783 MB |    9861 MB |  108061 GB |  108051 GB |\n",
            "|       from small pool |      46 MB |      75 MB |     818 GB |     818 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9830 MB |    9908 MB |  108879 GB |  108870 GB |\n",
            "|       from large pool |    9783 MB |    9861 MB |  108061 GB |  108051 GB |\n",
            "|       from small pool |      46 MB |      75 MB |     818 GB |     818 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11920 MB |   11956 MB |    6778 GB |    6766 GB |\n",
            "|       from large pool |   11872 MB |   11872 MB |    6675 GB |    6663 GB |\n",
            "|       from small pool |      48 MB |      84 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2089 MB |    4405 MB |  122947 GB |  122945 GB |\n",
            "|       from large pool |    2088 MB |    4404 MB |  121926 GB |  121924 GB |\n",
            "|       from small pool |       1 MB |      10 MB |    1020 GB |    1020 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     611    |   13140 K  |   13139 K  |\n",
            "|       from large pool |     261    |     263    |    7571 K  |    7571 K  |\n",
            "|       from small pool |     347    |     496    |    5568 K  |    5568 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     611    |   13140 K  |   13139 K  |\n",
            "|       from large pool |     261    |     263    |    7571 K  |    7571 K  |\n",
            "|       from small pool |     347    |     496    |    5568 K  |    5568 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      82    |     100    |   57057    |   56975    |\n",
            "|       from large pool |      58    |      58    |    4172    |    4114    |\n",
            "|       from small pool |      24    |      42    |   52885    |   52861    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      80    |      81    |    7412 K  |    7412 K  |\n",
            "|       from large pool |      70    |      71    |    4321 K  |    4321 K  |\n",
            "|       from small pool |      10    |      28    |    3091 K  |    3091 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:10 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  79% 1367/1720 [07:14<02:09,  2.72it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:14:12 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.87 GiB (GPU 0; 14.76 GiB total capacity; 10.56 GiB already allocated; 2.36 GiB free; 11.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:12 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1706         |        cudaMalloc retries: 2783      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10818 MB |   12834 MB |  108955 GB |  108944 GB |\n",
            "|       from large pool |   10774 MB |   12790 MB |  108136 GB |  108126 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     818 GB |     818 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10818 MB |   12834 MB |  108955 GB |  108944 GB |\n",
            "|       from large pool |   10774 MB |   12790 MB |  108136 GB |  108126 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     818 GB |     818 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11870 MB |   14052 MB |    6780 GB |    6769 GB |\n",
            "|       from large pool |   11824 MB |   13970 MB |    6677 GB |    6665 GB |\n",
            "|       from small pool |      46 MB |      82 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1051 MB |    2247 MB |  123028 GB |  123027 GB |\n",
            "|       from large pool |    1049 MB |    2246 MB |  122007 GB |  122006 GB |\n",
            "|       from small pool |       2 MB |       4 MB |    1021 GB |    1021 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13148 K  |   13147 K  |\n",
            "|       from large pool |     258    |     263    |    7576 K  |    7575 K  |\n",
            "|       from small pool |     349    |     496    |    5572 K  |    5571 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13148 K  |   13147 K  |\n",
            "|       from large pool |     258    |     263    |    7576 K  |    7575 K  |\n",
            "|       from small pool |     349    |     496    |    5572 K  |    5571 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      79    |     100    |   57075    |   56996    |\n",
            "|       from large pool |      56    |      59    |    4173    |    4117    |\n",
            "|       from small pool |      23    |      41    |   52902    |   52879    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      80    |      84    |    7417 K  |    7417 K  |\n",
            "|       from large pool |      69    |      71    |    4324 K  |    4324 K  |\n",
            "|       from small pool |      11    |      21    |    3093 K  |    3092 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:12 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  80% 1371/1720 [07:15<01:37,  3.57it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:14:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.24 GiB (GPU 0; 14.76 GiB total capacity; 7.32 GiB already allocated; 2.36 GiB free; 11.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1707         |        cudaMalloc retries: 2784      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7499 MB |    7525 MB |  108987 GB |  108980 GB |\n",
            "|       from large pool |    7456 MB |    7482 MB |  108168 GB |  108161 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     819 GB |     818 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7499 MB |    7525 MB |  108987 GB |  108980 GB |\n",
            "|       from large pool |    7456 MB |    7482 MB |  108168 GB |  108161 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     819 GB |     818 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11868 MB |   11956 MB |    6780 GB |    6769 GB |\n",
            "|       from large pool |   11824 MB |   11824 MB |    6677 GB |    6665 GB |\n",
            "|       from small pool |      44 MB |     132 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    4368 MB |    4368 MB |  123073 GB |  123069 GB |\n",
            "|       from large pool |    4367 MB |    4367 MB |  122051 GB |  122047 GB |\n",
            "|       from small pool |       1 MB |      20 MB |    1021 GB |    1021 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     610    |   13153 K  |   13152 K  |\n",
            "|       from large pool |     261    |     263    |    7578 K  |    7578 K  |\n",
            "|       from small pool |     345    |     496    |    5574 K  |    5573 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     610    |   13153 K  |   13152 K  |\n",
            "|       from large pool |     261    |     263    |    7578 K  |    7578 K  |\n",
            "|       from small pool |     345    |     496    |    5574 K  |    5573 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      78    |     122    |   57118    |   57040    |\n",
            "|       from large pool |      56    |      56    |    4173    |    4117    |\n",
            "|       from small pool |      22    |      66    |   52945    |   52923    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      54    |      54    |    7420 K  |    7420 K  |\n",
            "|       from large pool |      42    |      42    |    4325 K  |    4325 K  |\n",
            "|       from small pool |      12    |      37    |    3094 K  |    3094 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  80% 1372/1720 [07:16<01:29,  3.89it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:14:13 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.67 GiB (GPU 0; 14.76 GiB total capacity; 8.07 GiB already allocated; 2.36 GiB free; 11.59 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:13 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1708         |        cudaMalloc retries: 2785      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8268 MB |    9661 MB |  108996 GB |  108988 GB |\n",
            "|       from large pool |    8225 MB |    9618 MB |  108177 GB |  108169 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     819 GB |     818 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8268 MB |    9661 MB |  108996 GB |  108988 GB |\n",
            "|       from large pool |    8225 MB |    9618 MB |  108177 GB |  108169 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     819 GB |     818 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11868 MB |   11956 MB |    6780 GB |    6769 GB |\n",
            "|       from large pool |   11824 MB |   11824 MB |    6677 GB |    6665 GB |\n",
            "|       from small pool |      44 MB |     132 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3599 MB |    4756 MB |  123084 GB |  123081 GB |\n",
            "|       from large pool |    3598 MB |    4755 MB |  122063 GB |  122059 GB |\n",
            "|       from small pool |       1 MB |      20 MB |    1021 GB |    1021 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   13153 K  |   13153 K  |\n",
            "|       from large pool |     258    |     263    |    7579 K  |    7579 K  |\n",
            "|       from small pool |     350    |     496    |    5574 K  |    5574 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   13153 K  |   13153 K  |\n",
            "|       from large pool |     258    |     263    |    7579 K  |    7579 K  |\n",
            "|       from small pool |     350    |     496    |    5574 K  |    5574 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      78    |     122    |   57118    |   57040    |\n",
            "|       from large pool |      56    |      56    |    4173    |    4117    |\n",
            "|       from small pool |      22    |      66    |   52945    |   52923    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      55    |      58    |    7420 K  |    7420 K  |\n",
            "|       from large pool |      45    |      45    |    4326 K  |    4325 K  |\n",
            "|       from small pool |      10    |      37    |    3094 K  |    3094 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:13 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  80% 1379/1720 [07:18<01:47,  3.18it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:14:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.70 GiB (GPU 0; 14.76 GiB total capacity; 8.21 GiB already allocated; 2.41 GiB free; 11.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:16 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1709         |        cudaMalloc retries: 2786      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8405 MB |    8438 MB |  109079 GB |  109071 GB |\n",
            "|       from large pool |    8362 MB |    8394 MB |  108260 GB |  108252 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     819 GB |     819 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8405 MB |    8438 MB |  109079 GB |  109071 GB |\n",
            "|       from large pool |    8362 MB |    8394 MB |  108260 GB |  108252 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     819 GB |     819 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11820 MB |   11906 MB |    6780 GB |    6769 GB |\n",
            "|       from large pool |   11776 MB |   11824 MB |    6677 GB |    6665 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3414 MB |    3429 MB |  123184 GB |  123180 GB |\n",
            "|       from large pool |    3413 MB |    3429 MB |  122161 GB |  122158 GB |\n",
            "|       from small pool |       0 MB |      18 MB |    1022 GB |    1022 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     610    |   13163 K  |   13162 K  |\n",
            "|       from large pool |     261    |     263    |    7585 K  |    7584 K  |\n",
            "|       from small pool |     345    |     496    |    5577 K  |    5577 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     610    |   13163 K  |   13162 K  |\n",
            "|       from large pool |     261    |     263    |    7585 K  |    7584 K  |\n",
            "|       from small pool |     345    |     496    |    5577 K  |    5577 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      76    |      97    |   57137    |   57061    |\n",
            "|       from large pool |      54    |      56    |    4173    |    4119    |\n",
            "|       from small pool |      22    |      41    |   52964    |   52942    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      53    |      56    |    7426 K  |    7425 K  |\n",
            "|       from large pool |      44    |      47    |    4329 K  |    4329 K  |\n",
            "|       from small pool |       9    |      25    |    3096 K  |    3096 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  80% 1381/1720 [07:18<01:50,  3.06it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:14:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.58 GiB (GPU 0; 14.76 GiB total capacity; 7.20 GiB already allocated; 2.41 GiB free; 11.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:16 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1710         |        cudaMalloc retries: 2787      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7368 MB |    7386 MB |  109100 GB |  109093 GB |\n",
            "|       from large pool |    7325 MB |    7343 MB |  108280 GB |  108273 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     819 GB |     819 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7368 MB |    7386 MB |  109100 GB |  109093 GB |\n",
            "|       from large pool |    7325 MB |    7343 MB |  108280 GB |  108273 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     819 GB |     819 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11820 MB |   11854 MB |    6780 GB |    6769 GB |\n",
            "|       from large pool |   11776 MB |   11776 MB |    6677 GB |    6665 GB |\n",
            "|       from small pool |      44 MB |      78 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    4451 MB |    4451 MB |  123213 GB |  123209 GB |\n",
            "|       from large pool |    4450 MB |    4450 MB |  122191 GB |  122187 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1022 GB |    1022 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     610    |   13165 K  |   13164 K  |\n",
            "|       from large pool |     261    |     263    |    7586 K  |    7586 K  |\n",
            "|       from small pool |     345    |     496    |    5578 K  |    5578 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     610    |   13165 K  |   13164 K  |\n",
            "|       from large pool |     261    |     263    |    7586 K  |    7586 K  |\n",
            "|       from small pool |     345    |     496    |    5578 K  |    5578 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      76    |      93    |   57154    |   57078    |\n",
            "|       from large pool |      54    |      54    |    4173    |    4119    |\n",
            "|       from small pool |      22    |      39    |   52981    |   52959    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      66    |      67    |    7427 K  |    7427 K  |\n",
            "|       from large pool |      52    |      53    |    4330 K  |    4330 K  |\n",
            "|       from small pool |      14    |      21    |    3096 K  |    3096 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  81% 1387/1720 [07:20<01:31,  3.65it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:14:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.07 GiB (GPU 0; 14.76 GiB total capacity; 7.25 GiB already allocated; 2.41 GiB free; 11.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1711         |        cudaMalloc retries: 2788      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7425 MB |    7453 MB |  109147 GB |  109140 GB |\n",
            "|       from large pool |    7382 MB |    7410 MB |  108328 GB |  108321 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     819 GB |     819 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7425 MB |    7453 MB |  109147 GB |  109140 GB |\n",
            "|       from large pool |    7382 MB |    7410 MB |  108328 GB |  108321 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     819 GB |     819 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11820 MB |   11858 MB |    6780 GB |    6769 GB |\n",
            "|       from large pool |   11776 MB |   11776 MB |    6677 GB |    6665 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    4394 MB |    4394 MB |  123276 GB |  123272 GB |\n",
            "|       from large pool |    4393 MB |    4393 MB |  122254 GB |  122249 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1022 GB |    1022 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     610    |   13173 K  |   13172 K  |\n",
            "|       from large pool |     261    |     263    |    7591 K  |    7590 K  |\n",
            "|       from small pool |     345    |     496    |    5581 K  |    5581 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     610    |   13173 K  |   13172 K  |\n",
            "|       from large pool |     261    |     263    |    7591 K  |    7590 K  |\n",
            "|       from small pool |     345    |     496    |    5581 K  |    5581 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      76    |      95    |   57173    |   57097    |\n",
            "|       from large pool |      54    |      54    |    4173    |    4119    |\n",
            "|       from small pool |      22    |      41    |   53000    |   52978    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      60    |      60    |    7431 K  |    7431 K  |\n",
            "|       from large pool |      46    |      46    |    4333 K  |    4333 K  |\n",
            "|       from small pool |      14    |      24    |    3098 K  |    3098 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  81% 1394/1720 [07:22<01:26,  3.79it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:14:20 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.06 GiB (GPU 0; 14.76 GiB total capacity; 8.46 GiB already allocated; 2.41 GiB free; 11.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:20 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1712         |        cudaMalloc retries: 2789      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8664 MB |    8676 MB |  109221 GB |  109212 GB |\n",
            "|       from large pool |    8621 MB |    8633 MB |  108401 GB |  108392 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     820 GB |     820 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8664 MB |    8676 MB |  109221 GB |  109212 GB |\n",
            "|       from large pool |    8621 MB |    8633 MB |  108401 GB |  108392 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     820 GB |     820 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11820 MB |   11858 MB |    6780 GB |    6769 GB |\n",
            "|       from large pool |   11776 MB |   11776 MB |    6677 GB |    6665 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3155 MB |    3155 MB |  123367 GB |  123364 GB |\n",
            "|       from large pool |    3154 MB |    3154 MB |  122344 GB |  122341 GB |\n",
            "|       from small pool |       0 MB |      18 MB |    1023 GB |    1023 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     611    |   13182 K  |   13181 K  |\n",
            "|       from large pool |     261    |     263    |    7596 K  |    7596 K  |\n",
            "|       from small pool |     346    |     496    |    5585 K  |    5585 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     611    |   13182 K  |   13181 K  |\n",
            "|       from large pool |     261    |     263    |    7596 K  |    7596 K  |\n",
            "|       from small pool |     346    |     496    |    5585 K  |    5585 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      76    |      95    |   57192    |   57116    |\n",
            "|       from large pool |      54    |      54    |    4173    |    4119    |\n",
            "|       from small pool |      22    |      41    |   53019    |   52997    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      32    |      47    |    7437 K  |    7437 K  |\n",
            "|       from large pool |      22    |      22    |    4337 K  |    4337 K  |\n",
            "|       from small pool |      10    |      31    |    3100 K  |    3100 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:20 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  82% 1403/1720 [07:25<01:40,  3.16it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:14:23 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.45 GiB (GPU 0; 14.76 GiB total capacity; 9.55 GiB already allocated; 1.90 GiB free; 12.05 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:23 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1713         |        cudaMalloc retries: 2791      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7275 MB |    9779 MB |  109333 GB |  109326 GB |\n",
            "|       from large pool |    7233 MB |    9737 MB |  108513 GB |  108506 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     820 GB |     820 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7275 MB |    9779 MB |  109333 GB |  109326 GB |\n",
            "|       from large pool |    7233 MB |    9737 MB |  108513 GB |  108506 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     820 GB |     820 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12344 MB |   12344 MB |    6783 GB |    6771 GB |\n",
            "|       from large pool |   12300 MB |   12300 MB |    6679 GB |    6667 GB |\n",
            "|       from small pool |      44 MB |      80 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2562 MB |    3985 MB |  123499 GB |  123496 GB |\n",
            "|       from large pool |    2562 MB |    3980 MB |  122475 GB |  122473 GB |\n",
            "|       from small pool |       0 MB |       9 MB |    1023 GB |    1023 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13195 K  |   13194 K  |\n",
            "|       from large pool |     258    |     262    |    7604 K  |    7604 K  |\n",
            "|       from small pool |     349    |     496    |    5590 K  |    5590 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13195 K  |   13194 K  |\n",
            "|       from large pool |     258    |     262    |    7604 K  |    7604 K  |\n",
            "|       from small pool |     349    |     496    |    5590 K  |    5590 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      76    |      94    |   57211    |   57135    |\n",
            "|       from large pool |      54    |      54    |    4174    |    4120    |\n",
            "|       from small pool |      22    |      40    |   53037    |   53015    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      42    |      46    |    7444 K  |    7444 K  |\n",
            "|       from large pool |      35    |      36    |    4341 K  |    4341 K  |\n",
            "|       from small pool |       7    |      19    |    3102 K  |    3102 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:23 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  82% 1413/1720 [07:28<01:41,  3.02it/s, loss=4.527, nll_loss=4.527, ppl=23.05, wps=12220.6, ups=2.59, wpb=4720.7, bsz=256, num_updates=8200, lr=0.0001, gnorm=1.71, loss_scale=4, train_wall=31, gb_free=8.1, wall=3200]2022-07-19 07:14:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.87 GiB (GPU 0; 14.76 GiB total capacity; 10.41 GiB already allocated; 2.51 GiB free; 11.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1714         |        cudaMalloc retries: 2793      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10660 MB |   12677 MB |  109436 GB |  109425 GB |\n",
            "|       from large pool |   10616 MB |   12633 MB |  108614 GB |  108604 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     821 GB |     821 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10660 MB |   12677 MB |  109436 GB |  109425 GB |\n",
            "|       from large pool |   10616 MB |   12633 MB |  108614 GB |  108604 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     821 GB |     821 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11716 MB |   13952 MB |    6789 GB |    6777 GB |\n",
            "|       from large pool |   11670 MB |   13874 MB |    6685 GB |    6674 GB |\n",
            "|       from small pool |      46 MB |      78 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1053 MB |    2434 MB |  123607 GB |  123606 GB |\n",
            "|       from large pool |    1053 MB |    2432 MB |  122582 GB |  122581 GB |\n",
            "|       from small pool |       0 MB |       4 MB |    1025 GB |    1025 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13208 K  |   13208 K  |\n",
            "|       from large pool |     258    |     263    |    7612 K  |    7612 K  |\n",
            "|       from small pool |     349    |     496    |    5596 K  |    5596 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13208 K  |   13208 K  |\n",
            "|       from large pool |     258    |     263    |    7612 K  |    7612 K  |\n",
            "|       from small pool |     349    |     496    |    5596 K  |    5596 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      75    |      94    |   57276    |   57201    |\n",
            "|       from large pool |      52    |      55    |    4177    |    4125    |\n",
            "|       from small pool |      23    |      39    |   53099    |   53076    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      75    |      78    |    7452 K  |    7452 K  |\n",
            "|       from large pool |      65    |      66    |    4346 K  |    4346 K  |\n",
            "|       from small pool |      10    |      18    |    3106 K  |    3106 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  83% 1423/1720 [07:31<01:16,  3.90it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.67 GiB (GPU 0; 14.76 GiB total capacity; 7.86 GiB already allocated; 2.51 GiB free; 11.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:29 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1715         |        cudaMalloc retries: 2794      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8050 MB |    9443 MB |  109535 GB |  109527 GB |\n",
            "|       from large pool |    8007 MB |    9400 MB |  108712 GB |  108704 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     823 GB |     823 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8050 MB |    9443 MB |  109535 GB |  109527 GB |\n",
            "|       from large pool |    8007 MB |    9400 MB |  108712 GB |  108704 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     823 GB |     823 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11714 MB |   11870 MB |    6789 GB |    6777 GB |\n",
            "|       from large pool |   11670 MB |   11670 MB |    6685 GB |    6674 GB |\n",
            "|       from small pool |      44 MB |     200 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3663 MB |    3663 MB |  123735 GB |  123732 GB |\n",
            "|       from large pool |    3662 MB |    3662 MB |  122708 GB |  122705 GB |\n",
            "|       from small pool |       1 MB |      17 MB |    1026 GB |    1026 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13222 K  |   13222 K  |\n",
            "|       from large pool |     258    |     263    |    7619 K  |    7619 K  |\n",
            "|       from small pool |     349    |     496    |    5603 K  |    5602 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13222 K  |   13222 K  |\n",
            "|       from large pool |     258    |     263    |    7619 K  |    7619 K  |\n",
            "|       from small pool |     349    |     496    |    5603 K  |    5602 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     152    |   57354    |   57280    |\n",
            "|       from large pool |      52    |      52    |    4177    |    4125    |\n",
            "|       from small pool |      22    |     100    |   53177    |   53155    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      57    |    7460 K  |    7460 K  |\n",
            "|       from large pool |      39    |      39    |    4350 K  |    4350 K  |\n",
            "|       from small pool |      11    |      40    |    3110 K  |    3110 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  83% 1425/1720 [07:31<01:18,  3.78it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:29 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.33 GiB (GPU 0; 14.76 GiB total capacity; 9.32 GiB already allocated; 2.29 GiB free; 11.67 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:29 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1716         |        cudaMalloc retries: 2795      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7155 MB |    9543 MB |  109555 GB |  109548 GB |\n",
            "|       from large pool |    7113 MB |    9501 MB |  108732 GB |  108725 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     823 GB |     823 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7155 MB |    9543 MB |  109555 GB |  109548 GB |\n",
            "|       from large pool |    7113 MB |    9501 MB |  108732 GB |  108725 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     823 GB |     823 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11946 MB |   14136 MB |    6791 GB |    6780 GB |\n",
            "|       from large pool |   11902 MB |   14058 MB |    6687 GB |    6676 GB |\n",
            "|       from small pool |      44 MB |      78 MB |     103 GB |     103 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2400 MB |    3362 MB |  123756 GB |  123754 GB |\n",
            "|       from large pool |    2400 MB |    3362 MB |  122730 GB |  122727 GB |\n",
            "|       from small pool |       0 MB |       3 MB |    1026 GB |    1026 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13224 K  |   13224 K  |\n",
            "|       from large pool |     258    |     262    |    7621 K  |    7620 K  |\n",
            "|       from small pool |     349    |     496    |    5603 K  |    5603 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13224 K  |   13224 K  |\n",
            "|       from large pool |     258    |     262    |    7621 K  |    7620 K  |\n",
            "|       from small pool |     349    |     496    |    5603 K  |    5603 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |      92    |   57372    |   57298    |\n",
            "|       from large pool |      52    |      53    |    4178    |    4126    |\n",
            "|       from small pool |      22    |      39    |   53194    |   53172    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      51    |      55    |    7461 K  |    7461 K  |\n",
            "|       from large pool |      43    |      44    |    4351 K  |    4351 K  |\n",
            "|       from small pool |       8    |      17    |    3110 K  |    3110 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:29 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  83% 1433/1720 [07:34<01:34,  3.05it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:32 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.96 GiB (GPU 0; 14.76 GiB total capacity; 8.16 GiB already allocated; 2.51 GiB free; 11.44 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:32 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1717         |        cudaMalloc retries: 2798      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8351 MB |    9892 MB |  109648 GB |  109640 GB |\n",
            "|       from large pool |    8308 MB |    9850 MB |  108824 GB |  108816 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     824 GB |     824 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8351 MB |    9892 MB |  109648 GB |  109640 GB |\n",
            "|       from large pool |    8308 MB |    9850 MB |  108824 GB |  108816 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     824 GB |     824 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11714 MB |   13946 MB |    6801 GB |    6790 GB |\n",
            "|       from large pool |   11670 MB |   13826 MB |    6697 GB |    6686 GB |\n",
            "|       from small pool |      44 MB |     120 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3362 MB |    3362 MB |  123862 GB |  123858 GB |\n",
            "|       from large pool |    3361 MB |    3361 MB |  122834 GB |  122830 GB |\n",
            "|       from small pool |       1 MB |      12 MB |    1027 GB |    1027 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13235 K  |   13234 K  |\n",
            "|       from large pool |     258    |     263    |    7626 K  |    7626 K  |\n",
            "|       from small pool |     349    |     496    |    5609 K  |    5608 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13235 K  |   13234 K  |\n",
            "|       from large pool |     258    |     263    |    7626 K  |    7626 K  |\n",
            "|       from small pool |     349    |     496    |    5609 K  |    5608 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     113    |   57473    |   57399    |\n",
            "|       from large pool |      52    |      53    |    4183    |    4131    |\n",
            "|       from small pool |      22    |      60    |   53290    |   53268    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      60    |      61    |    7468 K  |    7467 K  |\n",
            "|       from large pool |      50    |      50    |    4354 K  |    4354 K  |\n",
            "|       from small pool |      10    |      34    |    3113 K  |    3113 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:32 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  83% 1436/1720 [07:35<01:16,  3.70it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:33 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.45 GiB (GPU 0; 14.76 GiB total capacity; 9.83 GiB already allocated; 2.17 GiB free; 11.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:33 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1718         |        cudaMalloc retries: 2800      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7559 MB |   10063 MB |  109672 GB |  109665 GB |\n",
            "|       from large pool |    7517 MB |   10021 MB |  108847 GB |  108840 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     824 GB |     824 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7559 MB |   10063 MB |  109672 GB |  109665 GB |\n",
            "|       from large pool |    7517 MB |   10021 MB |  108847 GB |  108840 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     824 GB |     824 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12062 MB |   12062 MB |    6804 GB |    6792 GB |\n",
            "|       from large pool |   12018 MB |   12018 MB |    6700 GB |    6688 GB |\n",
            "|       from small pool |      44 MB |     138 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1998 MB |    2902 MB |  123886 GB |  123884 GB |\n",
            "|       from large pool |    1996 MB |    2900 MB |  122858 GB |  122856 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1028 GB |    1028 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13238 K  |   13238 K  |\n",
            "|       from large pool |     258    |     262    |    7628 K  |    7627 K  |\n",
            "|       from small pool |     349    |     496    |    5610 K  |    5610 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13238 K  |   13238 K  |\n",
            "|       from large pool |     258    |     262    |    7628 K  |    7627 K  |\n",
            "|       from small pool |     349    |     496    |    5610 K  |    5610 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     121    |   57521    |   57447    |\n",
            "|       from large pool |      52    |      52    |    4184    |    4132    |\n",
            "|       from small pool |      22    |      69    |   53337    |   53315    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      63    |      67    |    7470 K  |    7470 K  |\n",
            "|       from large pool |      52    |      53    |    4355 K  |    4355 K  |\n",
            "|       from small pool |      11    |      21    |    3114 K  |    3114 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:33 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  84% 1440/1720 [07:36<01:20,  3.48it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:34 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.80 GiB (GPU 0; 14.76 GiB total capacity; 9.63 GiB already allocated; 2.68 GiB free; 11.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:34 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1719         |        cudaMalloc retries: 2801      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9863 MB |    9881 MB |  109710 GB |  109700 GB |\n",
            "|       from large pool |    9819 MB |    9837 MB |  108885 GB |  108875 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     824 GB |     824 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9863 MB |    9881 MB |  109710 GB |  109700 GB |\n",
            "|       from large pool |    9819 MB |    9837 MB |  108885 GB |  108875 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     824 GB |     824 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11540 MB |   11578 MB |    6806 GB |    6794 GB |\n",
            "|       from large pool |   11494 MB |   11494 MB |    6701 GB |    6690 GB |\n",
            "|       from small pool |      46 MB |      84 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1676 MB |    2115 MB |  123929 GB |  123928 GB |\n",
            "|       from large pool |    1674 MB |    2114 MB |  122901 GB |  122899 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1028 GB |    1028 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     611    |   13243 K  |   13243 K  |\n",
            "|       from large pool |     261    |     263    |    7631 K  |    7630 K  |\n",
            "|       from small pool |     346    |     496    |    5612 K  |    5612 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     611    |   13243 K  |   13243 K  |\n",
            "|       from large pool |     261    |     263    |    7631 K  |    7630 K  |\n",
            "|       from small pool |     346    |     496    |    5612 K  |    5612 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      75    |      94    |   57542    |   57467    |\n",
            "|       from large pool |      52    |      52    |    4185    |    4133    |\n",
            "|       from small pool |      23    |      42    |   53357    |   53334    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      71    |      71    |    7473 K  |    7473 K  |\n",
            "|       from large pool |      57    |      57    |    4357 K  |    4357 K  |\n",
            "|       from small pool |      14    |      18    |    3115 K  |    3115 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:34 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  84% 1442/1720 [07:37<01:36,  2.87it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.84 GiB (GPU 0; 14.76 GiB total capacity; 8.35 GiB already allocated; 2.69 GiB free; 11.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1720         |        cudaMalloc retries: 2802      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8545 MB |   10027 MB |  109737 GB |  109728 GB |\n",
            "|       from large pool |    8502 MB |    9984 MB |  108912 GB |  108903 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     824 GB |     824 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8545 MB |   10027 MB |  109737 GB |  109728 GB |\n",
            "|       from large pool |    8502 MB |    9984 MB |  108912 GB |  108903 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     824 GB |     824 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11538 MB |   11572 MB |    6806 GB |    6794 GB |\n",
            "|       from large pool |   11494 MB |   11494 MB |    6701 GB |    6690 GB |\n",
            "|       from small pool |      44 MB |      78 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2992 MB |    2992 MB |  123959 GB |  123956 GB |\n",
            "|       from large pool |    2991 MB |    2991 MB |  122930 GB |  122927 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1028 GB |    1028 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13245 K  |   13245 K  |\n",
            "|       from large pool |     258    |     263    |    7632 K  |    7632 K  |\n",
            "|       from small pool |     349    |     496    |    5613 K  |    5613 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13245 K  |   13245 K  |\n",
            "|       from large pool |     258    |     263    |    7632 K  |    7632 K  |\n",
            "|       from small pool |     349    |     496    |    5613 K  |    5613 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |      91    |   57558    |   57484    |\n",
            "|       from large pool |      52    |      52    |    4185    |    4133    |\n",
            "|       from small pool |      22    |      39    |   53373    |   53351    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      52    |    7474 K  |    7474 K  |\n",
            "|       from large pool |      41    |      41    |    4358 K  |    4358 K  |\n",
            "|       from small pool |       9    |      22    |    3116 K  |    3116 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  84% 1443/1720 [07:37<01:26,  3.21it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.47 GiB (GPU 0; 14.76 GiB total capacity; 7.69 GiB already allocated; 2.69 GiB free; 11.27 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1721         |        cudaMalloc retries: 2803      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7871 MB |   10027 MB |  109744 GB |  109736 GB |\n",
            "|       from large pool |    7827 MB |    9984 MB |  108919 GB |  108912 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     824 GB |     824 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7871 MB |   10027 MB |  109744 GB |  109736 GB |\n",
            "|       from large pool |    7827 MB |    9984 MB |  108919 GB |  108912 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     824 GB |     824 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11538 MB |   11572 MB |    6806 GB |    6794 GB |\n",
            "|       from large pool |   11494 MB |   11494 MB |    6701 GB |    6690 GB |\n",
            "|       from small pool |      44 MB |      78 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3666 MB |    3666 MB |  123970 GB |  123966 GB |\n",
            "|       from large pool |    3666 MB |    3666 MB |  122941 GB |  122938 GB |\n",
            "|       from small pool |       0 MB |       3 MB |    1028 GB |    1028 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     613    |   13246 K  |   13245 K  |\n",
            "|       from large pool |     261    |     263    |    7632 K  |    7632 K  |\n",
            "|       from small pool |     345    |     496    |    5613 K  |    5613 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     613    |   13246 K  |   13245 K  |\n",
            "|       from large pool |     261    |     263    |    7632 K  |    7632 K  |\n",
            "|       from small pool |     345    |     496    |    5613 K  |    5613 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |      91    |   57558    |   57484    |\n",
            "|       from large pool |      52    |      52    |    4185    |    4133    |\n",
            "|       from small pool |      22    |      39    |   53373    |   53351    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      60    |      60    |    7474 K  |    7474 K  |\n",
            "|       from large pool |      47    |      47    |    4358 K  |    4358 K  |\n",
            "|       from small pool |      13    |      22    |    3116 K  |    3116 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  85% 1457/1720 [07:41<01:29,  2.92it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:39 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.15 GiB (GPU 0; 14.76 GiB total capacity; 10.96 GiB already allocated; 2.47 GiB free; 11.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:39 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1722         |        cudaMalloc retries: 2806      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   11222 MB |   13387 MB |  109898 GB |  109887 GB |\n",
            "|       from large pool |   11178 MB |   13343 MB |  109072 GB |  109061 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     826 GB |     826 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   11222 MB |   13387 MB |  109898 GB |  109887 GB |\n",
            "|       from large pool |   11178 MB |   13343 MB |  109072 GB |  109061 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     826 GB |     826 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11756 MB |   14098 MB |    6814 GB |    6803 GB |\n",
            "|       from large pool |   11710 MB |   13942 MB |    6710 GB |    6699 GB |\n",
            "|       from small pool |      46 MB |     156 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  545935 KB |    2481 MB |  124135 GB |  124135 GB |\n",
            "|       from large pool |  543936 KB |    2479 MB |  123105 GB |  123104 GB |\n",
            "|       from small pool |    1999 KB |       4 MB |    1030 GB |    1030 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13266 K  |   13265 K  |\n",
            "|       from large pool |     258    |     263    |    7643 K  |    7643 K  |\n",
            "|       from small pool |     349    |     496    |    5622 K  |    5621 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13266 K  |   13265 K  |\n",
            "|       from large pool |     258    |     263    |    7643 K  |    7643 K  |\n",
            "|       from small pool |     349    |     496    |    5622 K  |    5621 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     131    |   57636    |   57563    |\n",
            "|       from large pool |      50    |      53    |    4188    |    4138    |\n",
            "|       from small pool |      23    |      78    |   53448    |   53425    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      75    |      76    |    7486 K  |    7486 K  |\n",
            "|       from large pool |      64    |      65    |    4365 K  |    4365 K  |\n",
            "|       from small pool |      11    |      18    |    3121 K  |    3121 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:39 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  85% 1460/1720 [07:42<01:40,  2.60it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.69 GiB (GPU 0; 14.76 GiB total capacity; 10.71 GiB already allocated; 2.47 GiB free; 11.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:41 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1723         |        cudaMalloc retries: 2807      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10929 MB |   11025 MB |  109939 GB |  109929 GB |\n",
            "|       from large pool |   10881 MB |   10977 MB |  109113 GB |  109102 GB |\n",
            "|       from small pool |      47 MB |      75 MB |     826 GB |     826 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10929 MB |   11025 MB |  109939 GB |  109929 GB |\n",
            "|       from large pool |   10881 MB |   10977 MB |  109113 GB |  109102 GB |\n",
            "|       from small pool |      47 MB |      75 MB |     826 GB |     826 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11760 MB |   11790 MB |    6814 GB |    6803 GB |\n",
            "|       from large pool |   11710 MB |   11710 MB |    6710 GB |    6699 GB |\n",
            "|       from small pool |      50 MB |      80 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     830 MB |    4594 MB |  124184 GB |  124183 GB |\n",
            "|       from large pool |     828 MB |    4592 MB |  123153 GB |  123153 GB |\n",
            "|       from small pool |       2 MB |       4 MB |    1030 GB |    1030 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     611    |   13269 K  |   13269 K  |\n",
            "|       from large pool |     261    |     263    |    7646 K  |    7645 K  |\n",
            "|       from small pool |     347    |     496    |    5623 K  |    5623 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     611    |   13269 K  |   13269 K  |\n",
            "|       from large pool |     261    |     263    |    7646 K  |    7645 K  |\n",
            "|       from small pool |     347    |     496    |    5623 K  |    5623 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      75    |      90    |   57653    |   57578    |\n",
            "|       from large pool |      50    |      50    |    4188    |    4138    |\n",
            "|       from small pool |      25    |      40    |   53465    |   53440    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      72    |      72    |    7488 K  |    7487 K  |\n",
            "|       from large pool |      58    |      58    |    4366 K  |    4366 K  |\n",
            "|       from small pool |      14    |      18    |    3121 K  |    3121 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  85% 1461/1720 [07:43<01:43,  2.49it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.90 GiB (GPU 0; 14.76 GiB total capacity; 8.31 GiB already allocated; 2.47 GiB free; 11.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:41 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1724         |        cudaMalloc retries: 2808      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8513 MB |   11025 MB |  109949 GB |  109940 GB |\n",
            "|       from large pool |    8470 MB |   10977 MB |  109122 GB |  109114 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     826 GB |     826 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8513 MB |   11025 MB |  109949 GB |  109940 GB |\n",
            "|       from large pool |    8470 MB |   10977 MB |  109122 GB |  109114 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     826 GB |     826 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11754 MB |   11790 MB |    6814 GB |    6803 GB |\n",
            "|       from large pool |   11710 MB |   11710 MB |    6710 GB |    6699 GB |\n",
            "|       from small pool |      44 MB |      80 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3240 MB |    4907 MB |  124200 GB |  124197 GB |\n",
            "|       from large pool |    3239 MB |    4905 MB |  123170 GB |  123166 GB |\n",
            "|       from small pool |       1 MB |       4 MB |    1030 GB |    1030 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13270 K  |   13269 K  |\n",
            "|       from large pool |     258    |     263    |    7646 K  |    7646 K  |\n",
            "|       from small pool |     349    |     496    |    5623 K  |    5623 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13270 K  |   13269 K  |\n",
            "|       from large pool |     258    |     263    |    7646 K  |    7646 K  |\n",
            "|       from small pool |     349    |     496    |    5623 K  |    5623 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |      90    |   57653    |   57581    |\n",
            "|       from large pool |      50    |      50    |    4188    |    4138    |\n",
            "|       from small pool |      22    |      40    |   53465    |   53443    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      62    |      74    |    7488 K  |    7488 K  |\n",
            "|       from large pool |      52    |      59    |    4366 K  |    4366 K  |\n",
            "|       from small pool |      10    |      18    |    3121 K  |    3121 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  85% 1468/1720 [07:45<01:41,  2.49it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.07 GiB (GPU 0; 14.76 GiB total capacity; 8.48 GiB already allocated; 2.47 GiB free; 11.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:43 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1725         |        cudaMalloc retries: 2809      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8682 MB |   10283 MB |  110046 GB |  110038 GB |\n",
            "|       from large pool |    8640 MB |   10240 MB |  109219 GB |  109211 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     826 GB |     826 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8682 MB |   10283 MB |  110046 GB |  110038 GB |\n",
            "|       from large pool |    8640 MB |   10240 MB |  109219 GB |  109211 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     826 GB |     826 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11754 MB |   11788 MB |    6814 GB |    6803 GB |\n",
            "|       from large pool |   11710 MB |   11710 MB |    6710 GB |    6699 GB |\n",
            "|       from small pool |      44 MB |      78 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3071 MB |    3071 MB |  124315 GB |  124312 GB |\n",
            "|       from large pool |    3069 MB |    3069 MB |  123284 GB |  123281 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1030 GB |    1030 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   13279 K  |   13279 K  |\n",
            "|       from large pool |     258    |     263    |    7652 K  |    7652 K  |\n",
            "|       from small pool |     350    |     496    |    5627 K  |    5626 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   13279 K  |   13279 K  |\n",
            "|       from large pool |     258    |     263    |    7652 K  |    7652 K  |\n",
            "|       from small pool |     350    |     496    |    5627 K  |    5626 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |      89    |   57670    |   57598    |\n",
            "|       from large pool |      50    |      50    |    4188    |    4138    |\n",
            "|       from small pool |      22    |      39    |   53482    |   53460    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      58    |      60    |    7493 K  |    7493 K  |\n",
            "|       from large pool |      46    |      46    |    4369 K  |    4369 K  |\n",
            "|       from small pool |      12    |      21    |    3123 K  |    3123 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  86% 1473/1720 [07:47<01:16,  3.25it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.19 GiB (GPU 0; 14.76 GiB total capacity; 7.58 GiB already allocated; 2.47 GiB free; 11.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:45 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1726         |        cudaMalloc retries: 2810      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7764 MB |    7795 MB |  110096 GB |  110088 GB |\n",
            "|       from large pool |    7720 MB |    7752 MB |  109269 GB |  109261 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     826 GB |     826 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7764 MB |    7795 MB |  110096 GB |  110088 GB |\n",
            "|       from large pool |    7720 MB |    7752 MB |  109269 GB |  109261 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     826 GB |     826 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11754 MB |   11790 MB |    6814 GB |    6803 GB |\n",
            "|       from large pool |   11710 MB |   11710 MB |    6710 GB |    6699 GB |\n",
            "|       from small pool |      44 MB |      80 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3989 MB |    3989 MB |  124380 GB |  124376 GB |\n",
            "|       from large pool |    3989 MB |    3989 MB |  123349 GB |  123345 GB |\n",
            "|       from small pool |       0 MB |       7 MB |    1031 GB |    1031 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     610    |   13286 K  |   13285 K  |\n",
            "|       from large pool |     261    |     263    |    7656 K  |    7656 K  |\n",
            "|       from small pool |     345    |     496    |    5629 K  |    5629 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     610    |   13286 K  |   13285 K  |\n",
            "|       from large pool |     261    |     263    |    7656 K  |    7656 K  |\n",
            "|       from small pool |     345    |     496    |    5629 K  |    5629 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |      90    |   57688    |   57616    |\n",
            "|       from large pool |      50    |      50    |    4188    |    4138    |\n",
            "|       from small pool |      22    |      40    |   53500    |   53478    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      54    |      54    |    7497 K  |    7497 K  |\n",
            "|       from large pool |      40    |      40    |    4372 K  |    4372 K  |\n",
            "|       from small pool |      14    |      25    |    3125 K  |    3125 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  86% 1474/1720 [07:47<01:10,  3.50it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:45 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 14.76 GiB total capacity; 7.81 GiB already allocated; 2.47 GiB free; 11.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:45 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1727         |        cudaMalloc retries: 2811      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7993 MB |    9298 MB |  110104 GB |  110096 GB |\n",
            "|       from large pool |    7951 MB |    9255 MB |  109277 GB |  109270 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     826 GB |     826 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7993 MB |    9298 MB |  110104 GB |  110096 GB |\n",
            "|       from large pool |    7951 MB |    9255 MB |  109277 GB |  109270 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     826 GB |     826 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11754 MB |   11790 MB |    6814 GB |    6803 GB |\n",
            "|       from large pool |   11710 MB |   11710 MB |    6710 GB |    6699 GB |\n",
            "|       from small pool |      44 MB |      80 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3760 MB |    5046 MB |  124392 GB |  124388 GB |\n",
            "|       from large pool |    3758 MB |    5044 MB |  123360 GB |  123357 GB |\n",
            "|       from small pool |       1 MB |       7 MB |    1031 GB |    1031 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   13286 K  |   13286 K  |\n",
            "|       from large pool |     258    |     263    |    7656 K  |    7656 K  |\n",
            "|       from small pool |     350    |     496    |    5629 K  |    5629 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   13286 K  |   13286 K  |\n",
            "|       from large pool |     258    |     263    |    7656 K  |    7656 K  |\n",
            "|       from small pool |     350    |     496    |    5629 K  |    5629 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |      90    |   57688    |   57616    |\n",
            "|       from large pool |      50    |      50    |    4188    |    4138    |\n",
            "|       from small pool |      22    |      40    |   53500    |   53478    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      54    |      56    |    7497 K  |    7497 K  |\n",
            "|       from large pool |      41    |      41    |    4372 K  |    4372 K  |\n",
            "|       from small pool |      13    |      25    |    3125 K  |    3125 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:45 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  86% 1477/1720 [07:48<01:00,  4.00it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:46 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.33 GiB (GPU 0; 14.76 GiB total capacity; 9.01 GiB already allocated; 2.47 GiB free; 11.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:46 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1728         |        cudaMalloc retries: 2812      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9195 MB |    9264 MB |  110130 GB |  110121 GB |\n",
            "|       from large pool |    9149 MB |    9218 MB |  109303 GB |  109294 GB |\n",
            "|       from small pool |      46 MB |      75 MB |     827 GB |     827 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9195 MB |    9264 MB |  110130 GB |  110121 GB |\n",
            "|       from large pool |    9149 MB |    9218 MB |  109303 GB |  109294 GB |\n",
            "|       from small pool |      46 MB |      75 MB |     827 GB |     827 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11758 MB |   11794 MB |    6814 GB |    6803 GB |\n",
            "|       from large pool |   11710 MB |   11710 MB |    6710 GB |    6699 GB |\n",
            "|       from small pool |      48 MB |      84 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2562 MB |    4523 MB |  124425 GB |  124423 GB |\n",
            "|       from large pool |    2560 MB |    4521 MB |  123394 GB |  123391 GB |\n",
            "|       from small pool |       1 MB |      11 MB |    1031 GB |    1031 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     611    |   13290 K  |   13289 K  |\n",
            "|       from large pool |     261    |     263    |    7658 K  |    7658 K  |\n",
            "|       from small pool |     347    |     496    |    5631 K  |    5630 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     611    |   13290 K  |   13289 K  |\n",
            "|       from large pool |     261    |     263    |    7658 K  |    7658 K  |\n",
            "|       from small pool |     347    |     496    |    5631 K  |    5630 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |      92    |   57708    |   57634    |\n",
            "|       from large pool |      50    |      50    |    4188    |    4138    |\n",
            "|       from small pool |      24    |      42    |   53520    |   53496    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      72    |      73    |    7499 K  |    7499 K  |\n",
            "|       from large pool |      60    |      61    |    4373 K  |    4373 K  |\n",
            "|       from small pool |      12    |      33    |    3125 K  |    3125 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:46 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  86% 1478/1720 [07:48<01:09,  3.49it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:46 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.53 GiB (GPU 0; 14.76 GiB total capacity; 7.60 GiB already allocated; 2.47 GiB free; 11.48 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:46 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1729         |        cudaMalloc retries: 2813      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7778 MB |    9264 MB |  110137 GB |  110130 GB |\n",
            "|       from large pool |    7735 MB |    9218 MB |  109310 GB |  109303 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     827 GB |     827 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7778 MB |    9264 MB |  110137 GB |  110130 GB |\n",
            "|       from large pool |    7735 MB |    9218 MB |  109310 GB |  109303 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     827 GB |     827 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11754 MB |   11794 MB |    6814 GB |    6803 GB |\n",
            "|       from large pool |   11710 MB |   11710 MB |    6710 GB |    6699 GB |\n",
            "|       from small pool |      44 MB |      84 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3975 MB |    4723 MB |  124440 GB |  124436 GB |\n",
            "|       from large pool |    3974 MB |    4721 MB |  123408 GB |  123404 GB |\n",
            "|       from small pool |       0 MB |      11 MB |    1031 GB |    1031 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     611    |   13290 K  |   13290 K  |\n",
            "|       from large pool |     261    |     263    |    7659 K  |    7658 K  |\n",
            "|       from small pool |     345    |     496    |    5631 K  |    5631 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     611    |   13290 K  |   13290 K  |\n",
            "|       from large pool |     261    |     263    |    7659 K  |    7658 K  |\n",
            "|       from small pool |     345    |     496    |    5631 K  |    5631 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |      92    |   57708    |   57636    |\n",
            "|       from large pool |      50    |      50    |    4188    |    4138    |\n",
            "|       from small pool |      22    |      42    |   53520    |   53498    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      75    |    7499 K  |    7499 K  |\n",
            "|       from large pool |      56    |      62    |    4373 K  |    4373 K  |\n",
            "|       from small pool |      12    |      33    |    3126 K  |    3126 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:46 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  86% 1479/1720 [07:48<01:03,  3.77it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:46 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.33 GiB (GPU 0; 14.76 GiB total capacity; 9.45 GiB already allocated; 2.30 GiB free; 11.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:46 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1730         |        cudaMalloc retries: 2814      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7291 MB |    9679 MB |  110147 GB |  110140 GB |\n",
            "|       from large pool |    7249 MB |    9637 MB |  109320 GB |  109313 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     827 GB |     827 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7291 MB |    9679 MB |  110147 GB |  110140 GB |\n",
            "|       from large pool |    7249 MB |    9637 MB |  109320 GB |  109313 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     827 GB |     827 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11928 MB |   14142 MB |    6817 GB |    6805 GB |\n",
            "|       from large pool |   11884 MB |   14098 MB |    6712 GB |    6701 GB |\n",
            "|       from small pool |      44 MB |      84 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2248 MB |    4887 MB |  124449 GB |  124447 GB |\n",
            "|       from large pool |    2246 MB |    4885 MB |  123417 GB |  123415 GB |\n",
            "|       from small pool |       2 MB |      11 MB |    1031 GB |    1031 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13291 K  |   13290 K  |\n",
            "|       from large pool |     258    |     263    |    7659 K  |    7659 K  |\n",
            "|       from small pool |     349    |     496    |    5631 K  |    5631 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13291 K  |   13290 K  |\n",
            "|       from large pool |     258    |     263    |    7659 K  |    7659 K  |\n",
            "|       from small pool |     349    |     496    |    5631 K  |    5631 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |      92    |   57709    |   57637    |\n",
            "|       from large pool |      50    |      51    |    4189    |    4139    |\n",
            "|       from small pool |      22    |      42    |   53520    |   53498    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      68    |      75    |    7500 K  |    7499 K  |\n",
            "|       from large pool |      58    |      62    |    4373 K  |    4373 K  |\n",
            "|       from small pool |      10    |      33    |    3126 K  |    3126 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:46 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  86% 1482/1720 [07:49<01:08,  3.49it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:47 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.45 GiB (GPU 0; 14.76 GiB total capacity; 9.54 GiB already allocated; 2.19 GiB free; 11.76 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:47 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1731         |        cudaMalloc retries: 2815      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7261 MB |    9765 MB |  110174 GB |  110167 GB |\n",
            "|       from large pool |    7219 MB |    9723 MB |  109346 GB |  109339 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     827 GB |     827 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7261 MB |    9765 MB |  110174 GB |  110167 GB |\n",
            "|       from large pool |    7219 MB |    9723 MB |  109346 GB |  109339 GB |\n",
            "|       from small pool |      41 MB |      75 MB |     827 GB |     827 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12044 MB |   13692 MB |    6821 GB |    6809 GB |\n",
            "|       from large pool |   12000 MB |   13574 MB |    6716 GB |    6705 GB |\n",
            "|       from small pool |      44 MB |     118 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2278 MB |    2599 MB |  124476 GB |  124474 GB |\n",
            "|       from large pool |    2276 MB |    2598 MB |  123444 GB |  123442 GB |\n",
            "|       from small pool |       2 MB |       4 MB |    1031 GB |    1031 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   13294 K  |   13293 K  |\n",
            "|       from large pool |     258    |     262    |    7661 K  |    7661 K  |\n",
            "|       from small pool |     350    |     496    |    5633 K  |    5632 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   13294 K  |   13293 K  |\n",
            "|       from large pool |     258    |     262    |    7661 K  |    7661 K  |\n",
            "|       from small pool |     350    |     496    |    5633 K  |    5632 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |     110    |   57748    |   57676    |\n",
            "|       from large pool |      50    |      51    |    4191    |    4141    |\n",
            "|       from small pool |      22    |      59    |   53557    |   53535    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |      47    |    7501 K  |    7501 K  |\n",
            "|       from large pool |      33    |      34    |    4375 K  |    4375 K  |\n",
            "|       from small pool |      10    |      19    |    3126 K  |    3126 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:47 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  86% 1487/1720 [07:51<01:13,  3.16it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:49 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 5.23 GiB (GPU 0; 14.76 GiB total capacity; 8.46 GiB already allocated; 3.10 GiB free; 10.86 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:49 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1732         |        cudaMalloc retries: 2816      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8661 MB |    8670 MB |  110218 GB |  110209 GB |\n",
            "|       from large pool |    8617 MB |    8626 MB |  109390 GB |  109382 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     827 GB |     827 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8661 MB |    8670 MB |  110218 GB |  110209 GB |\n",
            "|       from large pool |    8617 MB |    8626 MB |  109390 GB |  109382 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     827 GB |     827 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11116 MB |   11190 MB |    6822 GB |    6812 GB |\n",
            "|       from large pool |   11070 MB |   11070 MB |    6718 GB |    6707 GB |\n",
            "|       from small pool |      46 MB |     120 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2454 MB |    2454 MB |  124532 GB |  124530 GB |\n",
            "|       from large pool |    2452 MB |    2452 MB |  123500 GB |  123497 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1032 GB |    1032 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     611    |   13300 K  |   13300 K  |\n",
            "|       from large pool |     260    |     262    |    7664 K  |    7664 K  |\n",
            "|       from small pool |     347    |     496    |    5636 K  |    5635 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     611    |   13300 K  |   13300 K  |\n",
            "|       from large pool |     260    |     262    |    7664 K  |    7664 K  |\n",
            "|       from small pool |     347    |     496    |    5636 K  |    5635 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      73    |     110    |   57787    |   57714    |\n",
            "|       from large pool |      50    |      50    |    4192    |    4142    |\n",
            "|       from small pool |      23    |      60    |   53595    |   53572    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      53    |      53    |    7505 K  |    7505 K  |\n",
            "|       from large pool |      40    |      40    |    4376 K  |    4376 K  |\n",
            "|       from small pool |      13    |      18    |    3128 K  |    3128 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:49 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  87% 1492/1720 [07:52<00:56,  4.05it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.98 GiB (GPU 0; 14.76 GiB total capacity; 8.83 GiB already allocated; 3.14 GiB free; 10.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:50 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1733         |        cudaMalloc retries: 2817      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9036 MB |    9074 MB |  110259 GB |  110251 GB |\n",
            "|       from large pool |    8992 MB |    9030 MB |  109431 GB |  109422 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     828 GB |     828 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9036 MB |    9074 MB |  110259 GB |  110251 GB |\n",
            "|       from large pool |    8992 MB |    9030 MB |  109431 GB |  109422 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     828 GB |     828 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11068 MB |   11152 MB |    6823 GB |    6812 GB |\n",
            "|       from large pool |   11022 MB |   11070 MB |    6718 GB |    6707 GB |\n",
            "|       from small pool |      46 MB |      82 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2031 MB |    4469 MB |  124585 GB |  124583 GB |\n",
            "|       from large pool |    2029 MB |    4469 MB |  123552 GB |  123550 GB |\n",
            "|       from small pool |       1 MB |      10 MB |    1033 GB |    1033 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     610    |   13307 K  |   13306 K  |\n",
            "|       from large pool |     261    |     263    |    7668 K  |    7668 K  |\n",
            "|       from small pool |     345    |     496    |    5638 K  |    5638 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     610    |   13307 K  |   13306 K  |\n",
            "|       from large pool |     261    |     263    |    7668 K  |    7668 K  |\n",
            "|       from small pool |     345    |     496    |    5638 K  |    5638 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      71    |      91    |   57805    |   57734    |\n",
            "|       from large pool |      48    |      50    |    4192    |    4144    |\n",
            "|       from small pool |      23    |      41    |   53613    |   53590    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      70    |      74    |    7509 K  |    7509 K  |\n",
            "|       from large pool |      60    |      64    |    4379 K  |    4379 K  |\n",
            "|       from small pool |      10    |      28    |    3130 K  |    3130 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  87% 1493/1720 [07:52<00:58,  3.88it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:50 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.19 GiB (GPU 0; 14.76 GiB total capacity; 7.61 GiB already allocated; 3.15 GiB free; 10.81 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:50 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1734         |        cudaMalloc retries: 2818      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7791 MB |    9074 MB |  110267 GB |  110259 GB |\n",
            "|       from large pool |    7748 MB |    9030 MB |  109439 GB |  109431 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     828 GB |     828 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7791 MB |    9074 MB |  110267 GB |  110259 GB |\n",
            "|       from large pool |    7748 MB |    9030 MB |  109439 GB |  109431 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     828 GB |     828 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11066 MB |   11152 MB |    6823 GB |    6812 GB |\n",
            "|       from large pool |   11022 MB |   11070 MB |    6718 GB |    6707 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3274 MB |    4469 MB |  124597 GB |  124594 GB |\n",
            "|       from large pool |    3273 MB |    4469 MB |  123564 GB |  123561 GB |\n",
            "|       from small pool |       0 MB |      10 MB |    1033 GB |    1033 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     610    |   13307 K  |   13307 K  |\n",
            "|       from large pool |     261    |     263    |    7668 K  |    7668 K  |\n",
            "|       from small pool |     345    |     496    |    5638 K  |    5638 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     610    |   13307 K  |   13307 K  |\n",
            "|       from large pool |     261    |     263    |    7668 K  |    7668 K  |\n",
            "|       from small pool |     345    |     496    |    5638 K  |    5638 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      70    |      91    |   57805    |   57735    |\n",
            "|       from large pool |      48    |      50    |    4192    |    4144    |\n",
            "|       from small pool |      22    |      41    |   53613    |   53591    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      40    |      74    |    7509 K  |    7509 K  |\n",
            "|       from large pool |      31    |      64    |    4379 K  |    4379 K  |\n",
            "|       from small pool |       9    |      28    |    3130 K  |    3130 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:50 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  87% 1500/1720 [07:54<00:59,  3.68it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:52 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.52 GiB (GPU 0; 14.76 GiB total capacity; 11.01 GiB already allocated; 1.38 GiB free; 12.58 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:52 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1735         |        cudaMalloc retries: 2819      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   11231 MB |   11331 MB |  110352 GB |  110341 GB |\n",
            "|       from large pool |   11183 MB |   11282 MB |  109524 GB |  109513 GB |\n",
            "|       from small pool |      48 MB |      75 MB |     828 GB |     828 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   11231 MB |   11331 MB |  110352 GB |  110341 GB |\n",
            "|       from large pool |   11183 MB |   11282 MB |  109524 GB |  109513 GB |\n",
            "|       from small pool |      48 MB |      75 MB |     828 GB |     828 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12878 MB |   12912 MB |    6824 GB |    6812 GB |\n",
            "|       from large pool |   12828 MB |   12828 MB |    6720 GB |    6707 GB |\n",
            "|       from small pool |      50 MB |      84 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1646 MB |    4458 MB |  124702 GB |  124701 GB |\n",
            "|       from large pool |    1644 MB |    4458 MB |  123669 GB |  123667 GB |\n",
            "|       from small pool |       1 MB |      12 MB |    1033 GB |    1033 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     611    |   13317 K  |   13316 K  |\n",
            "|       from large pool |     261    |     263    |    7674 K  |    7674 K  |\n",
            "|       from small pool |     347    |     496    |    5642 K  |    5642 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     611    |   13317 K  |   13316 K  |\n",
            "|       from large pool |     261    |     263    |    7674 K  |    7674 K  |\n",
            "|       from small pool |     347    |     496    |    5642 K  |    5642 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |      91    |   57826    |   57752    |\n",
            "|       from large pool |      49    |      49    |    4193    |    4144    |\n",
            "|       from small pool |      25    |      42    |   53633    |   53608    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      53    |      60    |    7514 K  |    7514 K  |\n",
            "|       from large pool |      39    |      40    |    4382 K  |    4382 K  |\n",
            "|       from small pool |      14    |      30    |    3132 K  |    3132 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:52 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  87% 1501/1720 [07:55<01:09,  3.13it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:53 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.56 GiB (GPU 0; 14.76 GiB total capacity; 9.96 GiB already allocated; 2.12 GiB free; 11.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:53 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1736         |        cudaMalloc retries: 2821      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7576 MB |   11331 MB |  110362 GB |  110355 GB |\n",
            "|       from large pool |    7534 MB |   11282 MB |  109534 GB |  109526 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     828 GB |     828 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7576 MB |   11331 MB |  110362 GB |  110355 GB |\n",
            "|       from large pool |    7534 MB |   11282 MB |  109534 GB |  109526 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     828 GB |     828 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12114 MB |   12912 MB |    6827 GB |    6815 GB |\n",
            "|       from large pool |   12070 MB |   12828 MB |    6722 GB |    6710 GB |\n",
            "|       from small pool |      44 MB |      84 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1915 MB |    4677 MB |  124715 GB |  124713 GB |\n",
            "|       from large pool |    1913 MB |    4676 MB |  123682 GB |  123680 GB |\n",
            "|       from small pool |       1 MB |      12 MB |    1033 GB |    1033 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   13317 K  |   13316 K  |\n",
            "|       from large pool |     258    |     263    |    7674 K  |    7674 K  |\n",
            "|       from small pool |     350    |     496    |    5642 K  |    5642 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   13317 K  |   13316 K  |\n",
            "|       from large pool |     258    |     263    |    7674 K  |    7674 K  |\n",
            "|       from small pool |     350    |     496    |    5642 K  |    5642 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      70    |      91    |   57827    |   57757    |\n",
            "|       from large pool |      48    |      49    |    4194    |    4146    |\n",
            "|       from small pool |      22    |      42    |   53633    |   53611    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      46    |      60    |    7515 K  |    7515 K  |\n",
            "|       from large pool |      36    |      42    |    4382 K  |    4382 K  |\n",
            "|       from small pool |      10    |      30    |    3132 K  |    3132 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:53 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  88% 1516/1720 [08:00<01:03,  3.20it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:14:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.90 GiB (GPU 0; 14.76 GiB total capacity; 10.69 GiB already allocated; 1.78 GiB free; 12.17 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:14:58 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1737         |        cudaMalloc retries: 2825      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7976 MB |   10947 MB |  110546 GB |  110538 GB |\n",
            "|       from large pool |    7934 MB |   10904 MB |  109716 GB |  109708 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     830 GB |     829 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7976 MB |   10947 MB |  110546 GB |  110538 GB |\n",
            "|       from large pool |    7934 MB |   10904 MB |  109716 GB |  109708 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     830 GB |     829 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12462 MB |   14190 MB |    6841 GB |    6828 GB |\n",
            "|       from large pool |   12418 MB |   14108 MB |    6736 GB |    6724 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     104 GB |     104 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1515 MB |    2360 MB |  124916 GB |  124914 GB |\n",
            "|       from large pool |    1513 MB |    2358 MB |  123881 GB |  123879 GB |\n",
            "|       from small pool |       1 MB |       7 MB |    1035 GB |    1035 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13339 K  |   13338 K  |\n",
            "|       from large pool |     258    |     262    |    7687 K  |    7687 K  |\n",
            "|       from small pool |     349    |     496    |    5651 K  |    5651 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13339 K  |   13338 K  |\n",
            "|       from large pool |     258    |     262    |    7687 K  |    7687 K  |\n",
            "|       from small pool |     349    |     496    |    5651 K  |    5651 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      70    |      90    |   57918    |   57848    |\n",
            "|       from large pool |      48    |      49    |    4200    |    4152    |\n",
            "|       from small pool |      22    |      41    |   53718    |   53696    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      47    |      51    |    7527 K  |    7527 K  |\n",
            "|       from large pool |      37    |      38    |    4390 K  |    4390 K  |\n",
            "|       from small pool |      10    |      21    |    3137 K  |    3137 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:14:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  89% 1528/1720 [08:03<00:50,  3.83it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:15:01 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.89 GiB (GPU 0; 14.76 GiB total capacity; 10.68 GiB already allocated; 129.75 MiB free; 13.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:01 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1738         |        cudaMalloc retries: 2827      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10896 MB |   10997 MB |  110666 GB |  110656 GB |\n",
            "|       from large pool |   10848 MB |   10949 MB |  109835 GB |  109824 GB |\n",
            "|       from small pool |      48 MB |      75 MB |     831 GB |     831 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10896 MB |   10997 MB |  110666 GB |  110656 GB |\n",
            "|       from large pool |   10848 MB |   10949 MB |  109835 GB |  109824 GB |\n",
            "|       from small pool |      48 MB |      75 MB |     831 GB |     831 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14158 MB |   14240 MB |    6849 GB |    6835 GB |\n",
            "|       from large pool |   14108 MB |   14108 MB |    6744 GB |    6730 GB |\n",
            "|       from small pool |      50 MB |     132 MB |     105 GB |     105 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3261 MB |    4601 MB |  125057 GB |  125054 GB |\n",
            "|       from large pool |    3259 MB |    4599 MB |  124020 GB |  124017 GB |\n",
            "|       from small pool |       1 MB |      40 MB |    1037 GB |    1037 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     611    |   13355 K  |   13355 K  |\n",
            "|       from large pool |     261    |     263    |    7695 K  |    7695 K  |\n",
            "|       from small pool |     347    |     496    |    5659 K  |    5659 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     611    |   13355 K  |   13355 K  |\n",
            "|       from large pool |     261    |     263    |    7695 K  |    7695 K  |\n",
            "|       from small pool |     347    |     496    |    5659 K  |    5659 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      74    |     115    |   58004    |   57930    |\n",
            "|       from large pool |      49    |      49    |    4204    |    4155    |\n",
            "|       from small pool |      25    |      66    |   53800    |   53775    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      79    |      80    |    7537 K  |    7537 K  |\n",
            "|       from large pool |      65    |      66    |    4395 K  |    4395 K  |\n",
            "|       from small pool |      14    |      49    |    3141 K  |    3141 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:01 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  89% 1530/1720 [08:04<01:07,  2.80it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:15:02 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.26 GiB (GPU 0; 14.76 GiB total capacity; 11.31 GiB already allocated; 181.75 MiB free; 13.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:02 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1739         |        cudaMalloc retries: 2828      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   11584 MB |   11620 MB |  110699 GB |  110687 GB |\n",
            "|       from large pool |   11539 MB |   11575 MB |  109867 GB |  109856 GB |\n",
            "|       from small pool |      45 MB |      75 MB |     831 GB |     831 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   11584 MB |   11620 MB |  110699 GB |  110687 GB |\n",
            "|       from large pool |   11539 MB |   11575 MB |  109867 GB |  109856 GB |\n",
            "|       from small pool |      45 MB |      75 MB |     831 GB |     831 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14106 MB |   14186 MB |    6849 GB |    6835 GB |\n",
            "|       from large pool |   14060 MB |   14108 MB |    6744 GB |    6730 GB |\n",
            "|       from small pool |      46 MB |      78 MB |     105 GB |     105 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2521 MB |    2618 MB |  125093 GB |  125090 GB |\n",
            "|       from large pool |    2520 MB |    2616 MB |  124056 GB |  124053 GB |\n",
            "|       from small pool |       0 MB |       4 MB |    1037 GB |    1037 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     611    |   13357 K  |   13357 K  |\n",
            "|       from large pool |     261    |     263    |    7697 K  |    7696 K  |\n",
            "|       from small pool |     346    |     496    |    5660 K  |    5660 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     611    |   13357 K  |   13357 K  |\n",
            "|       from large pool |     261    |     263    |    7697 K  |    7696 K  |\n",
            "|       from small pool |     346    |     496    |    5660 K  |    5660 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      70    |      88    |   58018    |   57948    |\n",
            "|       from large pool |      47    |      49    |    4204    |    4157    |\n",
            "|       from small pool |      23    |      39    |   53814    |   53791    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      72    |      74    |    7538 K  |    7538 K  |\n",
            "|       from large pool |      59    |      62    |    4396 K  |    4396 K  |\n",
            "|       from small pool |      13    |      17    |    3142 K  |    3142 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:02 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  89% 1534/1720 [08:06<01:02,  2.97it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:15:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.02 GiB (GPU 0; 14.76 GiB total capacity; 8.44 GiB already allocated; 2.45 GiB free; 11.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1740         |        cudaMalloc retries: 2829      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8639 MB |   10210 MB |  110745 GB |  110736 GB |\n",
            "|       from large pool |    8596 MB |   10167 MB |  109913 GB |  109904 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     831 GB |     831 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8639 MB |   10210 MB |  110745 GB |  110736 GB |\n",
            "|       from large pool |    8596 MB |   10167 MB |  109913 GB |  109904 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     831 GB |     831 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11774 MB |   14138 MB |    6849 GB |    6837 GB |\n",
            "|       from large pool |   11730 MB |   14060 MB |    6744 GB |    6732 GB |\n",
            "|       from small pool |      44 MB |      78 MB |     105 GB |     105 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3134 MB |    3134 MB |  125152 GB |  125149 GB |\n",
            "|       from large pool |    3133 MB |    3133 MB |  124115 GB |  124112 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1037 GB |    1037 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   13362 K  |   13362 K  |\n",
            "|       from large pool |     258    |     263    |    7700 K  |    7700 K  |\n",
            "|       from small pool |     350    |     496    |    5662 K  |    5662 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   13362 K  |   13362 K  |\n",
            "|       from large pool |     258    |     263    |    7700 K  |    7700 K  |\n",
            "|       from small pool |     350    |     496    |    5662 K  |    5662 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      68    |      86    |   58034    |   57966    |\n",
            "|       from large pool |      46    |      47    |    4204    |    4158    |\n",
            "|       from small pool |      22    |      39    |   53830    |   53808    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      50    |    7541 K  |    7541 K  |\n",
            "|       from large pool |      40    |      40    |    4398 K  |    4398 K  |\n",
            "|       from small pool |      10    |      20    |    3143 K  |    3143 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  89% 1536/1720 [08:06<00:53,  3.46it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:15:04 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.26 GiB (GPU 0; 14.76 GiB total capacity; 10.62 GiB already allocated; 2.45 GiB free; 11.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:04 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1741         |        cudaMalloc retries: 2830      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10874 MB |   10900 MB |  110763 GB |  110752 GB |\n",
            "|       from large pool |   10829 MB |   10855 MB |  109931 GB |  109920 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     831 GB |     831 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10874 MB |   10900 MB |  110763 GB |  110752 GB |\n",
            "|       from large pool |   10829 MB |   10855 MB |  109931 GB |  109920 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     831 GB |     831 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11776 MB |   11810 MB |    6849 GB |    6837 GB |\n",
            "|       from large pool |   11730 MB |   11730 MB |    6744 GB |    6732 GB |\n",
            "|       from small pool |      46 MB |      80 MB |     105 GB |     105 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     901 MB |    4518 MB |  125178 GB |  125177 GB |\n",
            "|       from large pool |     900 MB |    4516 MB |  124140 GB |  124140 GB |\n",
            "|       from small pool |       1 MB |       6 MB |    1037 GB |    1037 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     611    |   13364 K  |   13364 K  |\n",
            "|       from large pool |     261    |     263    |    7701 K  |    7701 K  |\n",
            "|       from small pool |     346    |     496    |    5663 K  |    5662 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     611    |   13364 K  |   13364 K  |\n",
            "|       from large pool |     261    |     263    |    7701 K  |    7701 K  |\n",
            "|       from small pool |     346    |     496    |    5663 K  |    5662 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |      86    |   58052    |   57983    |\n",
            "|       from large pool |      46    |      46    |    4204    |    4158    |\n",
            "|       from small pool |      23    |      40    |   53848    |   53825    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      69    |      70    |    7542 K  |    7542 K  |\n",
            "|       from large pool |      57    |      58    |    4398 K  |    4398 K  |\n",
            "|       from small pool |      12    |      22    |    3143 K  |    3143 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:04 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  90% 1540/1720 [08:07<00:46,  3.89it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:15:05 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.38 GiB (GPU 0; 14.76 GiB total capacity; 7.61 GiB already allocated; 2.45 GiB free; 11.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:05 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1742         |        cudaMalloc retries: 2831      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7790 MB |    7800 MB |  110789 GB |  110781 GB |\n",
            "|       from large pool |    7747 MB |    7757 MB |  109957 GB |  109949 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     832 GB |     832 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7790 MB |    7800 MB |  110789 GB |  110781 GB |\n",
            "|       from large pool |    7747 MB |    7757 MB |  109957 GB |  109949 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     832 GB |     832 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11774 MB |   11914 MB |    6849 GB |    6837 GB |\n",
            "|       from large pool |   11730 MB |   11730 MB |    6744 GB |    6732 GB |\n",
            "|       from small pool |      44 MB |     184 MB |     105 GB |     105 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3983 MB |    4061 MB |  125217 GB |  125213 GB |\n",
            "|       from large pool |    3982 MB |    4057 MB |  124179 GB |  124175 GB |\n",
            "|       from small pool |       0 MB |       3 MB |    1038 GB |    1038 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     611    |   13369 K  |   13368 K  |\n",
            "|       from large pool |     260    |     262    |    7703 K  |    7703 K  |\n",
            "|       from small pool |     347    |     496    |    5665 K  |    5665 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     611    |   13369 K  |   13368 K  |\n",
            "|       from large pool |     260    |     262    |    7703 K  |    7703 K  |\n",
            "|       from small pool |     347    |     496    |    5665 K  |    5665 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      68    |     138    |   58121    |   58053    |\n",
            "|       from large pool |      46    |      46    |    4204    |    4158    |\n",
            "|       from small pool |      22    |      92    |   53917    |   53895    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      53    |      53    |    7545 K  |    7545 K  |\n",
            "|       from large pool |      39    |      39    |    4400 K  |    4400 K  |\n",
            "|       from small pool |      14    |      18    |    3145 K  |    3145 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:05 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  90% 1544/1720 [08:08<00:46,  3.77it/s, loss=4.378, nll_loss=4.378, ppl=20.8, wps=13015.3, ups=2.75, wpb=4739.4, bsz=256, num_updates=8300, lr=0.0001, gnorm=1.664, loss_scale=4, train_wall=30, gb_free=5.5, wall=3236]2022-07-19 07:15:06 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.24 GiB (GPU 0; 14.76 GiB total capacity; 7.77 GiB already allocated; 2.45 GiB free; 11.50 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:06 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1743         |        cudaMalloc retries: 2832      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7959 MB |    7992 MB |  110827 GB |  110819 GB |\n",
            "|       from large pool |    7915 MB |    7949 MB |  109994 GB |  109986 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     832 GB |     832 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7959 MB |    7992 MB |  110827 GB |  110819 GB |\n",
            "|       from large pool |    7915 MB |    7949 MB |  109994 GB |  109986 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     832 GB |     832 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11774 MB |   11812 MB |    6849 GB |    6837 GB |\n",
            "|       from large pool |   11730 MB |   11730 MB |    6744 GB |    6732 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     105 GB |     105 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3814 MB |    4316 MB |  125268 GB |  125264 GB |\n",
            "|       from large pool |    3814 MB |    4312 MB |  124230 GB |  124226 GB |\n",
            "|       from small pool |       0 MB |       3 MB |    1038 GB |    1038 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     610    |   13374 K  |   13373 K  |\n",
            "|       from large pool |     261    |     263    |    7706 K  |    7706 K  |\n",
            "|       from small pool |     345    |     496    |    5667 K  |    5667 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     610    |   13374 K  |   13373 K  |\n",
            "|       from large pool |     261    |     263    |    7706 K  |    7706 K  |\n",
            "|       from small pool |     345    |     496    |    5667 K  |    5667 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      68    |      87    |   58140    |   58072    |\n",
            "|       from large pool |      46    |      46    |    4204    |    4158    |\n",
            "|       from small pool |      22    |      41    |   53936    |   53914    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      28    |      48    |    7548 K  |    7548 K  |\n",
            "|       from large pool |      16    |      34    |    4402 K  |    4402 K  |\n",
            "|       from small pool |      12    |      18    |    3146 K  |    3146 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:06 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  90% 1550/1720 [08:10<01:01,  2.76it/s, loss=4.597, nll_loss=4.597, ppl=24.2, wps=11872.1, ups=2.5, wpb=4740.7, bsz=256, num_updates=8400, lr=0.0001, gnorm=1.899, loss_scale=4, train_wall=31, gb_free=4.2, wall=3276]2022-07-19 07:15:08 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.33 GiB (GPU 0; 14.76 GiB total capacity; 9.76 GiB already allocated; 125.75 MiB free; 13.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:08 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1744         |        cudaMalloc retries: 2833      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7602 MB |    9990 MB |  110906 GB |  110899 GB |\n",
            "|       from large pool |    7559 MB |    9947 MB |  110073 GB |  110066 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     833 GB |     832 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7602 MB |    9990 MB |  110906 GB |  110899 GB |\n",
            "|       from large pool |    7559 MB |    9947 MB |  110073 GB |  110066 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     833 GB |     832 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14162 MB |   14202 MB |    6851 GB |    6838 GB |\n",
            "|       from large pool |   14118 MB |   14118 MB |    6746 GB |    6732 GB |\n",
            "|       from small pool |      44 MB |      84 MB |     105 GB |     105 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    4171 MB |    4568 MB |  125365 GB |  125361 GB |\n",
            "|       from large pool |    4170 MB |    4565 MB |  124326 GB |  124322 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1038 GB |    1038 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13382 K  |   13381 K  |\n",
            "|       from large pool |     258    |     263    |    7711 K  |    7711 K  |\n",
            "|       from small pool |     349    |     496    |    5670 K  |    5670 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13382 K  |   13381 K  |\n",
            "|       from large pool |     258    |     263    |    7711 K  |    7711 K  |\n",
            "|       from small pool |     349    |     496    |    5670 K  |    5670 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |      89    |   58161    |   58092    |\n",
            "|       from large pool |      47    |      47    |    4205    |    4158    |\n",
            "|       from small pool |      22    |      42    |   53956    |   53934    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      57    |      60    |    7552 K  |    7552 K  |\n",
            "|       from large pool |      47    |      47    |    4404 K  |    4404 K  |\n",
            "|       from small pool |      10    |      20    |    3147 K  |    3147 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:08 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  91% 1571/1720 [08:16<00:50,  2.96it/s, loss=4.597, nll_loss=4.597, ppl=24.2, wps=11872.1, ups=2.5, wpb=4740.7, bsz=256, num_updates=8400, lr=0.0001, gnorm=1.899, loss_scale=4, train_wall=31, gb_free=4.2, wall=3276]2022-07-19 07:15:14 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.45 GiB (GPU 0; 14.76 GiB total capacity; 9.85 GiB already allocated; 9.75 MiB free; 13.94 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:14 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1745         |        cudaMalloc retries: 2835      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7586 MB |   10090 MB |  111120 GB |  111112 GB |\n",
            "|       from large pool |    7543 MB |   10047 MB |  110285 GB |  110277 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     834 GB |     834 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7586 MB |   10090 MB |  111120 GB |  111112 GB |\n",
            "|       from large pool |    7543 MB |   10047 MB |  110285 GB |  110277 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     834 GB |     834 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14278 MB |   14278 MB |    6856 GB |    6842 GB |\n",
            "|       from large pool |   14234 MB |   14234 MB |    6751 GB |    6737 GB |\n",
            "|       from small pool |      44 MB |     190 MB |     105 GB |     105 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    4187 MB |    4187 MB |  125621 GB |  125617 GB |\n",
            "|       from large pool |    4186 MB |    4186 MB |  124580 GB |  124576 GB |\n",
            "|       from small pool |       1 MB |       4 MB |    1041 GB |    1041 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13412 K  |   13412 K  |\n",
            "|       from large pool |     258    |     262    |    7729 K  |    7728 K  |\n",
            "|       from small pool |     349    |     496    |    5683 K  |    5683 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13412 K  |   13412 K  |\n",
            "|       from large pool |     258    |     262    |    7729 K  |    7728 K  |\n",
            "|       from small pool |     349    |     496    |    5683 K  |    5683 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |     142    |   58236    |   58167    |\n",
            "|       from large pool |      47    |      47    |    4207    |    4160    |\n",
            "|       from small pool |      22    |      95    |   54029    |   54007    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      58    |      60    |    7570 K  |    7569 K  |\n",
            "|       from large pool |      48    |      48    |    4415 K  |    4415 K  |\n",
            "|       from small pool |      10    |      17    |    3154 K  |    3154 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:14 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  91% 1573/1720 [08:17<00:41,  3.52it/s, loss=4.597, nll_loss=4.597, ppl=24.2, wps=11872.1, ups=2.5, wpb=4740.7, bsz=256, num_updates=8400, lr=0.0001, gnorm=1.899, loss_scale=4, train_wall=31, gb_free=4.2, wall=3276]2022-07-19 07:15:15 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.22 GiB (GPU 0; 14.76 GiB total capacity; 9.61 GiB already allocated; 241.75 MiB free; 13.72 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:15 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1746         |        cudaMalloc retries: 2836      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7569 MB |    9840 MB |  111132 GB |  111125 GB |\n",
            "|       from large pool |    7526 MB |    9797 MB |  110297 GB |  110289 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     835 GB |     835 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7569 MB |    9840 MB |  111132 GB |  111125 GB |\n",
            "|       from large pool |    7526 MB |    9797 MB |  110297 GB |  110289 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     835 GB |     835 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14046 MB |   14186 MB |    6859 GB |    6845 GB |\n",
            "|       from large pool |   14002 MB |   14002 MB |    6753 GB |    6739 GB |\n",
            "|       from small pool |      44 MB |     184 MB |     105 GB |     105 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    4204 MB |    4206 MB |  125636 GB |  125632 GB |\n",
            "|       from large pool |    4203 MB |    4204 MB |  124594 GB |  124590 GB |\n",
            "|       from small pool |       1 MB |      87 MB |    1041 GB |    1041 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13414 K  |   13414 K  |\n",
            "|       from large pool |     258    |     263    |    7729 K  |    7729 K  |\n",
            "|       from small pool |     349    |     496    |    5685 K  |    5684 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13414 K  |   13414 K  |\n",
            "|       from large pool |     258    |     263    |    7729 K  |    7729 K  |\n",
            "|       from small pool |     349    |     496    |    5685 K  |    5684 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      69    |     139    |   58307    |   58238    |\n",
            "|       from large pool |      47    |      47    |    4208    |    4161    |\n",
            "|       from small pool |      22    |      92    |   54099    |   54077    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      56    |      84    |    7571 K  |    7571 K  |\n",
            "|       from large pool |      46    |      47    |    4415 K  |    4415 K  |\n",
            "|       from small pool |      10    |      76    |    3155 K  |    3155 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:15 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  92% 1578/1720 [08:18<00:40,  3.50it/s, loss=4.597, nll_loss=4.597, ppl=24.2, wps=11872.1, ups=2.5, wpb=4740.7, bsz=256, num_updates=8400, lr=0.0001, gnorm=1.899, loss_scale=4, train_wall=31, gb_free=4.2, wall=3276]2022-07-19 07:15:16 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.10 GiB (GPU 0; 14.76 GiB total capacity; 8.69 GiB already allocated; 2.50 GiB free; 11.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:16 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1747         |        cudaMalloc retries: 2837      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8896 MB |    8929 MB |  111186 GB |  111177 GB |\n",
            "|       from large pool |    8852 MB |    8885 MB |  110350 GB |  110341 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     835 GB |     835 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8896 MB |    8929 MB |  111186 GB |  111177 GB |\n",
            "|       from large pool |    8852 MB |    8885 MB |  110350 GB |  110341 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     835 GB |     835 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11726 MB |   11814 MB |    6859 GB |    6847 GB |\n",
            "|       from large pool |   11682 MB |   11730 MB |    6753 GB |    6742 GB |\n",
            "|       from small pool |      44 MB |      84 MB |     105 GB |     105 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2829 MB |    2844 MB |  125705 GB |  125703 GB |\n",
            "|       from large pool |    2829 MB |    2844 MB |  124663 GB |  124660 GB |\n",
            "|       from small pool |       0 MB |       9 MB |    1042 GB |    1042 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     610    |   13421 K  |   13420 K  |\n",
            "|       from large pool |     261    |     263    |    7733 K  |    7733 K  |\n",
            "|       from small pool |     345    |     496    |    5687 K  |    5687 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     610    |   13421 K  |   13420 K  |\n",
            "|       from large pool |     261    |     263    |    7733 K  |    7733 K  |\n",
            "|       from small pool |     345    |     496    |    5687 K  |    5687 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      66    |      88    |   58327    |   58261    |\n",
            "|       from large pool |      44    |      46    |    4208    |    4164    |\n",
            "|       from small pool |      22    |      42    |   54119    |   54097    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      52    |      56    |    7574 K  |    7574 K  |\n",
            "|       from large pool |      38    |      41    |    4417 K  |    4417 K  |\n",
            "|       from small pool |      14    |      33    |    3156 K  |    3156 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:16 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  92% 1582/1720 [08:19<00:31,  4.35it/s, loss=4.597, nll_loss=4.597, ppl=24.2, wps=11872.1, ups=2.5, wpb=4740.7, bsz=256, num_updates=8400, lr=0.0001, gnorm=1.899, loss_scale=4, train_wall=31, gb_free=4.2, wall=3276]2022-07-19 07:15:17 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.64 GiB (GPU 0; 14.76 GiB total capacity; 7.58 GiB already allocated; 2.50 GiB free; 11.45 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:17 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1748         |        cudaMalloc retries: 2838      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7762 MB |    7785 MB |  111213 GB |  111206 GB |\n",
            "|       from large pool |    7719 MB |    7742 MB |  110377 GB |  110369 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     836 GB |     836 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7762 MB |    7785 MB |  111213 GB |  111206 GB |\n",
            "|       from large pool |    7719 MB |    7742 MB |  110377 GB |  110369 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     836 GB |     836 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11726 MB |   11814 MB |    6859 GB |    6847 GB |\n",
            "|       from large pool |   11682 MB |   11682 MB |    6753 GB |    6742 GB |\n",
            "|       from small pool |      44 MB |     132 MB |     105 GB |     105 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3963 MB |    3963 MB |  125744 GB |  125740 GB |\n",
            "|       from large pool |    3962 MB |    3962 MB |  124701 GB |  124697 GB |\n",
            "|       from small pool |       0 MB |      13 MB |    1042 GB |    1042 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     610    |   13426 K  |   13425 K  |\n",
            "|       from large pool |     261    |     263    |    7736 K  |    7735 K  |\n",
            "|       from small pool |     345    |     496    |    5689 K  |    5689 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     610    |   13426 K  |   13425 K  |\n",
            "|       from large pool |     261    |     263    |    7736 K  |    7735 K  |\n",
            "|       from small pool |     345    |     496    |    5689 K  |    5689 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      66    |     110    |   58371    |   58305    |\n",
            "|       from large pool |      44    |      44    |    4208    |    4164    |\n",
            "|       from small pool |      22    |      66    |   54163    |   54141    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      45    |      51    |    7577 K  |    7577 K  |\n",
            "|       from large pool |      32    |      33    |    4419 K  |    4419 K  |\n",
            "|       from small pool |      13    |      33    |    3158 K  |    3158 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:17 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  92% 1586/1720 [08:20<00:42,  3.13it/s, loss=4.597, nll_loss=4.597, ppl=24.2, wps=11872.1, ups=2.5, wpb=4740.7, bsz=256, num_updates=8400, lr=0.0001, gnorm=1.899, loss_scale=4, train_wall=31, gb_free=4.2, wall=3276]2022-07-19 07:15:18 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.62 GiB (GPU 0; 14.76 GiB total capacity; 9.32 GiB already allocated; 2.15 GiB free; 11.80 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:18 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1749         |        cudaMalloc retries: 2840      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    6861 MB |    9540 MB |  111262 GB |  111255 GB |\n",
            "|       from large pool |    6807 MB |    9486 MB |  110425 GB |  110418 GB |\n",
            "|       from small pool |      53 MB |      75 MB |     836 GB |     836 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    6861 MB |    9540 MB |  111262 GB |  111255 GB |\n",
            "|       from large pool |    6807 MB |    9486 MB |  110425 GB |  110418 GB |\n",
            "|       from small pool |      53 MB |      75 MB |     836 GB |     836 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12088 MB |   12088 MB |    6861 GB |    6850 GB |\n",
            "|       from large pool |   12032 MB |   12032 MB |    6756 GB |    6744 GB |\n",
            "|       from small pool |      56 MB |      78 MB |     105 GB |     105 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2546 MB |    3537 MB |  125798 GB |  125796 GB |\n",
            "|       from large pool |    2544 MB |    3535 MB |  124755 GB |  124753 GB |\n",
            "|       from small pool |       2 MB |       3 MB |    1043 GB |    1043 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     609    |     614    |   13431 K  |   13430 K  |\n",
            "|       from large pool |     239    |     243    |    7739 K  |    7738 K  |\n",
            "|       from small pool |     370    |     496    |    5691 K  |    5691 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     609    |     614    |   13431 K  |   13430 K  |\n",
            "|       from large pool |     239    |     243    |    7739 K  |    7738 K  |\n",
            "|       from small pool |     370    |     496    |    5691 K  |    5691 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      72    |      83    |   58389    |   58317    |\n",
            "|       from large pool |      44    |      44    |    4209    |    4165    |\n",
            "|       from small pool |      28    |      39    |   54180    |   54152    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      38    |      43    |    7580 K  |    7580 K  |\n",
            "|       from large pool |      20    |      25    |    4421 K  |    4421 K  |\n",
            "|       from small pool |      18    |      22    |    3159 K  |    3159 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:18 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  94% 1611/1720 [08:27<00:40,  2.66it/s, loss=4.597, nll_loss=4.597, ppl=24.2, wps=11872.1, ups=2.5, wpb=4740.7, bsz=256, num_updates=8400, lr=0.0001, gnorm=1.899, loss_scale=4, train_wall=31, gb_free=4.2, wall=3276]2022-07-19 07:15:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB (GPU 0; 14.76 GiB total capacity; 10.98 GiB already allocated; 173.75 MiB free; 13.78 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1750         |        cudaMalloc retries: 2843      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8392 MB |   11246 MB |  111505 GB |  111497 GB |\n",
            "|       from large pool |    8350 MB |   11204 MB |  110665 GB |  110657 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     839 GB |     839 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8392 MB |   11246 MB |  111505 GB |  111497 GB |\n",
            "|       from large pool |    8350 MB |   11204 MB |  110665 GB |  110657 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     839 GB |     839 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14114 MB |   14114 MB |    6871 GB |    6858 GB |\n",
            "|       from large pool |   14070 MB |   14070 MB |    6765 GB |    6752 GB |\n",
            "|       from small pool |      44 MB |     140 MB |     106 GB |     105 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2867 MB |    3538 MB |  126072 GB |  126069 GB |\n",
            "|       from large pool |    2865 MB |    3537 MB |  125025 GB |  125023 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1046 GB |    1046 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   13466 K  |   13466 K  |\n",
            "|       from large pool |     258    |     263    |    7758 K  |    7757 K  |\n",
            "|       from small pool |     350    |     496    |    5708 K  |    5708 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   13466 K  |   13466 K  |\n",
            "|       from large pool |     258    |     263    |    7758 K  |    7757 K  |\n",
            "|       from small pool |     350    |     496    |    5708 K  |    5708 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      67    |     115    |   58507    |   58440    |\n",
            "|       from large pool |      45    |      45    |    4214    |    4169    |\n",
            "|       from small pool |      22    |      70    |   54293    |   54271    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      56    |      57    |    7601 K  |    7601 K  |\n",
            "|       from large pool |      44    |      45    |    4432 K  |    4432 K  |\n",
            "|       from small pool |      12    |      15    |    3168 K  |    3168 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  94% 1612/1720 [08:28<00:38,  2.79it/s, loss=4.597, nll_loss=4.597, ppl=24.2, wps=11872.1, ups=2.5, wpb=4740.7, bsz=256, num_updates=8400, lr=0.0001, gnorm=1.899, loss_scale=4, train_wall=31, gb_free=4.2, wall=3276]2022-07-19 07:15:25 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.24 GiB (GPU 0; 14.76 GiB total capacity; 7.36 GiB already allocated; 2.96 GiB free; 11.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:25 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1751         |        cudaMalloc retries: 2844      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7531 MB |   11246 MB |  111512 GB |  111505 GB |\n",
            "|       from large pool |    7488 MB |   11204 MB |  110672 GB |  110665 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     839 GB |     839 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7531 MB |   11246 MB |  111512 GB |  111505 GB |\n",
            "|       from large pool |    7488 MB |   11204 MB |  110672 GB |  110665 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     839 GB |     839 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11260 MB |   14114 MB |    6871 GB |    6860 GB |\n",
            "|       from large pool |   11216 MB |   14070 MB |    6765 GB |    6754 GB |\n",
            "|       from small pool |      44 MB |     140 MB |     106 GB |     105 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3728 MB |    3728 MB |  126083 GB |  126080 GB |\n",
            "|       from large pool |    3727 MB |    3727 MB |  125036 GB |  125033 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1046 GB |    1046 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     606    |     613    |   13467 K  |   13466 K  |\n",
            "|       from large pool |     261    |     263    |    7758 K  |    7758 K  |\n",
            "|       from small pool |     345    |     496    |    5708 K  |    5708 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     606    |     613    |   13467 K  |   13466 K  |\n",
            "|       from large pool |     261    |     263    |    7758 K  |    7758 K  |\n",
            "|       from small pool |     345    |     496    |    5708 K  |    5708 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      66    |     115    |   58507    |   58441    |\n",
            "|       from large pool |      44    |      45    |    4214    |    4170    |\n",
            "|       from small pool |      22    |      70    |   54293    |   54271    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      51    |      57    |    7601 K  |    7601 K  |\n",
            "|       from large pool |      37    |      45    |    4432 K  |    4432 K  |\n",
            "|       from small pool |      14    |      15    |    3168 K  |    3168 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:25 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  94% 1613/1720 [08:28<00:30,  3.55it/s, loss=4.597, nll_loss=4.597, ppl=24.2, wps=11872.1, ups=2.5, wpb=4740.7, bsz=256, num_updates=8400, lr=0.0001, gnorm=1.899, loss_scale=4, train_wall=31, gb_free=4.2, wall=3276]2022-07-19 07:15:26 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.09 GiB (GPU 0; 14.76 GiB total capacity; 10.27 GiB already allocated; 2.95 GiB free; 11.00 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:26 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1752         |        cudaMalloc retries: 2845      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10514 MB |   11246 MB |  111524 GB |  111513 GB |\n",
            "|       from large pool |   10469 MB |   11204 MB |  110684 GB |  110674 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     839 GB |     839 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10514 MB |   11246 MB |  111524 GB |  111513 GB |\n",
            "|       from large pool |   10469 MB |   11204 MB |  110684 GB |  110674 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     839 GB |     839 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11262 MB |   14114 MB |    6871 GB |    6860 GB |\n",
            "|       from large pool |   11216 MB |   14070 MB |    6765 GB |    6754 GB |\n",
            "|       from small pool |      46 MB |     140 MB |     106 GB |     105 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  765249 KB |    4692 MB |  126097 GB |  126097 GB |\n",
            "|       from large pool |  763948 KB |    4690 MB |  125050 GB |  125050 GB |\n",
            "|       from small pool |    1301 KB |       3 MB |    1046 GB |    1046 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13467 K  |   13467 K  |\n",
            "|       from large pool |     261    |     263    |    7758 K  |    7758 K  |\n",
            "|       from small pool |     346    |     496    |    5708 K  |    5708 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13467 K  |   13467 K  |\n",
            "|       from large pool |     261    |     263    |    7758 K  |    7758 K  |\n",
            "|       from small pool |     346    |     496    |    5708 K  |    5708 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      67    |     115    |   58508    |   58441    |\n",
            "|       from large pool |      44    |      45    |    4214    |    4170    |\n",
            "|       from small pool |      23    |      70    |   54294    |   54271    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      63    |      63    |    7601 K  |    7601 K  |\n",
            "|       from large pool |      49    |      50    |    4432 K  |    4432 K  |\n",
            "|       from small pool |      14    |      15    |    3168 K  |    3168 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:26 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  95% 1631/1720 [08:32<00:17,  4.98it/s, loss=4.597, nll_loss=4.597, ppl=24.2, wps=11872.1, ups=2.5, wpb=4740.7, bsz=256, num_updates=8400, lr=0.0001, gnorm=1.899, loss_scale=4, train_wall=31, gb_free=4.2, wall=3276]2022-07-19 07:15:30 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.52 GiB (GPU 0; 14.76 GiB total capacity; 11.48 GiB already allocated; 2.00 GiB free; 11.95 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:30 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1753         |        cudaMalloc retries: 2846      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   11714 MB |   11821 MB |  111664 GB |  111652 GB |\n",
            "|       from large pool |   11666 MB |   11772 MB |  110822 GB |  110810 GB |\n",
            "|       from small pool |      48 MB |      75 MB |     841 GB |     841 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   11714 MB |   11821 MB |  111664 GB |  111652 GB |\n",
            "|       from large pool |   11666 MB |   11772 MB |  110822 GB |  110810 GB |\n",
            "|       from small pool |      48 MB |      75 MB |     841 GB |     841 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12238 MB |   12372 MB |    6873 GB |    6861 GB |\n",
            "|       from large pool |   12188 MB |   12188 MB |    6766 GB |    6754 GB |\n",
            "|       from small pool |      50 MB |     184 MB |     106 GB |     106 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |  536151 KB |    4461 MB |  126277 GB |  126276 GB |\n",
            "|       from large pool |  534494 KB |    4459 MB |  125227 GB |  125227 GB |\n",
            "|       from small pool |    1657 KB |      21 MB |    1049 GB |    1049 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     611    |   13493 K  |   13492 K  |\n",
            "|       from large pool |     261    |     263    |    7772 K  |    7772 K  |\n",
            "|       from small pool |     347    |     496    |    5720 K  |    5720 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     611    |   13493 K  |   13492 K  |\n",
            "|       from large pool |     261    |     263    |    7772 K  |    7772 K  |\n",
            "|       from small pool |     347    |     496    |    5720 K  |    5720 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      79    |     146    |   58587    |   58508    |\n",
            "|       from large pool |      54    |      54    |    4224    |    4170    |\n",
            "|       from small pool |      25    |      92    |   54363    |   54338    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      71    |      71    |    7617 K  |    7617 K  |\n",
            "|       from large pool |      59    |      59    |    4441 K  |    4441 K  |\n",
            "|       from small pool |      12    |      35    |    3175 K  |    3175 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:30 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  95% 1635/1720 [08:33<00:23,  3.62it/s, loss=4.597, nll_loss=4.597, ppl=24.2, wps=11872.1, ups=2.5, wpb=4740.7, bsz=256, num_updates=8400, lr=0.0001, gnorm=1.899, loss_scale=4, train_wall=31, gb_free=4.2, wall=3276]2022-07-19 07:15:31 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 6.77 GiB (GPU 0; 14.76 GiB total capacity; 10.55 GiB already allocated; 2.02 GiB free; 11.93 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:31 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1754         |        cudaMalloc retries: 2847      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10799 MB |   10815 MB |  111699 GB |  111688 GB |\n",
            "|       from large pool |   10754 MB |   10770 MB |  110856 GB |  110846 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     842 GB |     842 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10799 MB |   10815 MB |  111699 GB |  111688 GB |\n",
            "|       from large pool |   10754 MB |   10770 MB |  110856 GB |  110846 GB |\n",
            "|       from small pool |      44 MB |      75 MB |     842 GB |     842 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12218 MB |   12336 MB |    6873 GB |    6861 GB |\n",
            "|       from large pool |   12172 MB |   12188 MB |    6766 GB |    6754 GB |\n",
            "|       from small pool |      46 MB |     148 MB |     106 GB |     106 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1418 MB |    1976 MB |  126321 GB |  126319 GB |\n",
            "|       from large pool |    1417 MB |    1976 MB |  125270 GB |  125269 GB |\n",
            "|       from small pool |       1 MB |       4 MB |    1050 GB |    1050 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     611    |   13498 K  |   13497 K  |\n",
            "|       from large pool |     261    |     263    |    7774 K  |    7774 K  |\n",
            "|       from small pool |     346    |     496    |    5723 K  |    5723 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     611    |   13498 K  |   13497 K  |\n",
            "|       from large pool |     261    |     263    |    7774 K  |    7774 K  |\n",
            "|       from small pool |     346    |     496    |    5723 K  |    5723 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      76    |     128    |   58636    |   58560    |\n",
            "|       from large pool |      53    |      54    |    4224    |    4171    |\n",
            "|       from small pool |      23    |      74    |   54412    |   54389    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      52    |      53    |    7619 K  |    7619 K  |\n",
            "|       from large pool |      38    |      39    |    4442 K  |    4442 K  |\n",
            "|       from small pool |      14    |      17    |    3177 K  |    3177 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:31 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  96% 1646/1720 [08:37<00:28,  2.57it/s, loss=4.597, nll_loss=4.597, ppl=24.2, wps=11872.1, ups=2.5, wpb=4740.7, bsz=256, num_updates=8400, lr=0.0001, gnorm=1.899, loss_scale=4, train_wall=31, gb_free=4.2, wall=3276]2022-07-19 07:15:35 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.96 GiB (GPU 0; 14.76 GiB total capacity; 11.71 GiB already allocated; 311.75 MiB free; 13.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:35 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1755         |        cudaMalloc retries: 2851      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8967 MB |   11995 MB |  111848 GB |  111839 GB |\n",
            "|       from large pool |    8924 MB |   11952 MB |  111005 GB |  110996 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     843 GB |     843 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8967 MB |   11995 MB |  111848 GB |  111839 GB |\n",
            "|       from large pool |    8924 MB |   11952 MB |  111005 GB |  110996 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     843 GB |     843 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   13976 MB |   13976 MB |    6895 GB |    6882 GB |\n",
            "|       from large pool |   13932 MB |   13932 MB |    6789 GB |    6775 GB |\n",
            "|       from small pool |      44 MB |     164 MB |     106 GB |     106 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1980 MB |    2803 MB |  126453 GB |  126451 GB |\n",
            "|       from large pool |    1979 MB |    2801 MB |  125402 GB |  125400 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1051 GB |    1051 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13513 K  |   13513 K  |\n",
            "|       from large pool |     258    |     263    |    7783 K  |    7783 K  |\n",
            "|       from small pool |     349    |     496    |    5729 K  |    5729 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13513 K  |   13513 K  |\n",
            "|       from large pool |     258    |     263    |    7783 K  |    7783 K  |\n",
            "|       from small pool |     349    |     496    |    5729 K  |    5729 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      61    |     121    |   58725    |   58664    |\n",
            "|       from large pool |      39    |      39    |    4236    |    4197    |\n",
            "|       from small pool |      22    |      82    |   54489    |   54467    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      50    |      52    |    7628 K  |    7628 K  |\n",
            "|       from large pool |      40    |      40    |    4447 K  |    4447 K  |\n",
            "|       from small pool |      10    |      19    |    3180 K  |    3180 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:35 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  97% 1661/1720 [08:43<00:23,  2.55it/s, loss=4.427, nll_loss=4.427, ppl=21.51, wps=13392.5, ups=3, wpb=4469.5, bsz=256, num_updates=8500, lr=0.0001, gnorm=1.817, loss_scale=4, train_wall=30, gb_free=6.9, wall=3309]2022-07-19 07:15:41 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.50 GiB (GPU 0; 14.76 GiB total capacity; 10.16 GiB already allocated; 1.63 GiB free; 12.32 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:41 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1756         |        cudaMalloc retries: 2856      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7846 MB |   10408 MB |  112041 GB |  112033 GB |\n",
            "|       from large pool |    7803 MB |   10365 MB |  111196 GB |  111189 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     844 GB |     844 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7846 MB |   10408 MB |  112041 GB |  112033 GB |\n",
            "|       from large pool |    7803 MB |   10365 MB |  111196 GB |  111189 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     844 GB |     844 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   12616 MB |   12616 MB |    6919 GB |    6907 GB |\n",
            "|       from large pool |   12572 MB |   12572 MB |    6813 GB |    6801 GB |\n",
            "|       from small pool |      44 MB |      78 MB |     106 GB |     106 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2207 MB |    2260 MB |  126631 GB |  126629 GB |\n",
            "|       from large pool |    2206 MB |    2258 MB |  125578 GB |  125576 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1052 GB |    1052 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13535 K  |   13534 K  |\n",
            "|       from large pool |     258    |     263    |    7796 K  |    7796 K  |\n",
            "|       from small pool |     349    |     495    |    5738 K  |    5738 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13535 K  |   13534 K  |\n",
            "|       from large pool |     258    |     263    |    7796 K  |    7796 K  |\n",
            "|       from small pool |     349    |     495    |    5738 K  |    5738 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      54    |      71    |   58845    |   58791    |\n",
            "|       from large pool |      32    |      32    |    4260    |    4228    |\n",
            "|       from small pool |      22    |      39    |   54585    |   54563    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      48    |      50    |    7641 K  |    7641 K  |\n",
            "|       from large pool |      39    |      39    |    4455 K  |    4455 K  |\n",
            "|       from small pool |       9    |      20    |    3186 K  |    3186 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:41 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  97% 1663/1720 [08:44<00:18,  3.07it/s, loss=4.427, nll_loss=4.427, ppl=21.51, wps=13392.5, ups=3, wpb=4469.5, bsz=256, num_updates=8500, lr=0.0001, gnorm=1.817, loss_scale=4, train_wall=30, gb_free=6.9, wall=3309]2022-07-19 07:15:42 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.36 GiB (GPU 0; 14.76 GiB total capacity; 9.18 GiB already allocated; 3.32 GiB free; 10.63 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:42 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1757         |        cudaMalloc retries: 2857      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9400 MB |   11149 MB |  112057 GB |  112048 GB |\n",
            "|       from large pool |    9357 MB |   11106 MB |  111212 GB |  111203 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     844 GB |     844 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9400 MB |   11149 MB |  112057 GB |  112048 GB |\n",
            "|       from large pool |    9357 MB |   11106 MB |  111212 GB |  111203 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     844 GB |     844 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10888 MB |   13526 MB |    6923 GB |    6912 GB |\n",
            "|       from large pool |   10844 MB |   13446 MB |    6816 GB |    6806 GB |\n",
            "|       from small pool |      44 MB |      80 MB |     106 GB |     106 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1487 MB |    2331 MB |  126647 GB |  126645 GB |\n",
            "|       from large pool |    1486 MB |    2330 MB |  125593 GB |  125592 GB |\n",
            "|       from small pool |       0 MB |       8 MB |    1053 GB |    1053 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13537 K  |   13536 K  |\n",
            "|       from large pool |     258    |     263    |    7797 K  |    7797 K  |\n",
            "|       from small pool |     349    |     496    |    5739 K  |    5739 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13537 K  |   13536 K  |\n",
            "|       from large pool |     258    |     263    |    7797 K  |    7797 K  |\n",
            "|       from small pool |     349    |     496    |    5739 K  |    5739 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      51    |      72    |   58864    |   58813    |\n",
            "|       from large pool |      29    |      32    |    4261    |    4232    |\n",
            "|       from small pool |      22    |      40    |   54603    |   54581    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      43    |      46    |    7642 K  |    7642 K  |\n",
            "|       from large pool |      35    |      37    |    4455 K  |    4455 K  |\n",
            "|       from small pool |       8    |      27    |    3186 K  |    3186 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:42 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  97% 1666/1720 [08:45<00:16,  3.30it/s, loss=4.427, nll_loss=4.427, ppl=21.51, wps=13392.5, ups=3, wpb=4469.5, bsz=256, num_updates=8500, lr=0.0001, gnorm=1.817, loss_scale=4, train_wall=30, gb_free=6.9, wall=3309]2022-07-19 07:15:43 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.41 GiB (GPU 0; 14.76 GiB total capacity; 8.88 GiB already allocated; 3.32 GiB free; 10.64 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:43 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1758         |        cudaMalloc retries: 2858      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    9058 MB |    9128 MB |  112089 GB |  112080 GB |\n",
            "|       from large pool |    9012 MB |    9082 MB |  111244 GB |  111235 GB |\n",
            "|       from small pool |      46 MB |      75 MB |     844 GB |     844 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    9058 MB |    9128 MB |  112089 GB |  112080 GB |\n",
            "|       from large pool |    9012 MB |    9082 MB |  111244 GB |  111235 GB |\n",
            "|       from small pool |      46 MB |      75 MB |     844 GB |     844 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   10892 MB |   10922 MB |    6923 GB |    6912 GB |\n",
            "|       from large pool |   10844 MB |   10844 MB |    6816 GB |    6806 GB |\n",
            "|       from small pool |      48 MB |      78 MB |     106 GB |     106 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    1833 MB |    3650 MB |  126686 GB |  126684 GB |\n",
            "|       from large pool |    1831 MB |    3649 MB |  125633 GB |  125631 GB |\n",
            "|       from small pool |       1 MB |       8 MB |    1053 GB |    1053 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     611    |   13540 K  |   13540 K  |\n",
            "|       from large pool |     261    |     263    |    7799 K  |    7799 K  |\n",
            "|       from small pool |     347    |     496    |    5741 K  |    5740 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     611    |   13540 K  |   13540 K  |\n",
            "|       from large pool |     261    |     263    |    7799 K  |    7799 K  |\n",
            "|       from small pool |     347    |     496    |    5741 K  |    5740 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      53    |      68    |   58881    |   58828    |\n",
            "|       from large pool |      29    |      29    |    4261    |    4232    |\n",
            "|       from small pool |      24    |      39    |   54620    |   54596    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      55    |      56    |    7644 K  |    7644 K  |\n",
            "|       from large pool |      43    |      44    |    4456 K  |    4456 K  |\n",
            "|       from small pool |      12    |      21    |    3187 K  |    3187 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:43 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  99% 1696/1720 [08:55<00:07,  3.12it/s, loss=4.427, nll_loss=4.427, ppl=21.51, wps=13392.5, ups=3, wpb=4469.5, bsz=256, num_updates=8500, lr=0.0001, gnorm=1.817, loss_scale=4, train_wall=30, gb_free=6.9, wall=3309]2022-07-19 07:15:53 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB (GPU 0; 14.76 GiB total capacity; 10.87 GiB already allocated; 255.75 MiB free; 13.70 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:53 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1759         |        cudaMalloc retries: 2861      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8273 MB |   11127 MB |  112452 GB |  112444 GB |\n",
            "|       from large pool |    8230 MB |   11084 MB |  111605 GB |  111597 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     846 GB |     846 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8273 MB |   11127 MB |  112452 GB |  112444 GB |\n",
            "|       from large pool |    8230 MB |   11084 MB |  111605 GB |  111597 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     846 GB |     846 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14032 MB |   14032 MB |    6932 GB |    6919 GB |\n",
            "|       from large pool |   13988 MB |   13988 MB |    6825 GB |    6812 GB |\n",
            "|       from small pool |      44 MB |      84 MB |     106 GB |     106 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2904 MB |    2904 MB |  127067 GB |  127064 GB |\n",
            "|       from large pool |    2903 MB |    2903 MB |  126012 GB |  126009 GB |\n",
            "|       from small pool |       1 MB |       3 MB |    1055 GB |    1055 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13584 K  |   13584 K  |\n",
            "|       from large pool |     258    |     263    |    7826 K  |    7826 K  |\n",
            "|       from small pool |     349    |     496    |    5758 K  |    5758 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13584 K  |   13584 K  |\n",
            "|       from large pool |     258    |     263    |    7826 K  |    7826 K  |\n",
            "|       from small pool |     349    |     496    |    5758 K  |    5758 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      52    |      72    |   58922    |   58870    |\n",
            "|       from large pool |      30    |      30    |    4265    |    4235    |\n",
            "|       from small pool |      22    |      42    |   54657    |   54635    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      46    |      47    |    7668 K  |    7668 K  |\n",
            "|       from large pool |      35    |      35    |    4471 K  |    4471 K  |\n",
            "|       from small pool |      11    |      17    |    3196 K  |    3196 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:53 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006:  99% 1707/1720 [08:58<00:03,  3.87it/s, loss=4.427, nll_loss=4.427, ppl=21.51, wps=13392.5, ups=3, wpb=4469.5, bsz=256, num_updates=8500, lr=0.0001, gnorm=1.817, loss_scale=4, train_wall=30, gb_free=6.9, wall=3309]2022-07-19 07:15:56 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.27 GiB (GPU 0; 14.76 GiB total capacity; 10.62 GiB already allocated; 2.56 GiB free; 11.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:56 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1760         |        cudaMalloc retries: 2863      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |   10873 MB |   13097 MB |  112568 GB |  112557 GB |\n",
            "|       from large pool |   10829 MB |   13053 MB |  111720 GB |  111710 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     847 GB |     847 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |   10873 MB |   13097 MB |  112568 GB |  112557 GB |\n",
            "|       from large pool |   10829 MB |   13053 MB |  111720 GB |  111710 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     847 GB |     847 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11664 MB |   14110 MB |    6939 GB |    6927 GB |\n",
            "|       from large pool |   11618 MB |   14064 MB |    6832 GB |    6820 GB |\n",
            "|       from small pool |      46 MB |      84 MB |     106 GB |     106 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |     788 MB |    2182 MB |  127187 GB |  127186 GB |\n",
            "|       from large pool |     788 MB |    2180 MB |  126130 GB |  126129 GB |\n",
            "|       from small pool |       0 MB |       7 MB |    1056 GB |    1056 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     607    |     613    |   13600 K  |   13599 K  |\n",
            "|       from large pool |     258    |     263    |    7835 K  |    7835 K  |\n",
            "|       from small pool |     349    |     496    |    5764 K  |    5764 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     607    |     613    |   13600 K  |   13599 K  |\n",
            "|       from large pool |     258    |     263    |    7835 K  |    7835 K  |\n",
            "|       from small pool |     349    |     496    |    5764 K  |    5764 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      50    |      72    |   58945    |   58895    |\n",
            "|       from large pool |      27    |      30    |    4267    |    4240    |\n",
            "|       from small pool |      23    |      42    |   54678    |   54655    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      52    |      55    |    7676 K  |    7676 K  |\n",
            "|       from large pool |      44    |      45    |    4476 K  |    4476 K  |\n",
            "|       from small pool |       8    |      24    |    3199 K  |    3199 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:56 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006: 100% 1712/1720 [09:00<00:03,  2.47it/s, loss=4.427, nll_loss=4.427, ppl=21.51, wps=13392.5, ups=3, wpb=4469.5, bsz=256, num_updates=8500, lr=0.0001, gnorm=1.817, loss_scale=4, train_wall=30, gb_free=6.9, wall=3309]2022-07-19 07:15:58 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.02 GiB (GPU 0; 14.76 GiB total capacity; 8.78 GiB already allocated; 2.56 GiB free; 11.39 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:58 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1761         |        cudaMalloc retries: 2864      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    8994 MB |   10566 MB |  112638 GB |  112630 GB |\n",
            "|       from large pool |    8951 MB |   10523 MB |  111791 GB |  111782 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     847 GB |     847 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    8994 MB |   10566 MB |  112638 GB |  112630 GB |\n",
            "|       from large pool |    8951 MB |   10523 MB |  111791 GB |  111782 GB |\n",
            "|       from small pool |      43 MB |      75 MB |     847 GB |     847 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   11662 MB |   13800 MB |    6941 GB |    6929 GB |\n",
            "|       from large pool |   11618 MB |   13716 MB |    6834 GB |    6822 GB |\n",
            "|       from small pool |      44 MB |      84 MB |     106 GB |     106 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    2667 MB |    3222 MB |  127261 GB |  127258 GB |\n",
            "|       from large pool |    2666 MB |    3221 MB |  126204 GB |  126201 GB |\n",
            "|       from small pool |       0 MB |       3 MB |    1056 GB |    1056 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   13606 K  |   13606 K  |\n",
            "|       from large pool |     258    |     263    |    7839 K  |    7839 K  |\n",
            "|       from small pool |     350    |     496    |    5767 K  |    5766 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   13606 K  |   13606 K  |\n",
            "|       from large pool |     258    |     263    |    7839 K  |    7839 K  |\n",
            "|       from small pool |     350    |     496    |    5767 K  |    5766 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      49    |      70    |   58966    |   58917    |\n",
            "|       from large pool |      27    |      28    |    4268    |    4241    |\n",
            "|       from small pool |      22    |      42    |   54698    |   54676    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      47    |      48    |    7680 K  |    7680 K  |\n",
            "|       from large pool |      37    |      38    |    4479 K  |    4479 K  |\n",
            "|       from small pool |      10    |      22    |    3201 K  |    3201 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:58 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006: 100% 1715/1720 [09:01<00:01,  3.25it/s, loss=4.427, nll_loss=4.427, ppl=21.51, wps=13392.5, ups=3, wpb=4469.5, bsz=256, num_updates=8500, lr=0.0001, gnorm=1.817, loss_scale=4, train_wall=30, gb_free=6.9, wall=3309]2022-07-19 07:15:59 | WARNING | fairseq.trainer | OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.45 GiB (GPU 0; 14.76 GiB total capacity; 10.19 GiB already allocated; 121.75 MiB free; 13.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "2022-07-19 07:15:59 | WARNING | fairseq.trainer | |===========================================================================|\n",
            "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
            "|---------------------------------------------------------------------------|\n",
            "|            CUDA OOMs: 1762         |        cudaMalloc retries: 2865      |\n",
            "|===========================================================================|\n",
            "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocated memory      |    7927 MB |   10431 MB |  112667 GB |  112660 GB |\n",
            "|       from large pool |    7885 MB |   10389 MB |  111819 GB |  111812 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     847 GB |     847 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active memory         |    7927 MB |   10431 MB |  112667 GB |  112660 GB |\n",
            "|       from large pool |    7885 MB |   10389 MB |  111819 GB |  111812 GB |\n",
            "|       from small pool |      42 MB |      75 MB |     847 GB |     847 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved memory   |   14166 MB |   14204 MB |    6943 GB |    6929 GB |\n",
            "|       from large pool |   14122 MB |   14122 MB |    6836 GB |    6822 GB |\n",
            "|       from small pool |      44 MB |      82 MB |     106 GB |     106 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable memory |    3734 MB |    3734 MB |  127293 GB |  127290 GB |\n",
            "|       from large pool |    3732 MB |    3732 MB |  126236 GB |  126233 GB |\n",
            "|       from small pool |       1 MB |      18 MB |    1057 GB |    1057 GB |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Allocations           |     608    |     613    |   13610 K  |   13609 K  |\n",
            "|       from large pool |     258    |     263    |    7841 K  |    7841 K  |\n",
            "|       from small pool |     350    |     496    |    5768 K  |    5768 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Active allocs         |     608    |     613    |   13610 K  |   13609 K  |\n",
            "|       from large pool |     258    |     263    |    7841 K  |    7841 K  |\n",
            "|       from small pool |     350    |     496    |    5768 K  |    5768 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| GPU reserved segments |      50    |      69    |   58986    |   58936    |\n",
            "|       from large pool |      28    |      28    |    4269    |    4241    |\n",
            "|       from small pool |      22    |      41    |   54717    |   54695    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Non-releasable allocs |      53    |      53    |    7682 K  |    7682 K  |\n",
            "|       from large pool |      43    |      43    |    4480 K  |    4480 K  |\n",
            "|       from small pool |      10    |      31    |    3201 K  |    3201 K  |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
            "|---------------------------------------------------------------------------|\n",
            "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
            "|===========================================================================|\n",
            "\n",
            "2022-07-19 07:15:59 | WARNING | fairseq.trainer | attempting to recover from OOM in forward/backward pass\n",
            "epoch 006: 100% 1719/1720 [09:02<00:00,  3.32it/s, loss=4.427, nll_loss=4.427, ppl=21.51, wps=13392.5, ups=3, wpb=4469.5, bsz=256, num_updates=8500, lr=0.0001, gnorm=1.817, loss_scale=4, train_wall=30, gb_free=6.9, wall=3309]2022-07-19 07:16:00 | INFO | fairseq_cli.train | begin validation on \"valid\" subset\n",
            "\n",
            "epoch 006 | valid on 'valid' subset:   0% 0/6 [00:00<?, ?it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  17% 1/6 [00:00<00:00,  5.15it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  50% 3/6 [00:00<00:00,  7.82it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  67% 4/6 [00:00<00:00,  6.38it/s]\u001b[A\n",
            "epoch 006 | valid on 'valid' subset:  83% 5/6 [00:00<00:00,  5.82it/s]\u001b[A\n",
            "                                                                      \u001b[A2022-07-19 07:16:01 | INFO | valid | epoch 006 | valid on 'valid' subset | loss 5.329 | nll_loss 5.329 | ppl 40.21 | wps 32000 | wpb 3717.5 | bsz 194.3 | num_updates 8553 | best_loss 5.329\n",
            "2022-07-19 07:16:01 | INFO | fairseq.checkpoint_utils | Preparing to save checkpoint for epoch 6 @ 8553 updates\n",
            "2022-07-19 07:16:01 | INFO | fairseq.trainer | Saving checkpoint to /content/drive/MyDrive/Colab Notebooks/result/train/checkpoint6.pt\n",
            "2022-07-19 07:16:11 | INFO | fairseq.trainer | Finished saving checkpoint to /content/drive/MyDrive/Colab Notebooks/result/train/checkpoint6.pt\n",
            "2022-07-19 07:16:41 | INFO | fairseq.checkpoint_utils | Saved checkpoint result/train/checkpoint6.pt (epoch 6 @ 8553 updates, score 5.329) (writing took 40.51624300500043 seconds)\n",
            "2022-07-19 07:16:41 | INFO | fairseq_cli.train | end of epoch 6 (average epoch stats below)\n",
            "2022-07-19 07:16:41 | INFO | train | epoch 006 | loss 4.578 | nll_loss 4.578 | ppl 23.89 | wps 11569.7 | ups 2.45 | wpb 4722.4 | bsz 256 | num_updates 8553 | lr 0.0001 | gnorm 1.738 | loss_scale 4 | train_wall 442 | gb_free 10.4 | wall 3370\n",
            "2022-07-19 07:16:42 | INFO | fairseq_cli.train | done training in 3370.2 seconds\n"
          ]
        }
      ],
      "source": [
        "# !fairseq-train result/preprocessing/ \\\n",
        "#   --task translation \\\n",
        "#   --arch transformer \\\n",
        "#   --save-dir result/train \\\n",
        "#   --source-lang ja --target-lang en \\\n",
        "#   --optimizer adam \\\n",
        "#   --max-epoch 3 \\\n",
        "#   --lr 1e-5 \\\n",
        "#   --batch-size 64 \\\n",
        "#   --fp16\n",
        "# !fairseq-train result/preprocessing/ \\\n",
        "#   --task translation \\\n",
        "#   --arch transformer \\\n",
        "#   --save-dir result/train \\\n",
        "#   --source-lang ja \\\n",
        "#   --target-lang en \\\n",
        "#   --optimizer adam \\\n",
        "#   --tokenizer space \\\n",
        "#   --max-epoch 3 \\\n",
        "#   --lr 1e-5 \\\n",
        "#   --batch-size 64 \\\n",
        "#   --patience 2 \\\n",
        "    # --share-decoder-input-output-embed \\\n",
        "    # --clip-norm 1.0 \\\n",
        "    # --lr-scheduler inverse_sqrt \\\n",
        "    # --warmup-updates 2000 \\\n",
        "    # --dropout 0.2 \\\n",
        "    # --label-smoothing 0.1 \\\n",
        "    # --max-tokens 8000 > 91.log\n",
        "\n",
        "%env CUDA_VISIBLE_DEVICES=0 \n",
        "!fairseq-train result/preprocessing \\\n",
        "    --fp16 \\\n",
        "    --save-dir result/train \\\n",
        "    --max-epoch 6 \\\n",
        "    --arch transformer \\\n",
        "    --optimizer adam \\\n",
        "    --lr 1e-4 \\\n",
        "    --weight-decay 0.0001 \\\n",
        "    --criterion label_smoothed_cross_entropy \\\n",
        "    --batch-size 256 \\\n",
        "    --save-interval 2 \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGsvWHVq2yHr",
        "outputId": "2a20ac50-08ee-4039-c66c-018eef917ecf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[K     |████████████████████████████████| 125 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
            "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip3 install tensorboardX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz3p80CV5n78"
      },
      "source": [
        "# 92"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HeEpXDn9x8Vr",
        "outputId": "58b0f15b-9390-47a5-a2e4-c657f40643ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2022-07-19 10:24:26 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'result/train/checkpoint_last.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'result/preprocessing', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-07-19 10:24:26 | INFO | fairseq.tasks.translation | [ja] dictionary: 62704 types\n",
            "2022-07-19 10:24:26 | INFO | fairseq.tasks.translation | [en] dictionary: 59624 types\n",
            "2022-07-19 10:24:26 | INFO | fairseq_cli.interactive | loading model(s) from result/train/checkpoint_last.pt\n",
            "2022-07-19 10:24:53 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-07-19 10:24:53 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-07-19 10:28:13 | INFO | fairseq_cli.interactive | Total time: 227.696 seconds; translation time: 193.864\n"
          ]
        }
      ],
      "source": [
        "# !fairseq-interactive --path result/checkpoint_best.pt result/preprocessing < ./test.ja | grep '^H' | cut -f3 > 92.out\n",
        "!fairseq-interactive --path result/train/checkpoint_last.pt result/preprocessing < ./tokenized_data/test.ja | grep '^H' | cut -f3 > 92.out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dflsYrp4av9",
        "outputId": "27794282-ffa5-4d27-cd4d-565343892cbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "infobox buddhist\n",
            "dogen was a zen monk in the early kamakura period  \n",
            "the founder of soto zen\n",
            "later in his life he also went by the name kigen  \n",
            "within the sect he is referred to by the honorary title koso  \n",
            "posthumously named bussho dento kokushi   or joyo daishi  \n",
            "he is generally called dogen zenji  \n",
            "he is reputed to have been the one that spread the practices of tooth brushing   face washing   table manners and cleaning in japan  \n",
            "another story has it that he was the first one to bring moso chiku   moso bamboo   to japan  \n",
            "though some points are unclear about dogen  s birth   all accounts agree that he was born in the line of udaijin   minister of the right   michichika tsuchimikado   minamoto no michichika or michichika koga    \n",
            "although it is generally accepted that he was born in shoden sanso in kohata   kyoto   to michichika and fujiwara no ishi   the daughter of daijo daijin   grand minister of state   motofusa matsudono   fujiwara no motofusa     recent research suggests that he may have been the son of michitomo horikawa   who was presumed to be his adoptive father  \n",
            "another account says his father was the son of michichika   minamoto no michimune or michiteru koga  \n",
            "according to the biography   kenzeiki     he lost his father   michichika   at 0 years of age   his mother at 0   and was adopted by his half brother michitomo horikawa  \n",
            "yet another account tells that his maternal uncle moroie matsudono   former regent and interior minister   wanted to adopt him as an heir after his parents died   but dogen   feeling the uncertainty of the world   declined  \n",
            "there is a story that he was acquainted with shinran   the founder of shin buddhism   as they were related via their mothers   but there is no proof  \n",
            "some have said that the   life and death   section of his   shobogenzo   was written for shinran  \n",
            "kenryaku 0   0     visited his maternal uncle ryoken at mt   hiei  \n",
            "0   entered the priesthood under koen   the head of tendai buddhism   and took the name buppobo dogen  \n",
            "0   pursued tendai religious studies under koin at onjo ji temple  \n",
            "0   studied under myozen   a disciple of eisai   at kennin ji temple  \n",
            "0   left from hakata with myozen to southern song china and visited jozan   where he received a certificate of dharma transmission from soto zen  s tendo nyozo  \n",
            "0   returned to japan  \n",
            "0   opened kosho ji temple at fukakusa   kyoto  \n",
            "around this time he came under pressure from mt   hiei  \n",
            "july   0   moved to echizen shihi no sho at the invitation of yoshishige hatano   lord of echizen province  \n",
            "on the way   he stopped in kutsuki at the invitation of the village chief   nobutsuna sasaki   this is the origin of kosho ji temple in takashima city    \n",
            "0   opened daibutsu ji temple in kasamatsu  \n",
            "0   changed the name of daibutsu ji to eihei ji temple   and his own name to kigen  \n",
            "around this time he went to kamakura to teach   at the invitation of regent tokiyori hojo and yoshishige hatano  \n",
            "although he only spent half of a year in kamakura   it was the beginning of a genuine flourishing of zen in the kanto region  \n",
            "0   after falling ill and turning over eihei ji to his disciple koun ejo   he died at the house   takatsuji dori nishinotoin dori   kyoto   of a lay disciple   kakunen   at age 0  \n",
            "cause of death was a boil  \n",
            "he emphasized that buddhahood is not something that is completed once one has attained a certain level   rather   continuing to seek further buddhahood through endless training   even after one has become a buddha   is the essence of buddhahood   shusho ichinyo     and the highest form of training is to be like the sakyamuni and simply do sitting meditation   shikan taza    \n",
            "most of kamakura buddhism affirmed the notion of the decadent age   but in his shobogenzo zuimonki he wrote   i tell you now   this is completely false  \n",
            "the idea of the decline of true buddha dharma is only a temporary means for teaching  \n",
            "the true teaching is different  \n",
            "by training according to the teaching   anybody can achieve enlightenment  \n",
            "the monks at the time of the sakyamuni were not all outstanding people  \n",
            "there were those who had rare and incredible evil in their hearts   and there were fools  \n",
            "the many rules that the saykamuni made were for the bad and foolish people  \n",
            "people are all owners of the quality to be able to become buddha  \n",
            "don  t think that you are not qualified   if you train according to the teaching   you will surely attain enlightenment     he thus noted that the disciples at the time of sakyamuni were not all outstanding people   denying the decadent age by saying that it is nothing more than a skillful means  \n",
            "  shobogenzo  \n",
            "  shobogenzo   shobogenzo zuimonki   japanese classic literature collection 0   commentary by minoru nishio et alia   iwanami shoten   0  \n",
            "  shobogenzo in modern japanese   0 0     translation by wafu nishijima   kanazawa bunko   0  \n",
            "  shobogenzo   0 0     commentary by yahoko mizuno   iwanami bunko   0  \n",
            "  shobogenzo   translated by kyoji ishii   kawade bunko   0  \n",
            "  eihei koroku   translated by kyoji ishii   kawade shobo shinsha   0  \n",
            "  eihei shingi  \n",
            "  tenzo kyokun  \n",
            "  tenzo kyokun   fushuku hanho   translation and commentary by shohachi nakamura et alia   kodansha scholastic collection   0  \n",
            "  fushuku hanho  \n",
            "  shobogenzo zuimonki   ejo edition   lectures by dogen  \n",
            "  shobogenzo zuimonki   new annotated edition   commentary by doshu okubo   sankibo buddhist books   0  \n",
            "  shobogenzo zuimonki   translation and commentary by shokin furuta   kadokawa bunko   0  \n",
            "  shobogenzo zuimonki   edited by tetsuro watsuji   iwanami bunko   0 revised edition  \n",
            "  shobogenzo zuimonki   translated by yahoko mizuno   chikuma scholastic collection   0  \n",
            "  shobogenzo zuimonki in modern translation   translated by rosan ikeda   okura publishing   0  \n",
            "  shobogenzo zuimonki   translation and commentary by masakazu yamazaki   kodansha scholastic collection   0  \n",
            "  the complete works of dogen zenji   supervised by genryu kagamishima   shunjusha publishing  \n",
            "reference works\n",
            "ton satomi     the story of dogen zenji   iwanami bunko\n",
            "michio takeuchi     dogen   yoshikawa kobunkan   biographical series     0\n",
            "shinkichi takahashi     the life of dogen zenji   hobunkan   0\n",
            "taijo tamamuro     dogen   shin jinbutsuoraisha   0\n",
            "aishin imaeda     dogen   a shamon  s life of zazen   nhk books   0\n",
            "akira suganuma     an encyclopedia of dogen   tokyodo publishing   0\n",
            "genryu kagamishima and koshiro tamaki   editors     dogen lectures   0 volumes   shunjusha   0 0\n",
            "koshiro tamaki     dogen   shunjusha   0\n",
            "ryotaro shiba     dogen   the roads of echizen     in 0 on the road no   0 0   asahi shimbun\n",
            "ryotaro shiba   a monk in the mountains   ibid  no   0 0\n",
            "ryotaro shiba   the itinerant priest of hokyo ji temple   ibid  no   0 0\n",
            "ryotaro shiba   the portrait of jakuen   ibid  no   0 0\n",
            "wahei tatematsu     dogen zenji   tokyo shoseki   0\n",
            "the umekoji steam locomotive museum is a preservation and display facility for steam locomotives   operated by west japan railway company   jr west japan     located in kankiji cho   shimogyo ku   kyoto city   kyoto prefecture  \n",
            "on october 0   0   the japan national railway opened a semicircular garage at the umekoji engine yard in shimogyou ku   kyoto in celebration of the 0th anniversary of the first railway in japan   the first of its kind   this facility preserves working steam locomotives   honoring them as precious artifacts of industrial culture that supported the japanese rail transport system  \n",
            "jr west japan took over the facility in 0   following the breakup and privatization of japanese national railways  \n",
            "the facility itself is also an active depot   umekoji railyard     and carries out inspections and maintenance on steam locomotives and japanese national railways diesel locomotive class de0 owned by sagano scenic railway  \n",
            "initially   this facility was established for working preservation of steam locomotives   which had begun disappearing rapidly in the latter half of the 0s  \n",
            "the koyama engine yard at koyama station in tochigi prefecture was the best candidate for the locomotive preservation area because it was close to the capital   but the umekoji engine yard was chosen in 0 for its central location in japan and the presence of other historic spots in the area  \n",
            "reviews had been made to select   in principle   lowest numbered extant steam locomotives     0 if possible   for preservation   but there are some steam locomotives   such as the class c0   0   that were not originally planned for preservation  \n",
            "the facilities consist of the steam locomotive display hall   taking advantage of the semicircular garage and turntable of the former umekoji engine depot   and the educational display gallery at the former nijo station building which was moved and rebuilt at this location  \n",
            "the semicircular garage of reinforced concrete construction   built in 0   was designated as an important cultural property by the japanese government on december 0   0   along with its 0 ton electric ceiling crane   completed in 0   and access rails  \n",
            "in 0   it was also chosen by the japan society of civil engineers as a public works heritage site  \n",
            "in 0   the former nijo station building   gallery   and the semicircular garage   along with the preserved steam locomotives as well as inspection and maintenance equipment   etc  were designated as railway memorial objects and railway semi memorial objects by jr west japan  \n",
            "the former nijo station building   built in 0 by the kyoto railway company to double as a headquarters building   is now the oldest 0 story   wooden   japanese style station building   and was modeled after heian jingu shrine with harmony with the surrounding landscape in mind  \n",
            "it was used as a station building by japanese national railways and jr west japan after kyoto railway company was nationalized in 0   but its use as a station building ended in 0   following the elevation of the sanin line   sagano line   between nijo and hanazono stations   in kyoto prefecture     and in 0 was rebuilt on this site to be used as an entrance   with its original indoor facilities such as ticket windows being used as a gallery  \n",
            "it was designated a cultural property of the city of kyoto in april 0  \n",
            "the semicircular garage houses and displays 0 steam locomotives   originally 0 when the facility first opened   of 0 different classes   all manufactured in japan from the taisho to showa periods  \n",
            "when this museum opened   there was a rule that locomotives should be preserved in working conditions   in fact   0 locomotives excluding the two   class c0   0 and class c0   0   were registered for operation   however   the scope of locomotives to be preserved has been reconsidered several times since then   as of 0   0 locomotives of 0 different classes are preserved in working conditions  \n",
            "of those   0 steam locomotives of 0 different classes are still registered for operation   and 0   class c0   0 and class c0   0   are in service to pull trains on main lines such as the yamaguchi on the yamaguchi line and the kita biwako on the hokuriku main line  \n",
            "the others in working conditions are registered but cannot run on the main lines because they have not undergone general inspections  \n",
            "also   the sl steamliner   pulled by a locomotive being preserved in working conditions for display in the museum   is running on the tracks of the museum site  \n",
            "this display track once stretched north from the garage to the northern edge of the grounds   where there is now a parking lot for large vehicles   where a rest facility for the museum and a park were once located     in line with relocation of the former nijo station building   it was rearranged to go through the southern edge of umekoji park and under the sagano line tracks  \n",
            "today this is still a functional engine depot   and is connected to rail lines in operation  \n",
            "after the closing of west japan railway company  s takatori plant   the facility has also taken over maintenance of steam locomotives  \n",
            "ume\n",
            "stands for umekoji\n",
            "all the steam locomotives   except for class c0   0 that was designated in 0   were designated as railway semi memorial objects together with its logs and maintenance tools in 0  \n",
            "unless otherwise noted   the steam locomotives were transferred from the previous engine depots in 0  \n",
            "registraion of steam locomotives marked with   had been deleted when they were brought to umekoji for preservation  \n",
            "japanese national railways steam locomotive class 0   working\n",
            "manufactured in 0 by kisha seizo co     ltd  \n",
            "transferred from the hirosaki transport area  \n",
            "registration deleted in 0   for working preservation    \n",
            "japanese national railways steam locomotive class 0\n",
            "manufactured in 0 by kawasaki dockyard co     ltd  \n",
            "transferred from the otaru switch yard  \n",
            "registration deleted in 0   for working preservation    \n",
            "put into static preservation in 0  \n",
            "japanese national railways steam locomotive class b0   working\n",
            "manufactured in 0 by tateyama heavy industries  \n",
            "transferred from the kagoshima engine depot  \n",
            "registration deleted in 0  \n",
            "restored to working conditions in 0 in commemmoration of the 0th anniversary of the museum  \n",
            "japanese national railways steam locomotive class c0\n",
            "manufactured in 0 by kawasaki rolling stock mfg   co     ltd  \n",
            "transferred from aizu wakamatsu station   koriyama general rolling stock center  \n",
            "registration deleted in 0  \n",
            "japanese national railways steam locomotive class c0  \n",
            "manufactured in 0 by by kisha seizo co     ltd  \n",
            "scrapped at the niitsu transport area in 0  \n",
            "preserved in a disassembled state as educational material at the niigata railway campus   and restored at the nagano general rolling stock center when moved to umekoji  \n",
            "japanese national railways steam locomotive class c0  \n",
            "manufactured in 0 by kisha seizo co     ltd  \n",
            "scrapped in 0 at the umekoji engine depot  \n",
            "preserved from 0 at the modern transportation museum after being held at the japanese national railways suita training center  \n",
            "japanese national railways steam locomotive class c0\n",
            "manufactured in 0 by kawasaki rolling stock mfg   co  \n",
            "transferred from the asahikawa rail yard  \n",
            "registration deleted in 0  \n",
            "japanese national railways steam locomotive class c0   working   registered   operable on the main line  \n",
            "manufactured in 0 by kawasaki rolling stock mfg   co  \n",
            "transferred from the matsumoto transport area  \n",
            "japanese national railways steam locomotive class c0   working   registered   operable on the main line  \n",
            "manufactured in 0 by kawasaki rolling stock mfg   co  \n",
            "transferred from the sakura engine depot  \n",
            "japanese national railways steam locomotive class c0\n",
            "manufactured in 0 by kisha seizo co     ltd  \n",
            "transferred from the kitami rail yard  \n",
            "registration deleted in 0  \n",
            "japanese national railways steam locomotive class c0\n",
            "manufactured in 0 by hitachi   ltd  \n",
            "transferred from the nara rail yard  \n",
            "registration deleted in 0  \n",
            "japanese national railways steam locomotive class c0   working   registered  \n",
            "manufactured in 0 by mitsubishi heavy industries   ltd  \n",
            "transferred from the miyazaki engine depot  \n",
            "registration deleted in 0   for working preservation    \n",
            "registration restored in 0  \n",
            "japanese national railways steam locomotive class c0  \n",
            "manufactured in 0 by hitachi   ltd  \n",
            "scrapped in 0 at hiroshima engine depot   0  \n",
            "designated as a railway semi memorial object in 0   and held at hiroshima railway campus  \n",
            "transferred to umekoji in 0  \n",
            "class c0   0   working   registered  \n",
            "manufactured in 0 by hitachi   ltd  \n",
            "transferred from the otaruchikko engine depot  \n",
            "registration deleted in 0   for working preservation    \n",
            "registration restored in 0  \n",
            "japanese national railways steam locomotive class d0\n",
            "manufactured in 0 by hitachi   ltd  \n",
            "transferred from the noogata engine depot  \n",
            "registration deleted in 0  \n",
            "japanese national railways steam locomotive class d0\n",
            "manufactured in 0 by kawasaki rolling stock mfg   co  \n",
            "transferred from the hamada engine depot  \n",
            "registration deleted in 0  \n",
            "class d0   0   working   registered  \n",
            "manufactured in 0 by hamamatsu plant   tokai passenger railway   railway ministry  \n",
            "transferred from the nakatsugawa engine depot  \n",
            "registration deleted in 0   for working preservation    \n",
            "registration restored in 0  \n",
            "japanese national railways steam locomotive class d0\n",
            "manufactured in 0 by mitsubishi heavy industries   ltd  \n",
            "transferred from the goryoukaku engine depot  \n",
            "registration deleted in 0  \n",
            "image jrw b0   class b0   0\n",
            "image jnr0   0\n",
            "image d0 0 with umekoji steam locomotive museum mg0   class d0   0\n",
            "image d0 0 and c0 0 umekoji   class d0   0 and c0   0\n",
            "image jnr d0   class d0   0   steamliner  \n",
            "image jnr c0   class c0   0\n",
            "image jrw c0   class c0   0   sl kita biwako  \n",
            "image 0 0 jpg\n",
            "image jnr c0 steamloco   class c0   0\n",
            "image jrw c0   class c0   0\n",
            "japanese national railways diesel locomotive class de0\n",
            "registered   0 de0s belonging to umekoji  \n",
            "japanese national railways passenger car 0 series\n",
            "used as a lounge  \n",
            "at first glance it seems to be in its original shape   but piping of household air conditioners is arranged in the back   with a row of outdoor units  \n",
            "the toilet cannot be used  \n",
            "umekoji   0 years of history   west japan railway company   0   isbn 0\n",
            "japanese national railways rolling stock 0   tokaido line iii   hoikusha   0   isbn 0x\n",
            "umekoji steam locomotive museum   takahiro seki  \n",
            "railway fan   magazine     koyusha     december issue   0   no  0 special feature   the age when umekoji  s steam locomotives were in service\n",
            "the issue featured development of umekoji park and acceptance of c0   0 in 0   and carried the logs of respective steam locomotives preserved  \n",
            "umekoji s t o r y  0 0   reizou takayama\n",
            "railway fan   koyusha     january february issue   0 no  0 0\n",
            "shomyo is one of the traditional forms of japanese music  \n",
            "it is a chant of buddhist scriptures and a religious music for ceremonies  \n",
            "it is also called bonbai and bonnoku in japanese  \n",
            "it is written as 聲明 in the old form  \n",
            "shomyo is the name for one of the academic fields   gomyo   of ancient india  \n",
            "gomyo refers to five academic fields   shomyo   phonology and grammar     kugyomyo   technics and technology     ihomyo   medical science     inmyo   ethics   and naimyo   study of a scholar  s religion   as in buddhism for a buddhist    \n",
            "it was introduced to japan together with buddhism and became well established  \n",
            "there is a record that a buddhist ceremony involving the use of shomyo was held at the opening ceremony for the todai ji temple great buddha eye in 0   so it is believed that shomyo flourished during the nara period  \n",
            "during the early heian period   saicho and kukai introduced shomyo and founded tendai shomyo and shingon shomyo   respectively  \n",
            "buddhist sects other than the tendai shu and shingon shu sects also have their own shomyo   each of which has been handed down to the present day  \n",
            "because shomyo was passed down by kuden   handing down from month to ear     there was no musical notation  \n",
            "therefore   its initiation was extremely difficult  \n",
            "later   hakase was invented as an equivalent of musical notation  \n",
            "there are differences in technical terms   such as hakase   in each school  \n",
            "however   hakase was a reference to chant   and in order to learn shomyo   kuden   also called roi   or direct initiation by an instructor   was necessary   therefore   conservation and inheritance were impossible from the perspective of a master to a disciple  \n",
            "therefore   the development of instructors and successors was a must  \n",
            "numerous schools were extinguished due to various circumstances  \n",
            "tendai shomyo developed independently   in accordance with what saicho had introduced  \n",
            "after saicho   ennin and annen made it prosperous  \n",
            "ryonin   the founder of yuzunembutsu shu sect   is known as the patriarch of the restoration  \n",
            "in 0   ryonin built raigoin temple   sakyo ward   kyoto city   in ohara   kyoto  \n",
            "raigoin temple   in ohara   was called gyozan as the temple  s title   having been named after gyozan   the birthplace of shomyo in china  \n",
            "before long   raigoin temple and shorinin temple were known as the dojos of ohara school gyozan shomyo  \n",
            "jakugen developed a school later   so there were two schools of shomyo in ohara  \n",
            "subsequently   shukai restored ohara shomyo  \n",
            "tanchi established a stream based on a new form of music theory  \n",
            "since then   it became the center of tendai shomyo   and it has been inherited by the present tendai shomyo  \n",
            "the yuzunembutsu shu sect   jodo shu sect and jodo shinshu sect represent the lineage of tendai shomyo  \n",
            "shingon shomyo has been passed down to the present day based on what kukai introduced  \n",
            "shomyo was systematized after shinga  \n",
            "particularly   kancho was known as the patriarch of the restoration  \n",
            "he worked on the composition and control of shomyo  \n",
            "there were numerous schools until the kamakura period   but the princely priest kakusho reorganized them into four   the honsoinryu school   shinsoinryu school   daigoryu school and nakagawa daishinryu school  \n",
            "shomyo   of the kogi   old   shingonshu sect   declined and became extinct with the emergence of the edo period  \n",
            "the honsoinryu school   shinsoinryu school and daigoryu school were extinguished by the middle of the meiji period  \n",
            "currently   it is divided into chizan shomyo   chishakuin   kyoto     buzan shomyo   hase dera temple   nara     nanzan shinryu school   mt   koya san and kogi shingonshu sect temples   kyoto    \n",
            "chizan shomyo and buzan shomyo   shingi   new   shingonshu sect shomyo     shomyo of chizan ha buddhists   shingon shu sect and that of buzan ha buddhists   shingon shu sect were originally derived from the nakagawa daishinryu school  \n",
            "raiyu took in the traditional school of daigo  \n",
            "in 0   when negoro ji temple   wakayama prefecture   was burned down by hideyoshi toyotomi and went into decline   the chizan and buzan schools formed a school based on the traditional school of daigo  \n",
            "it features buzan  s   logic   and chizan  s   shomyo    \n",
            "nanzan shinryu school   kogi shingonshu sect shomyo     this school is based on the nakagawa daishinryu school  \n",
            "the nakagawa daishinryu school has daishin   of naka no gawadera temple   nara   as the founder of the school  \n",
            "during the years 0 and 0   shoshin   of sanboin temple in renge dani   mt   koya san     moved his stronghold to mt   koya san  \n",
            "later   it was called the nanzan shinryu school   being named after nanzan   another name for mt   koya san  \n",
            "it is also called the shinryu school or the yasan shinryu school  \n",
            "the music of heikyoku   yokyoku   minyo or joruri can be said to be variations of shomyo  \n",
            "its has had a great impact on monophony   unaccompanied music    \n",
            "emperor tenchi   tenji  \n",
            "male\n",
            "empress suiko   0     emperor tenchi   tenji     january 0   0  \n",
            "the 0th emperor\n",
            "kokufu shigo is amemikoto hirakasuwake no mikoto   amatsumikoto sakiwake no mikoto\n",
            "the first name is kazuraki   katsuragi    \n",
            "it is presumed that he was once called kazuraki no miko   katsuragi no miko    \n",
            "he is generally known as nakano oe no oji   naka no oe no miko    \n",
            "the   oe   means   prince     and   nakano oe   means   second prince    \n",
            "overview\n",
            "he was the second prince of emperor jome  \n",
            "the mother was takara no himemiko   later called empress kogyoku    \n",
            "the empress was yamatohime no okimi   daughter of emperor  s half brother   furuhito no oe no miko  \n",
            "the emperor tenchi   tenji   plotted a rebellion with nakatomi no kamatari and seized power in a coup to kill soga no iruka   by which his uncle emperor kotoku came to power and he became a prince  \n",
            "he established a new era  the taika  and made many reforms during this period   he was a key person in the taika reforms and itsushi no hen    \n",
            "he devised a plan to trap an opposition group that included arima no miko   who might have caused a coup in the future   and they were executed  \n",
            "as baekje was destroyed by the silla   kingdom   and tang in 0   the baekje prince   buyeo pung   who was staying at the imperial court   was sent back to his country to save baekje  \n",
            "the emperor stayed at tsukushi to send covert troops to baekje   but empress saimei died in 0  \n",
            "eventually   the emperor took control of the government without having an enthronement ceremony   but then he was severely defeated in the battle of hakusukinoe and moved to otsu city to be enthroned in 0  \n",
            "after the battle of hakusukinoe a castle surrounded by water was built   and noroshi and sakimori   the conscript soldier system in the old days   were organized to protect the territory  \n",
            "moreover   the system of ranking officials was changed from 0 levels to 0 levels  \n",
            "in 0 the first national family registry     kogo nenjaku     was introduced  \n",
            "according to   chronicles of japan   nihon shoki       the emperor tenchi   tenji   wanted the first prince   otomo no miko   to be his successor  \n",
            "however   after the death of emperor tenchi   tenji   his brother   oama no miko   emperor temmu     defeated otomo no miko in the jinshin war   the jinshin disturbance   and was enthroned  \n",
            "subsequently   emperor temmu  s ancestry remained in power until empress shotoku  \n",
            "after the death of empress shotoku   sirakabe no okimi   a grandchild of emperor tenchi   tenji   was enthroned as emperor konin and subsequently emperor tenchi  s   tenji  s   ancestry stayed in power  \n",
            "it is said that because emperor tenchi   tenji   took nukata no okimi away from oama no miko   emperor temmu     he sent his four daughters to oama no miko   emperor temmu   to become princesses for an expiation  \n",
            "brief personal history\n",
            "born in 0\n",
            "rittaishi   the ceremony to institute the crown prince   was held on july 0   0  \n",
            "politics began without having an enthronement ceremony august 0   0  \n",
            "the enthronement was held on february 0   0  \n",
            "he died on january 0   0 at age 0  \n",
            "  according to   fuso ryakki     he died from an illness   but another explanation that has been offered is that he went missing in the mountains and that emperor temmu had him assassinated    \n",
            "it was one of the mysteries of the mid seventh century that emperor tenchi   tenji   did not stay in power for a long time  \n",
            "various theories have been suggested in regard to the matter  \n",
            "this was the consideration for setting up emperor temmu  s backbone  \n",
            "there was an accepted theory that emperor temmu was a younger brother of emperor tenchi   tenji     but this is denied   another theory is that emperor temmu was aya no miko   whom empress kogyoku had before she married emperor jomei   and that he was an older half brother of emperor tenchi   tenji    \n",
            "it is apparent that emperor temmu was older than emperor tenchi   tenji   after tracing back their ages according to the emperor tenchi  s   tenji  s   age at death in   chronicles of japan   nihon shoki     and the emperor temmu  s age at death in other history books  \n",
            "the information is consistent in the same historical records   and there are eight to nine years of age difference between the two emperors  \n",
            "some people say that emperor temmu  s age was purposely falsified since it was an embarrassing fact that the younger brother became the emperor before his older brother even though they are half brothers with the same mother   but some argue that according to   chronicles of japan   nihon shoki       emperor temmu  s age was 0 when his father   emperor jomei   was enthroned   but it was said that he was 0 when his father died  \n",
            "therefore   the emperor  s real year of birth is 0   according to the royal family tree   honcho koin jounroku    \n",
            "it was usual to have no successor to the imperial throne for some period   in fact   emperor tenchi   tenji   and empress jito   who were in power before and after emperor temmu   took control of the government without having an enthronement ceremony    \n",
            "some argue that this caused the errors in their ages  \n",
            "some people have said that because   chronicles of japan   nihon shoki     and other indicated history books are edited in different years and they are different in character   they cannot be treated alike    \n",
            "please refer to the section   the age of emperor temmu    \n",
            "there is a theory that the isshi no hen was a coup caused by karu no miko   emperor kotoku     in which naka no oe no oji   naka no oe no miko   lost his position  \n",
            "it has recently been indicated that naka no oe no oji   naka no oe no miko   and sogano iruka had a relatively good relationship and similar basic policies  \n",
            "this way there is no reason that naka no oe no oji   naka no oe no miko   would have assassinated iruka  \n",
            "because there is falsification in the definition of taika  s reform in   chronicles of japan   nihon shoki       this theory has become apparent  \n",
            "this theory gets a lot of attention because it can explain empress kogyoku  s abdication or the reason that the soga family   except iruka   was not displaced from politics after the coup  \n",
            "there is a theory that the enthronement of emperor tenchi   tenji   was delayed due to the complaints against his relationship with women  \n",
            "this is based on the theory that emperor kotoku sent a poem to his wife hashihito no himemiko   emperor tenchi  s   tenji   sister with the same mother   indicating that emperor tenchi   tenji   was having an affair with her   which is contained in   chronicles of japan   nihon shoki      \n",
            "in those days   romance and marriage were allowed between half sisters and brothers from different mothers but not between half brothers and sisters from the same mother   however   in the comic story   tenjo no niji   the rainbow in the sky       by machiko satonaka   it is described that there was a romance involving this kind of forbidden love   hashihito no himemiko was persuaded to marry with karu no miko   emperor kotoku     but she kept the relationship with emperor tenchi   tenji     and arima no miko was furious after knowing this and became angry at hashihito no himemiko  \n",
            "there is a theory that hashihito no himemiko ascended the throne as the princess of the previous emperor but one and became an empress after the death of empress saimei   however   for some reason her name was erased from the record  \n",
            "in the man  yoshu     the anthology of myriad leaves         nakatsusumera mikoto   is considered to be hashihito no himemiko   and it is said that the one named   nakatsusumera mikoto   was a temporary emperor until the enthronement of emperor tenchi   tenji    \n",
            "if hashihito no himemiko was the same person as   nakatsusumera mikoto     there is a question why only she had this special name   alternatively   it is said that this name could have been for empress saimei   but there is no proof  \n",
            "it is hard to prove   since this is about political history with limited data   however   it is expected to have clearer information from close study in relation to the findings of archaeology  \n",
            "poems\n",
            "the emperor tenchi   tenji   was a poet who had four poems in   man  yoshu   the anthology of myriad leaves      \n",
            "he was respected as the emperor of the heian dynasty   and one of his poems is found at the beginning of   one hundred waka poems   the ogura anthology of one hundred tanka poems by one hundred poets      \n",
            "i am sitting in the barn next to a rice field when rice is harvested in the autumn and my kimono sleeves get wet by night dew   as the roof of the barn is covered with rough bladed hay  \n",
            "the following poem is from   man  yoshu   the anthology of myriad leaves      \n",
            "in ancient days   mt   kagu yama loved mt   unebi yama and fought against mt   miminashi yama for love   there is no doubt we still fight for love between wives   as it has been the same since the age of the gods  \n",
            "the saionji family were court nobility descended from the fujiwara clan  \n",
            "the saionji family was of the fujiwara clan fujiwara hokke kaninryu   and the kakaku   family rank of court nobility   was the seiga family  \n",
            "the toin family and imadegawa   kikutei family were branch families of the saionji family  \n",
            "there was a samurai family  the saionji clan in iyo no kuni  as a major offshoot that used the saionji surname  \n",
            "the family crest is hidari mitsudomoe  \n",
            "the progenitor of the saionji family was fujiwara no michisue   the third son of fujiwara no kimizane  \n",
            "kintsune saionji   who was a great grandson of michisue   was confined by the imperial court during the jokyu no ran war because   having married the niece of minamoto no yoritomo   he was being watched over as a relative of the kamakura bakufu   japanese feudal government headed by a shogun     after the war   however   kintsune gained the confidence of the shogunate   wielded the real power in the imperial court through holding the office of kanto moshitsugi   and rose to the position of daijo daijin   grand minister of state     raising the social standing of the saionji family  \n",
            "the family name of saionji comes from the name of the temple kintsune built at bettei   in kyoto   kitayama so   kadono gun   yamashiro no kuni    \n",
            "this hall is also called kitayama dono palace   and the current kinkaku ji temple was built by yoshimitsu ashikaga after he was given this land by the saionji family  \n",
            "since the generation of kintsune   the saionji family gained support from the kamakura shogunate and came to power that surpassed the sekke throughout the kamakura era  \n",
            "sanekane saionji approached daikaku ji through genealogy and made his daughters marry kameyama ho ou   the ex emperor   or the emperor godaigo  \n",
            "however   since the generation of sanekane  s son kinhira saionji   the saionji family this time supported the jimyoin to genealogy when the daikakuji to genealogy started to keep a distance from the bakufu   thereby deepening the conflict with the daikakuji to genealogy  \n",
            "thereafter   at the generation of kinmune saionji   the saionji family was dismissed from the post of kanto moshitsugi after the fall of the kamakura shogunate  \n",
            "kinmune harbored yasuie hojo   who was a remnant of the hojo clan   the assassinated emperor godaigo   who had started kenmu no shinsei   plotted a rebellion to back the ex emperor gofushimi of jimyoin to genealogy   and was arrested and executed   the plot having been exposed by the betrayal of kinmune  s younger brother   kinshige saionji  \n",
            "during the period of the northern and southern courts   kinmune  s son sanetoshi saionji served hokucho   japan   and rose to the post of minister of the right   whereby the family name was restored  \n",
            "during the muromachi and edo periods the saionji family was also known for having biwa   japanese lute   players as a family business  \n",
            "in the early days of the edo period   saneharu saionji welcomed tokuhime   who was the eldest daughter of kyumu nagaoka   also known as tadataka hosokawa     as the midaidokoro   wife of a shogun or a highest ranking nobleman   of the heir   later disinherited   of tadaoki hosokawa   and invited her to accept the post of sadaijin   minister of the left     moreover   kyumu nagaoka offered various forms of financial support to the saionji family   including donating a property that yielded 0 koku of rice   menko shuroku    \n",
            "at the end of the edo period   kinmochi saionji was adopted from the tokudaiji family of the same kaninryu   worked as a politician after the meiji restoration   served as prime minister   and had great influence as a genro on the political scene during the taisho and showa eras  \n",
            "there is an anecdote that kinmochi was not good at playing the biwa japanese lute   which should have been the family business of the saionji family   and during kinmochi  s time in office as prime minister   the emperor meiji said out of mischief to kinmochi     for the first time in a long while   i would like to listen to biwa japanese lute     and therefore he was forced to play the biwa japanese lute in dire distress together with the officials of utaryo in the imperial household ministry  \n",
            "mt   hiei is a mountain that straddles the western part of otsu city in shiga prefecture and the northeastern part of kyoto city in kyoto prefecture  \n",
            "mt   hiei is the name for the soji ridge   which is formed by two peaks   daihiei   0 0m   on the border between otsu city and kyoto city  s sakyo ward   and shimeigatake in sakyo ward   0m    \n",
            "daihiei  s first order triangulation point is located in otsu city  \n",
            "next to mt   koya   it has long been an object of religious belief   flourishing with enryaku ji temple and hiyoshi taisha shrine on it  \n",
            "it is part of the higashiyama mountains   kyoto prefecture    \n",
            "it is also known as eizan   hokurei   tendaisan   miyakofuji   etc  \n",
            "overview\n",
            "because of its location near the kimon gate on the northeast side of kyoto   mt   hiei is considered a castle guardian  \n",
            "the kojiki   records of ancient matters   calls mt   hiei   hienoyama     and records that oyamakui no kami is enshrined on hienoyama in omi province   and narikabura is the object of worship  \n",
            "ever since enryaku ji temple was founded on hienoyama   oyamakui no kami   as the god of the land   has been a guardian deity of the tendai sect and enryaku ji temple   and a mountain king sect based on him has spread in the area  \n",
            "also   it is the mountain where enryaku ji temple  s thousand day circumambulation practice is done   including a pilgrimage to the shrines at the peak of mt   hiei   and to hiyoshi taisha shrine at its foot  \n",
            "hiking is popular   and a famous trail begins at shugakuin in sakyo ward of kyoto city and goes up kirarazaka hill   another one on the shiga prefecture side begins in monzenmachi   sakamoto   otsu city     and goes up through mudojidani valley  \n",
            "the path on kirarazaka hill has long been used by monks   monk soldiers and imperial messengers traveling between kyoto and enryaku ji temple   and is still visited by many hikers today  \n",
            "the tokaido nature trail passes through the mountain on the way from otsu to kyoto   ohara  \n",
            "many tourists come on holidays   since they can reach the top via toll road   cable car   and ropeway  \n",
            "there used to be a mt   hiei amusement park and a mt   hiei artificial ski slope   but both were closed before 0   the site of the ski slope is now a cosmos flower garden   and there is an art museum   hiei garden museum   on the former location of the amusement park  \n",
            "there was also a forest land amusement park in yase   at the foot of the mountain on the kyoto side   although that was closed on november 0   0   and a members only resort hotel now stands in that location  \n",
            "daihiei and shimeigatake\n",
            "the survey findings from the geographical survey institute show the eastern peak as daihiei   the western peak as shimeigatake   and both of them together as mt   hiei  \n",
            "according to   ten no ki     the record of survey     the first order triangulation point on the eastern peak is called   mt   hiei    \n",
            "although this marker is located at the border between otsu and kyoto cities   it is inside otsu city  \n",
            "when one views mt   hiei from the kyoto basin   it is difficult to see the daihiei peak   so some think that shimeigatake is the peak of mt   hiei  \n",
            "the keifuku electric railroad eizan ropeway has its hiei mountaintop station placed on the shimeigatake peak  \n",
            "there are conflicting opinions as to the proper way to write and read   shimeigatake     the gsi lists shimeigatake   shimeidake   while   place names of kyoto   lists shimeigatake and shimyo no mine  \n",
            "mt   hiei gets its alternate names   mt   tendai and shimeigatake   from mt   tiantai and mt   siming   sacred mountains in the people  s republic of china that have associations with the tendai sect  \n",
            "geography\n",
            "mt   hiei viewed from the side   left side  \n",
            "the tanba plateau and the hira mountain region are separated by the hanaore fault  \n",
            "for this reason   it is considered to belong to the hiei mountain region   or the hiei daigo mountain region  \n",
            "access\n",
            "hieizan railway line   sakamoto cable  \n",
            "keifuku electric railway keifuku cable line   eizan cable  \n",
            "keifuku electric railway eizan ropeway\n",
            "hieizan driveway\n",
            "kyoto   shiga prefectural route 0   shimogamo otsu road   yamanakagoe  \n",
            "hieizan drive bus   hieizan mountain shuttle bus\n",
            "keihan bus yamashina ticket office   to mt   hiei\n",
            "kyoto bus arashiyama ticket office   mt   hiei line   hieizan drive bus  \n",
            "television broadcast towers\n",
            "on the side of sakyo ward of kyoto city   there are television stations that service the kyoto prefecture area  \n",
            "the antennae have directional capabilities to prevent the signal from leaking to the shiga prefecture side  \n",
            "land based digital television broadcasting facilities\n",
            "began broadcasting on april 0   0  \n",
            "land based analog television broadcasting facilities\n",
            "notes\n",
            "nhk osaka   nhk educational television   mbs mainichi broadcasting   abc asahi broadcasting   ktv kansai tv   and ytv yomiuri tv signals can be received directly from mt   ikoma   both digital and analog broadcasting    \n",
            "there are also many households that watch nhk sogo from the osaka broadcast on mt   ikoma  \n",
            "the 0 stations of the tokaido road   tokaido gojusan tsugi   can mean  \n",
            "the 0 posting stations on the tokaido road   which connected edo   nihonbashi   chuo ward   tokyo   and kyoto   sanjo ohashi bridge   during the edo period  \n",
            "to be explained in this section  \n",
            "the ukiyoe works painted by hiroshige utagawa depicted 0 posting stations   or 0   including the starting   ending stations    \n",
            "game software for family computers released by sunsoft\n",
            "the 0 stations of the tokaido road   a game  \n",
            "the   0 stations   of the tokaido road refers to 0 stations in   tokaido road     which is the major road running through the tokaido region and is among five provinces and seven circuits   gokishichido    \n",
            "many of the 0 stations are established in places where the scenery is beautiful   or in famous historic spots   and often served as the subjects of ukiyoe or waka   haiku  \n",
            "furthermore   when it is called 0 stations it refers to the road up to kyoto   but when referring to the road up to osaka the term 0 stations is used  \n",
            "from here onward we will categorize them according to their respective provinces  \n",
            "serial numbers are used   starting at shinagawa juku station  \n",
            "  with respect to the local governments that became defunct as a result of the great heisei merger   the names of former local governments are also listed  \n",
            "nihonbashi bridge   chuo ward   tokyo  \n",
            "0   shinagawa juku station   shinagawa ward   tokyo  \n",
            "0   kawasaki juku station   kawasaki ward   kawasaki city   kanagawa prefecture  \n",
            "0   kanagawa juku station   kanagawa ward   yokohama city   kanagawa prefecture  \n",
            "0   hodogaya juku station   hodogaya ward   yokohama city   kanagawa prefecture  \n",
            "0   totsuka juku station   totsuka ward   yokohama city   kanagawa prefecture  \n",
            "0   fujisawa shuku station   fujisawa city   kanagawa prefecture  \n",
            "0   hiratsuka juku station   hiratsuka city   kanagawa prefecture  \n",
            "0   oiso juku station   oiso machi   naka gun   kanagawa prefecture  \n",
            "0   odawara juku station   odawara city   kanagawa prefecture  \n",
            "0   hakone juku station   hakone machi   ashigara shimo gun   kanagawa prefecture  \n",
            "0   mishima shuku station   mishima city   shizuoka prefecture  \n",
            "0   numazu juku station   numazu city   shizuoka prefecture  \n",
            "0   hara juku station   tokaido road     numazu city   shizuoka prefecture  \n",
            "0   yoshiwara juku station   fuji city   shizuoka prefecture  \n",
            "0   kambara juku station   shimizu ward   shizuoka city   shizuoka prefecture   formerly kambara cho   ihara gun    \n",
            "0   yui shuku station   yui cho   ihara gun   shizuoka prefecture  \n",
            "0   okitsu shuku station   shimizu ward   shizuoka city   shizuoka prefecture   formerly shimizu city    \n",
            "0   ejiri juku station   shimizu ward   shizuoka city   shizuoka prefecture   formerly shimizu city    \n",
            "0   fuchu shuku station   aoi ward   shizuoka city   shizuoka prefecture  \n",
            "0   mariko juku station   suruga ward   shizuoka city   shizuoka prefecture  \n",
            "0   okabe juku station   okabe cho   shida gun   shizuoka prefecture  \n",
            "0   fujieda juku station   fujieda city   shizuoka prefecture  \n",
            "0   shimada juku station   shimada city   shizuoka prefecture  \n",
            "0   kanaya juku station   shimada city   shizuoka prefecture   formerly kanaya cho   haibara gun    \n",
            "0   nissaka shuku station   kakegawa city   shizuoka prefecture  \n",
            "0   kakegawa juku station   kakegawa city   shizuoka prefecture  \n",
            "0   fukuroi juku station   fukuroi city   shizuoka prefecture  \n",
            "0   mitsuke juku station   iwata city   shizuoka prefecture  \n",
            "0   hamamatsu juku station   naka ward   hamamatsu city   shizuoka prefecture  \n",
            "0   maisaka juku station   nishi ward   hamamatsu city   shizuoka prefecture   formerly maisaka cho   hamana gun    \n",
            "0   arai juku station   arai cho   hamana gun   shizuoka prefecture  \n",
            "0   shirasuka juku station   kosai city   shizuoka prefecture  \n",
            "0   futagawa juku station   toyohashi city   aichi prefecture  \n",
            "0   yoshida juku station   toyohashi city   aichi prefecture  \n",
            "0   goyu shuku station   toyokawa city   aichi prefecture  \n",
            "0   akasaka shuku   tokaido     toyokawa city   aichi prefecture  \n",
            "0   fujikawa shuku station   okazaki city   aichi prefecture  \n",
            "0   okazaki shuku station   okazaki city   aichi prefecture  \n",
            "0   chiryu juku station   chiryu city   aichi prefecture  \n",
            "0   narumi juku station   midori ward   nagoya city   aichi prefecture  \n",
            "0   miya juku station   atsuta ward   nagoya city   aichi prefecture  \n",
            "0   kuwana juku station   kuwana city   mie prefecture  \n",
            "0   yokkaichi juku station   yokkaichi city   mie prefecture  \n",
            "0   ishiyakushi juku station   suzuka city   mie prefecture  \n",
            "0   shono juku station   suzuka city   mie prefecture  \n",
            "0   kameyama juku station   kameyama city   mie prefecture  \n",
            "0   seki juku station   kameyama city   mie prefecture   formerly seki cho   suzuka gun    \n",
            "0   sakashita juku station   kameyama city   mie prefecture   formerly seki cho   suzuka gun    \n",
            "0   tsuchiyama juku station   koka city   shiga prefecture   formerly tsuchiyama cho   koka gun    \n",
            "0   minakuchi juku station   koka city   shiga prefecture   formerly minakuchi cho   koka gun    \n",
            "0   ishibe juku station   konan city   shiga prefecture   formerly ishibe cho   koka gun    \n",
            "0   kusatsu shuku    juku   station   kusatsu city   shiga prefecture  \n",
            "0   otsu juku station   otsu city shiga prefecture  \n",
            "sanjo ohashi bridge   higashiyama ward   kyoto city   kyoto prefecture  \n",
            "according to one theory   ieyasu tokugawa set the number of stations from edo   kanji characters with the same sound   穢土   meaning unclean land   to kyo   which means the land where fugen bosatsu   普賢菩薩   lives     based on a chapter from kegon kyo sutra   in which zenzai doshi   善財童子   of   nyuhokkaibon   入法界品       following the orders of monju bosatsu   achieved enlightenment at the place of fugen bosatsu   through 0 teachers  \n",
            "as an extension of the tokaido road   the stations of the kyokaido road   osaka kaido road   were established in 0  \n",
            "in some cases the stations of the kyokaido road are included and are collectively referred to as the 0 stations of the tokaido road  \n",
            "furthermore   when entering the kyokaido road from the tokaido road   sanjo ohashi bridge will not be crossed   and the route taken will turn southwest at higechaya oiwake   髭茶屋追分     oiwake cho   otsu city     so the station following otsu juku station will be fushimi juku station  \n",
            "these will be categorized according to their respective provinces  \n",
            "the first number is based on the country  \n",
            "the second number is a serial number   starting with shinagawa juku station  \n",
            "0   fushimi juku station   kyo kaido road     fushimi ward   kyoto city   kyoto prefecture  \n",
            "0   yodo juku station   fushimi ward   kyoto city   kyoto prefecture  \n",
            "0   hirakata juku station   hirakata city   osaka prefecture  \n",
            "0   moriguchi shuku station   moriguchi city   osaka prefecture  \n",
            "korai bashi bridge   chuo ward   osaka city   osaka prefecture  \n",
            "note   it was formerly kyobashi bridge   a bridge over neyagawa river   which is located between the chuo and miyakojima wards of osaka city   osaka prefecture    \n",
            "  the pillow book   is a zuihitsu essay   literally     random jottings       which is said to have been written by sei shonagon   a female writer who lived in the mid heian period  \n",
            "the title was also written using chinese characters such as   枕草紙     makurazoshi notebook       枕冊子     makura sasshi       枕双紙     makura soshi     and   春曙抄     shunsho sho commentary     and the oldest manuscript   called   maedabon   in japanese     a copy of which was made in the kamakura period   has a gold lacquered casket with the chinese characters   清少納言枕草子     sei shonagon   the pillow book   written on it  \n",
            "it was also called   sei shonagon ki    \n",
            "together with   the tale of genji     this book is considered one of the twin masterpieces of heian literature   and it had a great influence on the renga   linked verse     haikai   seventeen syllable verse     and kanazoshi   old stories witten in the kana script   that followed  \n",
            "along with both   hojo ki   the ten foot square hut       written by kamo no chomei   and   tsurezuregusa   essays in idleness     written by kenko yoshida   it is called one of the three japanese major lists   histories and satires  \n",
            "its writing style is extremely individualistic   there is no similar book except   gizan zassan   gizan collection       compiled by shangyin li   courtesy name yishan     a poet of the late tang dynasty  \n",
            "according to an afterword to the book   her motive for writing the book and the origin of the title came from the episode in which   when naidaijin   minister of the center   fujiwara no korechika presented to his younger sister fujiwara no empress teishi and emperor ichijo a bundle of paper   which was still expensive in those days   the empress inquired     emperor used the paper to copy   shiki   records of the grand historian       what do you think we could write on this     to which sei shonagon answered     it   the book   would be good to use as a pillow    \n",
            "she was granted the paper   such episode being based on sankanbon   version of manuscript     and both incomplete and complete versions of noinbon   version of manuscript   have similar episodes describing the response that   it   the book   would be good to use as a pillow       but the sakaibon and maedabon   versions of the manuscript   don  t contain this episode    \n",
            "it is widely thought that the title   the pillow book   is also based on the episode  \n",
            "  a study of the pillow book   written by kazuhiko hayashi mentions his own view and some other past scholars   names such as keichu   shinobu orikuchi   and kikan ikeda  \n",
            "the following are typical views about the meaning of   pillow    \n",
            "― bedding   a pun that suggests a pillow on the mattress   with shiki reworded as shikibuton   mattress      \n",
            "― dictionary for the writer   there are many chapters in which utamakura   a place famed in poetry     rules   and terms are listed  \n",
            "― memorandum   she kept the paper by her pillow for private jottings  \n",
            "― treasured book\n",
            "― irrelevant to anything\n",
            "however   scholars have yet to reach a consensus  \n",
            "also   in the book of   eiga monogatari   a tale of flowering fortunes       the term   makura no soshi   as a common noun was used to describe the beautiful kasane   layering   colors  \n",
            "according to the afterword   the first draft was written around 0   and sachujo   guard of the imperial palace   minamoto no tsunefusa took the draft from the author  s residence and brought it before the eyes of the world  \n",
            "after that   she wrote continuously   and some records say the book was written around 0  \n",
            "some lines of   the pillow book     as quoted from the old commentary   shimei sho commentary of   the tale of genji       don  t exist in the surviving manuscript   which suggests that the writing process was complicated  \n",
            "there are great differences between the existing manuscripts  \n",
            "it consists of three volumes and takes the form of a collection of various writings  \n",
            "it is a manuscript containing an okugaki   postscript   written in 0 by a person called bogyuguo   who seems to have been fujiwara no sadaie  \n",
            "it is   clear writing   and is easy to understand   being considered the most similar to the original form  \n",
            "it was divided into two groups by kikan ikeda  \n",
            "korui   first class     the opening 0 chapters   which start with the line     the best time in spring is dawn     are omitted   and instead the manuscript begins with the line     comfortable thing    \n",
            "0 chapters\n",
            "book collection of yomei paperback   book collection of toshoryo   shoryo department   imperial household agency   and book collection of the tatamatsunomiya faamily\n",
            "otsurui   second class     0 chapters\n",
            "old book collection of hamao yatomi   book collection of kariya city library   old book collection of the date family   old book collection of the kajuji family   old book collection of akika nakamura   book collection of koshido bunko\n",
            "the priest noin was related by marriage to sei shonagon   one of his sisters was a wife of tachibana no norinaga   a son of sei shonagon     and seemed to have something to do with handing down the manuscript  \n",
            "the date of the work has been traced back to the end of the kamakura period  \n",
            "after arguing which book is more authentic   it is now widely accepted that the original book of noinbon is inferior to sankanbon  \n",
            "two hundred and thirty chapters   excluding the first 0 chapters  \n",
            "0 chapters\n",
            "ruisan form   a classified collection in book form  \n",
            "manuscript copied in the muromachi period\n",
            "the postscript says kiyohara no shigekata   edakata copied the book   whose owner was doha   a secluded monk who lived in sakai   and therefore the manuscript is called sakaibon  \n",
            "two volumes\n",
            "the book is missing chapters on diary and reminiscence  \n",
            "gokogoninbon   type of manuscript     0 chapters\n",
            "the postscript says emperor go kogon made a copy of the book  \n",
            "shinkanbon   a manuscript in the emperor  s own hand  \n",
            "ninety five chapters\n",
            "the above two types of manuscripts were integrated into a book  \n",
            "sakaibon in general refers to this  \n",
            "volume 0 consists of 0 chapters  \n",
            "volume 0 consists of 0 chapters  \n",
            "volume 0 consists of 0 chapters  \n",
            "volume 0 consists of 0 chapters  \n",
            "volume 0 might have been lost  \n",
            "it takes ruisan form  \n",
            "there exists only one manuscript that has been handed down in the maeda family of kaga province   the book collection of maeda ikutokukai    \n",
            "it is preserved in a box of gold lacquered craft   and on its surface are the characters   清少納言枕草子   sei shonagon   the pillow book     in gold incrustation  \n",
            "it is an important cultural property  \n",
            "it was copied during the early kamakura period and is considered to be the oldest of all the manuscripts of   the pillow book    \n",
            "among them   the sakaibon group gokogoninbon was separately collected in two volumes of   gunshoruiju   japanese history book       and three kinds of sakaibon were collected in   shinko gunshoruiju   library      \n",
            "additionally   noinbon was used as a master copy in making the old plate of type in the early edo period   therefore   it remained a dominant manuscript along with   the pillow book bochu   marginal notes     and   shunsho sho commentary of the pillow book     annotated by kigin kitamura   until modern times  \n",
            "however   in 0   when jutaro tanaka   0 0   reevaluated the second class of sankanbon   it came to be considered more important and was published more   being used as a textbook and read at school after the war  \n",
            "other existent manuscripts are seven chapters of   the pillow book ekotoba   story in pictures     made in the late kamakura period   consisting of emaki   a picture scroll   with hakubyoga   ink line painting     and sankanbon manuscript seems to have been used for the notes  \n",
            "the second class sankanbon contains more than 0 independent chapters in total  \n",
            "it consists of various types of writing     chapters of ruiju   of   monowazukushi   enumerating the things in the same category       as represented by the lists of things like   insects       flowering trees       dispiriting things       endearingly lovely things     and   chapters of zuiso   essay       which is an observation of daily life and nature in the seasons   and   chapters of reminiscence   chapters of a diary     in which the author wrote about the life around empress teishi   whom she served in the imperial court  \n",
            "however   some chapters are too obscure to be classified   for example the first chapter   entitled     the best time in spring is dawn     is generally classified into the chapters of zuiso   but some people disagree    \n",
            "it was written in the plain vernacular using hiragana   the japanese cursive syllabary     and most of the chapters were written in a witty style   but sometimes sentimental lamentation appears reflecting the fall of the michitaka fujiwara  s family and unhappiness gone through by her majesty empress teishi  \n",
            "the author  s sophisticated taste was in harmony with a keen observation on things   generating the intellectual aesthetic world of   wokashi   amusing or delightful     in contrast with the emotional   mono no aware   pathos     of   the tale of genji    \n",
            "positive evaluation\n",
            "in the pillow book   the author cherished humanity as well as nature   therefore   she accepted and formed them in each phase as various beauties   sakuwo mekata    \n",
            "even in each chapter   various types of rich writing such as lists   essays or reminiscences are freely interwoven   like a flying horse soaring into the sky with a strand of associations called up one after another     boku hagitani  \n",
            "although it followed the conventional   spring   flower  morning   type of link that was often seen in kokinshu in those days   its use of the   season time   type of expression   such as   the best time in spring is dawn       which omitted the middle poetic element   was innovative   challenging as it did the rigid esthetic values held by readers who were familiar with poetic traditions     munetoshi fujimoto  \n",
            "she expressed her love and respect for empress teishi  \n",
            "the author wrote it to console empress teishi   who felt depressed over the fall of the michitaka family  \n",
            "therefore   it is natural that she didn  t mention the fall of the michitaka family   as above    \n",
            "there are some chapters that seem like mere self praise   but in them the author claimed that the empress and the people around her received the emperor  s favor and led a life filled with aesthetic sentiment and exquisite taste   living in a world that was cut off from the politics of the time and disappointment     osamu ueno    \n",
            "negative evaluation\n",
            "it is a representation of her shallowness trying to forget her original social rank and assimilate herself to the upper class     ken akiyama  \n",
            "  it is based on the fact that she didn  t use polite expressions when she wrote   not only about her family but also about those who belonged to the upper classes    \n",
            "it is only   a record of civilization in the inner palace     and wasn  t written by   a private individual     joji ishida    \n",
            "yoshinobu   yoshihisa   tokugawa was the 0th seii taishogun of the edo shogunate   held the position from december 0   0 to december 0   0   both in the old lunar calendar    \n",
            "he was the only shogun who did not work in edo jo castle  \n",
            "his rank awarded by the imperial court was naidaijin   inner minister   ju ichii kun itto koshaku  \n",
            "he was a member of the house of peers of japan  \n",
            "he served as the guardian   shogun kokenshoku   of the 0th shogun iemochi tokugawa   after iemochi  s death he assumed his position as the 0th shogun of the edo shogunate  \n",
            "after he returned his right to administer government to emperor meiji   taisei hokan     he worked aiming to unite the nobles and samurai   kobu gattai     however   during the battle of toba fushimi that took place after the restoration of imperial rule   osei fukko     he left the shogunate army behind at osaka jo castle and returned to edo jo castle   thereafter   he received a subjugation order from the imperial court and entered into disciplinary confinement which led to the bloodless surrender of edo jo castle  \n",
            "after being freed from the disciplinary confinement   he immersed himself in his hobbies   he was awarded the title of koshaku and lived into the taisho period  \n",
            "the name   yoshinobu   is also popularly read as   keiki     yusoku yomi   the way of pronouncing in the chinese derived reading   so as not to directly pronounce the real name of the respectable person in the japanese reading    \n",
            "there is a record among official documents of the edo shogunate that during his tenure as the shogun his name was also read as   yoshihisa    \n",
            "there are also records of his name written as   yoshihisa   with his own signature using roman letters   and   his name printed as   yoshihisa     in newspapers in english  \n",
            "he is most often called   yoshinobu   in his homeland of mito   but he is often called   keiki   in shizuoka   where he spent the rest of his life  \n",
            "according to those who knew him in his lifetime   yoshinobu himself liked to be called   keiki sama     and he called himself   keiki   in a telegraph sent to his younger brother   akitake tokugawa  \n",
            "yoshihisa tokugawa   who was yoshinobu  s heir   was also called   keikyu sama   from those around him  \n",
            "the use of the two names   keiki sama   and   keiki san   has been confirmed   and even though it seems to have become uncommon in the modern days   the use of   keiki san   is not limited to shizuoka but is confirmed in various regions   in both cases the name is often used with much affection  \n",
            "though ryotaro shiba commented that   the name   keiki   was used quite often among those associated with the former shogunate     the use of the name   keiki   was also confirmed among those related to higo han clan who worked toward the fall of the shogunate   there is a possibility that he was widely yet secretly respected and admired  \n",
            "he was born at the mito han clan residence of koishikawa in edo   as the seventh son of the ninth chief   of mito han clan     nariaki tokugawa on september 0   0  \n",
            "his mother was   his father  s   legitimate wife   princess yoshiko   he was the first shogun since the third shogun iemitsu tokugawa whose biological mother was the father  s legitimate wife  \n",
            "however   although iemitsu  s biological mother is officially known as the father hidetada  s legitimate wife   ogo no kata   but one theory suggests that his biological mother was kasuga no tsubone    \n",
            "his childhood name was shichiromaro  \n",
            "based on nariaki  s educational policy     a boy should be educated in his home domain   yoshinobu moved to mito at the age of seven months and spent most of his time there until he succeeded to the hitotsubashi tokugawa family   during which time he was educated and taught martial arts by seishisai aizawa  \n",
            "yoshinobu  s intelligence was noted early   at first   instead of sending him to another family for adoption   nariaki planned to keep him by his eldest son and heir   yoshiatsu tokugawa  s side  \n",
            "on august 0   0   the shogunate gave a secret order to designate shichiromaro of mito han clan as the heir to the hitotsubashi family   one of the three branches of the tokugawa han clan   gosankyo    \n",
            "accepting this order   shichiromaro succeeded to the hitotsubashi family on september 0   in december   he received henki   one character of shogun  s name   慶   from the 0th shogun ieyoshi   家慶   tokugawa and changed his name to yoshinobu   慶喜    \n",
            "considering yoshinobu as a strong candidate as the heir to the shogun   ieyoshi visited the hitotsubashi residence often   but he abandoned this idea after accepting roju masahiro abe  s admonishing advice  \n",
            "during the disorder caused by the arrival of the black ships   shogun ieyoshi died in 0   his heir   the 0th shogun iesada tokugawa   had poor health with no prospect of having his own son   and therefore the issue of   who would become the shogun  s heir   arose   issue of shogun  s heir    \n",
            "there was an opposition between the hitotsubashi party that included nariaki   masahiro abe and the chief of satsuma han clan   nariakira shimazu   who recommended yoshinobu   and the nanki party primarily consisting of the chief of hikone han clan   naosuke ii   and the ooku members headed by iesada  s birth mother   honjuin   who recommended the chief of kii han clan   yoshitomi tokugawa  \n",
            "the hitotsubashi party executed political maneuvers such as sending into ooku nariakira shimazu  s adopted daughter   tenshoin   as shogun  s wife   midaidokoro     but after the death of masahiro abe and nariakira shimazu   its power weakened   naosuke ii who became tairo in 0 settled on   shogun  s heir as yoshitomi tokugawa   through arbitration  \n",
            "in the same year   naosuke ii signed the treaty of amity and commerce between the united states and japan without the emperor  s permission  \n",
            "yoshinobu   along with nariaki and the chief of fukui han clan   yoshinaga matsudaira   made an unexpected visit to the   edo jo   castle and closely interrogated naosuke   but in tern   he was questioned on his offense of making an unexpected visit to the castle   he was ordered to retire and was given a disciplinary confinement in the following year   0     ansei purge    \n",
            "yoshinobu himself seemed to be uninterested in becoming the heir to the shogun and he sent a letter to nariaki whose content indicated   i appreciate your efforts but i  d rather not become the shogun than to make mistakes after becoming the shogun    \n",
            "he was relieved from his disciplinary confinement in 0  \n",
            "in 0   guarded by the satsuma han clan army under the command of hisamitsu shimazu   the emperor  s messenger   shigetomi ohara   entered edo and delivered emperor komei  s order to   promote yoshinobu tokugawa to the shogun  s guardian   shogun kokenshoku   and shungaku   yoshinaga   matsudaira to tairo    \n",
            "on july 0   the shogunate ordered yoshinobu to be the shogun  s guardian   shogun kokenshoku   and shungaku to be the shogunal prime minister   seiji sosaishoku    \n",
            "yoshinobu and shungaku directed a reform of the shogunate known as the bunkyu reform that included installation of the office of kyoto protector   kyoto shugoshoku   and relaxation of sankin kotai requirement  \n",
            "in 0   he went to kyoto as iemochi tokugawa  s harbinger   and he endeavored to negotiate with the members of the imperial court who urged the execution of joi   expulsion of foreigners from japan    \n",
            "upon emperor komei  s visit to iwashimizu hachiman gu shrine to pray for the execution of joi   if iemochi received setto   emperor  s sword that symbolically authorizes the shogun to take charge of the upcoming battle on behalf of the emperor   from the emperor   he would have no choice but to execute joi   yoshinobu quickly made iemochi cancel the audience   with the emperor   by saying that iemochi had a cold and a fever   pretended illness    \n",
            "when the sonno joi party   the anti foreigner royalists   centered around the choshu han clan lost their case upon the political change on august 0   yoshinobu went to kyoto again in order to participate in a meeting jointly held by the party trying to unite the nobles and samurai   kobu gattai   and the party assisting the shogunate   sabaku    \n",
            "however   when no agreement was reached   he used a bold method of negotiation   he intentionally got drunk at a banquet with prince kuni asahiko and reviled at munenari date   shungaku matsudaira and hisamitsu shimazu   and furthermore   violently said to nakagawa no miya     how much is shimazu paying you     so as to breakdown their organization  \n",
            "thereafter   he remained in kyoto and he was appointed to kinri goshuei sotoku   head of the imperial palace   emperor  s protector     together with the protector   shugoshoku   katamori matsudaira   aizu han clan   and kyoto shoshidai sadaaki matsudaira   chief of kuwana han clan   regulated the nobles and those loyal to the emperor   ichi kai so   hitotsubashi aizu kuwana   structure    \n",
            "during the tenguto war he showed his cruelty by slashing his supporters such as kounsai takeda and the vassals of his home domain of mito  \n",
            "at the kinmon rebellion in 0 he led the shogunate army and he himself attacked the choshu army that had occupied the takatsukasa residence  \n",
            "after the first subjugation of choshu that followed   he made every effort to secure the emperor  s permission on the treaty of amity and commerce between the unites states and japan that had been left unauthorized by the emperor   he succeeded in securing the emperor  s authorization albeit with provisos  \n",
            "at the second subjugation of choshu in 0   yoshinobu suppressed interferences from satsuma han clan and received a direct order from the emperor to subjugate choshu  \n",
            "however   because of the satsuma choshu alliance   saccho alliance     satsuma han clan refused to send its army and consequently the shogunate army had to retreat  \n",
            "during the second subjugation of choshu   shogun iemochi passed away at osaka jo castle on july 0  \n",
            "yoshinobu appealed to the imperial court to make an official announcement of truce by the emperor and he succeeded in concluding an armistice treaty  \n",
            "he was recommended to be iemochi  s successor but yoshinobu firmly declined  \n",
            "he succeeded to the tokugawa shogun family on aug   0 but he continued to refuse to assume his position as the shogun  \n",
            "since then the roju   shogun  s political advisor   and others sincerely requested him to become the shogun   but he still did not accept the request   he finally assumed his position as the shogun on december 0   after accepting   emperor komei  s   direct order to become the shogun  \n",
            "by becoming the shogun as a favor to others he aimed to manipulate politics to his advantage  \n",
            "through the french minister leon roches   he received 0 0 million dollars from france and founded yokosuka ironworks   a shipyard   and a ship repair dock   and he also led a revolution in the military system by inviting a group of military advisors headed by jules brunet  \n",
            "he also sent his biological younger brother   akitake tokugawa   to the paris exposition and encouraged the younger members of the shogunate vassals to study in europe  \n",
            "on the matter of opening of port of hyogo   he tenaciously persuaded the imperial court and obtained the emperor  s permission  \n",
            "yoshinobu anticipated a secret order to saccho   satsuma and choshu  han clans   from the emperor to destroy the shogunate   and on oct   0   0   he   as a preemptive measure   suggested to the imperial court that he would return his political power to the emperor   his request was accepted by the emperor on the 0th   the following day   taisei hokan    \n",
            "it is said that he judged the imperial court at the time to have no capability to administer government   and he therefore sought continuation of tokugawa family  s political power by leading the session of clan chiefs  \n",
            "however   because of the anti shogunate activists toshimichi okubo and tomomi iwakura  s scheme   there was a command to revive the political power of the imperial court in december   and yoshinobu was ordered   jikan nochi     to resign from his position as naidaijin and surrender the tokugawa territory    \n",
            "in order to avoid confrontations yoshinobu retreated to osaka jo castle   assembled the ambassadors from various foreign countries and asserted the legitimacy of tokugawa   furthermore   he appealed to the imperial court and had the jikan nochi modified to more moderate terms  \n",
            "however   in response to the provocation caused by satsuma han clan in the city of edo in 0 he mobilized his army and blockaded kyoto using the armies of aizu and kuwana han clans  \n",
            "nevertheless   at the battle of toba fushimi that broke out on january 0   even though the military force was holding up adequately   he judged that the former shogunate army was at a disadvantage   he abandoned his army and retreated to edo jo castle aboard warship kaiyo maru  \n",
            "soon an order was issued to subjugate yoshinobu as the enemy of the imperial court   and the imperial army led by general prince arisugawa taruhito set off to the east  \n",
            "yoshinobu suppressed the resistance party led by tadamasa oguri and asserted to comply with the order  \n",
            "in february   he entrusted kaishu katsu to take control of the situation and entered into a disciplinary confinement at daiji in located in kan  ei ji temple in ueno  \n",
            "furthermore   he passed on the estate of tokugawa soke   the main branch of tokugawa family   to his adopted son kamenosuke tayasu   future iesato tokugawa    \n",
            "the negotiation between katsu and the staff officer of the imperial army   takamori saigo   reached an agreement and bloodless surrender of edo jo castle took place   yoshinobu was moved to mito and he continued his disciplinary confinement in a room within kodokan of   mito   han   domain   school  \n",
            "in july     the entire   tokugawa family was relocated to sunpu  \n",
            "ichinoshin hara   warrior of mito han clan   vassal of hitotsubashi family  \n",
            "enshiro hiraoka   vassal of hitotsubashi family  \n",
            "amane nishi   political advisor to shogun yoshinobu tokugawa  \n",
            "tomoyoshi toki\n",
            "chojuro nakane   vassal of hitotsubashi family  \n",
            "magotaro umezawa\n",
            "keijuro kawamura\n",
            "chikatsugu matsudaira\n",
            "in september of 0 yoshinobu  s disciplinary confinement was lifted as a result of the end of boshin war  \n",
            "thereafter he did not participate in politics and spent his days immersed in his interests such as photography   hunting   net fishing   playing go and noh songs  \n",
            "he moved to sugamo   tokyo in 0  \n",
            "in the following year he visited the imperial palace and had and audience with emperor meiji  \n",
            "in 0 he was bestowed a title of koshaku and he established the yoshinobu tokugawa family aside from the main branch of tokugawa family  \n",
            "he passed on the family estate to yoshihisa tokugawa and he retired in 0  \n",
            "he died of pneumonia in 0  \n",
            "he was 0 years old and had the longest life among the successive shoguns  \n",
            "  until 0 the dates noted use the lunar calendar  \n",
            "0 0\n",
            "he became the head of the hitotsubashi family on september 0  \n",
            "he changed his name to yoshinobu on december 0  \n",
            "on the same day   he was given the rank of ju sanmi sakonoe gon chujo and assigned to gyobukyo  \n",
            "he married tadaka ichijo  s adopted daughter   mikako   on december 0   0  \n",
            "he became a sangi   councilor    \n",
            "in 0   he became a strong candidate for the heir to iesada tokugawa  \n",
            "because of ansei incident he was ordered to retire and enter into disciplinary confinement in august   0  \n",
            "he was relieved from retirement and disciplinary confinement in september   0  \n",
            "0\n",
            "on july 0   his position as the head of the hitotsubashi family was restored  \n",
            "on the same day   he became the guardian of the shogun   shogun kokenshoku   due to the emperor  s order  \n",
            "in november   he was reassigned to the rank of gon chunagon  \n",
            "in december   0   he assumed the position of chogi san  yo  \n",
            "0\n",
            "on march 0   he resigned from the position of chogi san  yo  \n",
            "on march 0   he resigned from his position as the guardian of the shogun   shogun kokenshoku    \n",
            "on the same day   he took the position as kinri goshuei sotoku   head of the imperial palace   emperor  s protector   and sekkai bogyo shiki   commander of osaka bay defense    \n",
            "at the kinmon incident he led the resistance army  \n",
            "0\n",
            "he resigned from the kinri goshuei sotoku   head of the imperial palace   emperor  s protector   towards the end of july  \n",
            "on august 0   he became the head of the main branch of the tokugawa family  \n",
            "on december 0   he was assigned the rank of sho nii gon dainagon and ukonoe daisho  \n",
            "he became the seii taishogun on the same day  \n",
            "0\n",
            "in september   0   his rank was changed to naidaijin  \n",
            "he retained the rank of ukonoe daisho  \n",
            "taisei hokan took place on october 0  \n",
            "on december 0   he resigned from the position of seii taishogun  \n",
            "on april 0   0   he was stripped of his political rank and duties  \n",
            "on september 0   0   his disciplinary confinement was lifted  \n",
            "on january 0   0   his courtly rank was restored to ju shii  \n",
            "on may 0   0   his rank was promoted to sho nii  \n",
            "on june 0   0   his rank was promoted to ju ichii  \n",
            "on november 0   0   he moved his residence to tokyo  \n",
            "on march 0   0   he had an audience with emperor meiji   first audience since taisei hokan    \n",
            "on june 0   0   he received the rank of koshaku  \n",
            "he was allowed to officially establish yoshinobu tokugawa   koshaku   family aside from the main branch of the tokugawa family  \n",
            "he was a member of house of peers   japan       december   0  \n",
            "0\n",
            "on april 0   his contributions to taisei hokan were recognized and he was awarded kun itto kyokujitsu daijusho from the meiji government  \n",
            "he retired on december 0  \n",
            "he died   at 0   0 a m    on november   0   0  \n",
            "on the same day   he was awarded kyokujitsu toka daijusho  \n",
            "as far as education and studying of martial arts are concerned   he was born and raised under the very best circumstance   among many forms of martial arts he was especially interested in and was an expert in shuriken   dirk throwing    \n",
            "even after taisei hokan   he practiced throwing the shuriken every day until he was sweating   and he is counted as one of the most famous persons among the shuriken experts  \n",
            "he tossed about a lot in bed in his sleep   and his father   nariaki tokugawa   thought that this would be a problem when he became the heir to the tokugawa family   in order to correct this   razor blades were planted into his pillow   this was done just to scare him   and in reality the razor blades were removed after he had fallen asleep so as to prevent any injuries    \n",
            "on the other hand   there is an anecdote that in his adulthood he slept with his wife and a concubine so that the three of them formed the letter   y   as a measure against assassination attempts  \n",
            "there is supposed to be a photograph of yoshinobu from his early childhood   but because cameras were probably not yet introduced to japan at that time   it is doubtful that the photograph is that of yoshinobu  \n",
            "yoshinobu visited iemochi when he fell ill   and at this occasion it is said that they had a normal conversation  \n",
            "his extraordinary talent was well known   and his father nariaki tokugawa  s confidant   tatewaki ajima   commented on him as   someone who will purify the tokugawa family line     he singly shouldered the expectation of restoring the shogunate  s authority   and when his highly touted tenure as the shogun began   his brilliance was praised as   the return of ieyasu tokugawa    \n",
            "yoshinobu  s intelligence was well known even among the anti shogunate party   and takayoshi kido of choshu han clan in particular was on guard saying     yoshinobu hitotsubashi  s daring and intelligent strategy cannot be underestimated  \n",
            "it is like seeing the return of ieyasu    \n",
            "regarding the decision on taisei hokan   ryoma sakamoto said     my respected shogun   i wonder what you are feeling in your heart today  \n",
            "i praise how you made the   difficult   decision   i praise how you made the   difficult   decision  \n",
            "i make a vow to dedicate my life to the lord     ryoma highly praised yoshinobu and he even considered giving yoshinobu a position as assistant kanpaku in the new government  \n",
            "however   yoshinobu did not learn about the existence of ryoma until the meiji era   after ryoma  s death    \n",
            "yoshinobu was not the only person who did not know about ryoma   ryoma  s fame in his life time was very much smaller than that after the meiji restoration  \n",
            "when he lost the battle of toba fushimi and retreated to edo   his enemies highly criticized him for   running away in the face of the enemy    \n",
            "there were people who made fun of yoshinobu for being a spineless coward   pointing out that even though he left behind ieyasu  s umajirushi   sign belonging to the clan general   chief and used in a battle   of golden fan   he always had his beloved concubine with him  \n",
            "however   there is a view that yoshinobu had no choice   but to retreat   because at that time it was necessary to suppress military revolts in edo as well as in musashi province   and because yoshinobu became the imperial court  s enemy   various clans were alienating themselves one after another from the shogunate   thus   even if he was able to protect osaka jo castle   a long term battle had to be avoided in order to prevent interventions from foreign countries  \n",
            "because he entered kan  ei ji temple for disciplinary confinement as soon as the new government designated him as the imperial court  s enemy   he was considered as a reasonable man who respected the emperor and the imperial court   perhaps this was due to the teaching of mito han   clan   that highly respected the emperor and also to the fact that his mother was from the imperial family     however   on the other hand   he was also called weak kneed because as the lord of his people   he was not able to stand resolutely in the face of the enemy  \n",
            "because of his compliance with the disciplinary confinement and the bloodless transfer of edo jo castle   the administrative power was transferred as a result of almost a bloodless revolution   he contributed greatly to the restoration and ensuring of independency of modern japan  \n",
            "businessman eiichi shibusawa was a vassal appointed during   yoshinobu  s   tenure as the head of the hitotsubashi tokugawa family   and they kept their contact even after the meiji restoration  \n",
            "in yoshinobu  s last years   shibusawa aimed to edit yoshinobu  s biography   he persuaded unwilling yoshinobu and started   sekimukai   meetings to hear stories directly from yoshinobu  \n",
            "a collection of these stories is called   sekimukai hikki    \n",
            "in one of the chapters which was recorded in the form of a discussion   one can see what yoshinobu  s voice   so to speak   was like  \n",
            "he used his age as an excuse to evade unpalatable questions and he let out his true feelings by saying     i didn  t really like hisamitsu shimazu   and   naomasa nabeshima was a cunning fellow     from these comments   one can see his personality and what he was feeling at that time  \n",
            "after yoshinobu  s death   tokugawa yoshinobu ko den   was completed based on these materials  \n",
            "in order to show his gratitude to emperor meiji who pardoned yoshinobu from being the enemy of the imperial court and gave him the title of koshaku   the highest rank of the peers   yoshinobu directed in his will that his funeral be conducted in the shinto style rather than the buddhist style  \n",
            "therefore   yoshinobu  s grave is neither in the tokugawa family graveyard of his family temple zojo ji temple or the tokugawa family graveyard at kan  ei ji temple   instead   a tumulus similar to that of the imperial family was built in yanaka cemetery  \n",
            "he did so because he was moved by the fact that emperor komei  s tomb was very simple     tokugawa yoshinobu ko den      \n",
            "after his pardon   yoshinobu enjoyed a relaxed life without any regards to his former vassals who devoted their lives to him   on this   roju katsukiyo itakura later commented     i regret working together with yoshinobu    \n",
            "even though he has an image of being a spineless coward   because he   ran away in the face of his enemy   during the battle of toba fushimi   there has recently been a focus on his attempt at constructing a new   modern political system after taisei hokan   and there is a movement to re evaluate his contributions including airing of   tokugawa yoshinobu     an nhk taiga drama    \n",
            "as with his father   nariaki   he liked pork from satsuma han clan and he was called butaichi sama   meaning hitotsubashi sama who likes pork    \n",
            "he was interested in items of western culture   and in his later years he liked to eat bread and drink milk and he enjoyed his interests in photography   fishing   riding the bicycle   microscopy and craft   embroidery     during his tenure as the shogun   he learned french from amane nishi but he gave up on this  \n",
            "he was well know as a photographer   but his techniques in photography seemed to have been mediocre  \n",
            "he often submitted his photographs to photography magazines   but his photographs were hardly ever printed in the magazines  \n",
            "his great grandchild   yoshitomo tokugawa   is a freelance photographer   he discovered photographs stored in the yoshinobu tokugawa family including the ones taken by yoshinobu and he published the photo album after organizing and editing them  \n",
            "yoshinobu  s biological younger brother   akitake tokugawa   was his friend who shared his hobbies in his life  \n",
            "he married mikako ichijo   name changed to mikako after the meiji restoration   on december 0   0  \n",
            "the first daughter   keikoin   between mika was born on july 0   0 but she soon died on july 0  \n",
            "afterwards   there were no more children between mika   the 0 boys and 0 girls born during the meiji era were children born between his two concubines  \n",
            "some of these children include the seventh son   yoshihisa   who became a koshaku and succeeded to the yoshinobu tokugawa family   tenth son   kuwashi   who married into kaishu katsu  s family and ninth daughter   tsuneko   who became prince fushimi hiroyasu  \n",
            "some of yoshihisa  s children include yoshimitsu tokugawa and princess kikuko who married prince takamatsu nobuhito  \n",
            "wife   mikako ichijo   name changed to mikako after the meiji restoration     daughter of kinhisa imadegawa   adopted daughter of tadaka ichijo   july 0   0   july 0   0\n",
            "concubine   nobu shinmura   daughter of masataka matsudaira   adopted daughter of takeo shinmura   died on february 0   0  \n",
            "first son   keiji   june 0   0   may 0   0  \n",
            "first daughter   kyoko   june 0   0   september 0   0   married satotaka tokugawa   on march 0   0  \n",
            "third daughter   tetsuko   october 0   0   december 0   0   married satomichi tokugawa on december 0   0  \n",
            "fifth son   nakahiro ikeda   august 0   0   january 0   0   adopted by terutomo ikeda on february 0   0  \n",
            "sixth son   hitoshi   august 0   0   november 0   0  \n",
            "sixth daughter   yoshiko   august 0   0   september 0   0  \n",
            "ninth daughter   tsuneko   september 0   0   august 0   0   married to prince fushimi hiroyasu on january 0   0  \n",
            "seventh son   yoshihisa tokugawa   september 0   0   january 0   0  \n",
            "eleventh daughter   hideko   march 0   0   july 0   0   married kuniyuki tokugawa on april 0   0  \n",
            "tenth son   kuwashi katsu   august 0   0   july 0   0   married into kaishu katsu  s family on january 0   0  \n",
            "concubine   ko nakane   first daughter of yoshisaburo nakane   died on december 0   0  \n",
            "second son   zenji   september 0   0   march 0   0  \n",
            "thrid son   takuma   october 0   0   july 0   0  \n",
            "fourth son   atsushi tokugawa   february 0   0   june 0   0  \n",
            "second daughter   kinko   april 0   0   july 0   0  \n",
            "fourth daughter   fudeko   july 0   0   november 0   0   married masaaki hachisuka on december 0   0  \n",
            "fifth daughter   hisako   august 0   0   october 0   0  \n",
            "seventh daughter   namiko   september 0   0   january 0   0   married hitoshi matsudaira on december 0   0  \n",
            "eighth daughter   kuniko   january 0   0   september 0   0   married kiko okochi on may 0   0  \n",
            "tenth daughter   itoko   september 0   0   october 0   0   married takachika shijo on may 0   0  \n",
            "a boy   stillborn on august 0   0  \n",
            "eighth son   yasushi   september 0   0   july 0   0  \n",
            "ninth son   makoto tokugawa   october 0   0   november 0   0  \n",
            "a girl   stillborn on june 0   0  \n",
            "concubine   yoshi   tatsugoro shinmon  s daughter  \n",
            "novels\n",
            "  the last shogun   tokugawa yoshinobu     ryotaro shiba  \n",
            "  tokugawa yoshinobu     sohachi yamaoka  \n",
            "movies\n",
            "  when the last sword is drawn   movie     0   starring hideaki ito\n",
            "drama series\n",
            "  ryoma ga yuku   nhk taiga drama       0   nhk taiga drama   yoshinobu performed by tatsunosuke onoe   the original    \n",
            "  ooku   tv drama   produced in0   drama content     0   fuji television   yoshinobu performed by shigeru amachi  \n",
            "  katsu kaishu   nhk taiga drama       0   nhk taiga drama   yoshinobu performed by masahiko tsugawa  \n",
            "  kashin   nhk taiga drama       0   nhk taiga drama   yoshinobu performed by takao ito  \n",
            "  ryoma ga yuku 0 version     0   tv tokyo new year  s wide historical drama   yoshinobu performed by tomoki kenmochi  \n",
            "  ooku   tv drama   produced in 0   drama content     0   fuji television   yoshinobu performed by gaku yamamoto  \n",
            "  byakkotai   a tv drama of nippon television       0   nippon television end of the year historical drama special   yoshinobu performed by nobuyuki ishida  \n",
            "  hana no shogai ii tairo to sakuradamon   a flamboyant life   ii tairo and sakurada mon gate       0   tv tokyo 0 hour super wide drama   yoshinobu performed by ikuo kokubun  \n",
            "  goryokaku   tv drama       0   nippon television end of the year historical drama special   yoshinobu performed by nobuyuki ishida  \n",
            "  tobu ga gotoku   nhk taiga drama       0   nhk taiga drama   yoshinobu performed by kunihiko mitamura  \n",
            "  katsu kaishu   tv drama       0   nippon television end of the year historical drama special   yoshinobu performed by masahiko tsugawa  \n",
            "  tokugawa yoshinobu   nhk taiga drama       0   nhk taiga drama   yoshinobu performed by masahiro motoki  \n",
            "  ooku   tv drama   produced in 0   drama content     0   fuji television   yoshinobu performed by ginnojo yamazaki  \n",
            "  matamo yametaka teishudono   bakumatsu no meibugyo   oguri kozunosuke     quitting job again   mr  husband     the great magistrate at the end of the edo period   oguri kozukenosuke       0   nhk new year  s historical drama   starring yoshinori hiruma  \n",
            "  shinsengumi       0   nhk taiga drama   starring yoshinobu performed by tomohiko imai  \n",
            "  atsuhime   nhk taiga drama     0   nhk taiga drama   yoshinobu performed by takehiro hira  \n",
            "  sekimukai hikki     memoirs  \n",
            "  tokugawa yoshinobu ko den   a biography of yoshinobu tokugawa     by eiichi shibusawa   0\n",
            "  shogun ga totta meiji   tokugawa yoshinobu ko satsuei shashinshu   meiji as photographed by a shogun   an album of photographs taken by yoshinobu tokugawa   asahi shinbun sha   0   isbn 0\n",
            "notes\n",
            "the jr nara line is a railway line   arterial line   of the west japan railway company   jr west   that runs between kizu station   kyoto prefecture   in kizugawa city   kyoto prefecture   and kyoto station in the shimogyo ward of kyoto city   kyoto prefecture  \n",
            "the entire rail line is included in the section covering the metropolitan area and its suburbs  \n",
            "although the nara line officially starts at kizu station because it is historically a branch line of the kansai main line   outbound trains   odd numbered trains   run from kyoto to kizu and inbound trains   even numbered trains   run in the opposite direction  \n",
            "unless otherwise specified   the descriptions are given according to the outbound direction from kyoto to kizu   except for in the track data section  \n",
            "the line  s own color is brown   which has been chosen to represent   a classic   sophisticated impression suitable for connections between two historic cities    \n",
            "administration   type of business     west japan railway company   railway business operator  \n",
            "track length   operating kilometers     0 0 km\n",
            "track gauge   0 mm\n",
            "number of stations   0   including the stations at both ends of the track section  \n",
            "double track section   shinden station   uji station   jr fujinomori station   kyoto station\n",
            "electrified section   the entire rail line is electrified   dc 0v    \n",
            "block   railway     automatic block system   double track section     single track automatic block system   single track section  \n",
            "safety equipment   ats   automatic train stop system  \n",
            "operation direction center   shin osaka integrated direction center\n",
            "maximum speed   0 km   h   double track section     0 km   h   single track section  \n",
            "all stations except kizu station are managed by the kyoto branch of the west japan railway company   while the kizu station is managed by the osaka branch of the west japan railway company  \n",
            "the line between kyoto station and kizu station is called the nara line   but the entire line lies within kyoto prefecture   not in nara prefecture  \n",
            "this is because   initially   the nara line was built by nara railways between kyoto station and nara station   but kizu station and other stations south of kizu station were separated from the nara line in order to be part of a route between osaka and nagoya after nara railways was merged into kansai railways  \n",
            "however   trains run from kizu station to nara station in nara prefecture through the kansai main line   yamatoji line    \n",
            "jr west established popular names   jr kyoto line   jr kobe line or the like   for the lines in the urban network with the revision of march 0   0   but a popular name was not given to the nara line  \n",
            "the nara line is also referred to as the   jr nara line   in order to distinguish it from the kintetsu nara line of kintetsu corporation  \n",
            "the nara line was referred to as the   jnr nara line   in the era of japan national railways   jnr     the former jr   and the information board included the english notation   jnr    \n",
            "the kintetsu nara line competes with the yamatoji line between nanba and nara   whereas the jr nara line competes with the kintetsu kyoto line  \n",
            "while many trains used to run on the keihan uji line and the kintetsu kyoto line   both of which run parallel to the nara line     the nara line was only electrified toward the end of jnr in 0   consequently   it remained just a small local line on which the jnr   jr commuter train series 0   comprising two cars or the like   ran for some time   even after electrification   until the privatization of the jnr  \n",
            "however   after the foundation of jr west and the nara line  s incorporation into the urban network   the improvement of transportation services was quickly achieved with   for example   trains comprised of four cars   some comprised of six cars     double tracked in part   or with increased numbers of trains   including rapid services such as the   miyakoji rapid service    \n",
            "today   the nara line is characterized by the access it offers to sightseeing areas such as byodo in temple in uji city   or by a commuter line for passengers from the southern part of the kyoto prefecture   such as joyo city  \n",
            "however   trains go around toward the east between momoyama station and shinden station and run the edge of the central part of town in joyo or to the south of joyo   thus the regional and transportation gaps between the nara line and the kintetsu kyoto line remain  \n",
            "rolling stock owned by the osaka branch for the nara rail yard is used   but station facilities are maintained by the kyoto branch   except for kizu station    \n",
            "the following commuter passes can be used at all stations on this line   j thru   icoca   suica   which is offered by the east japan railway company   jr east     toica   which is offered by the central japan railway company   jr central     and pitapa   which is offered by the surutto kansai association  \n",
            "simple ticket gates are used at joyo station or stations to the south of joyo station   as some unmanned stations still exist  \n",
            "after the limited express   kuroshio   connecting kyoto station with wakayama station via the nara   sakurai and wakayama lines ceased operation in 0   only the local trains ran until 0   except for the temporary operation of a special limited express     but now there are four types of trains   the miyakoji rapid service   the rapid service   the trains classified by operation and section   and the local train  \n",
            "the miyakoji rapid service  the fastest train on the nara line  runs mainly during the day  \n",
            "the train basically runs every 0 minutes and variously connects with the local trains at uji station  \n",
            "the public gave this train the nickname   miyakoji rapid train   before it went into operation  \n",
            "not only japanese people but also foreign visitors use the miyakoji rapid service as a train connecting kyoto with nara   both of which are sightseeing destinations  \n",
            "the cars are all from the jr suburban train series 0   which comprises either four or six cars  \n",
            "the maximum train speed is 0 km   h in double track sections   or 0 km   h in other sections  \n",
            "the standard travel times are approximately 0 minutes from kyoto to nara and about 0 minutes from nara to kyoto  \n",
            "a train bound for kyoto stops at kamikoma station or tanakura station for about two minutes and passes the miyakoji rapid service bound for nara   thus making a slight difference between the travel times of the inbound train and outbound train  \n",
            "during the new year holiday the train temporarily stops at inari station for visitors to fushimi inari taisha shrine  \n",
            "history\n",
            "march 0   0   operations began  \n",
            "march 0   0   the train began making stops at tofuku ji station and tamamizu station through a revision of the schedule  \n",
            "the rapid service runs during the morning and evening rush periods  \n",
            "it stops at jr ogura station and shinden station   kyoto prefecture     through which the   miyakoji rapid service   passes  \n",
            "the cars used are all from the jr suburban train series 0   which comprises either four or six cars  \n",
            "the train basically runs every 0 minutes and variously connects with the local train at the uji station  \n",
            "like the miyakoji rapid service   the rapid service passes through narayama station   at which the rapid train on the yamatoji line stops  \n",
            "before the   miyakoji rapid train   came into operation in march 0   the rapid train had stopped at kyoto   uji   joyo   except at the beginning     kizu and nara stations   employing the jnr   jr suburban train series 0 for its cars   since then   however   this series has not been used on the nara line  \n",
            "some rapid service trains run through to the yamatoji line in the morning and evening due to the schedule of operation  \n",
            "the regional rapid service runs mainly in the morning and nighttime hours  \n",
            "initially   the operation was limited to weekdays   but with the schedule revision of march 0   0 it was changed to saturdays and holidays in addition to weekdays  \n",
            "the regional rapid service stops at every station between uji station and nara station   while some regional rapid services bound for kizu used to pass the local train at uji station in the late evening   etc     due to the schedule   however   based on the current schedule   the regional rapid service doesn  t pass the local train before reaching the arrival station  \n",
            "all regional rapid services employ the jr suburban train series 0   except that one regional rapid service leaving kyoto station between 0 p m  and 0 p m  on saturdays and holidays employs the jnr   jr commuter train series 0  \n",
            "due to the operation schedule   some regional rapid services run through from the yamatoji line during the morning rush period  \n",
            "the regional rapid service on the nara line formerly used orange letters as the line  s own color   but it was the same color as the one used for the   miyakoji rapid service   and the rapid service so as to distinguish the regional rapid service on the nara line from that of the yamatoji line   however   in march 0   with the replacement of signs along with the revision of the schedule   a sign for the regional rapid service was changed to include green letters   whereby the use of the line color was stopped  \n",
            "at the kamikoma station or stations to the north of kamikoma station   purple letters are used on the timetable boards  \n",
            "the local train runs between kyoto station   uji station   joyo station and nara station  \n",
            "the train runs every 0 minutes during the day between kyoto station and joyo station   and every 0 minutes between joyo station and kizu station  \n",
            "basically   the cars are from the jnr   jr commuter train series 0   but some are from the jr suburban train series 0  \n",
            "due to the schedule   some local trains go through to the yamatoji line in the early morning and late evening  \n",
            "formerly   some local trains would go through to the sakurai line   but with the schedule revision of september 0   0   the nara and sakurai lines were divided  \n",
            "formerly   a limited express ran on the nara line as a special train  \n",
            "a limited express which had the nickname   kuroshio xx   from 0 to 0 and   kuroshio xx   from 0 to 0   the jnr   jr limited express series 0ran between kyoto station and shirahama station via the kansai main line   the kansai main line hanwa freight line and the hanwa line  \n",
            "the kinki edition of the yomiuri shinbun newspaper announced on january 0   0 that a limited express comprising the jnr   jr limited express series 0 would begin operating   as a regular train   on the nara line in about 0   but eventually the plan was canceled  \n",
            "the miyakoji leisure train ran between kyoto station and sakurai station   the miyakoji rapid service ran for the section between kyoto station and nara station   for some time   but now there is no train excluding a special train going through to the sakurai line  \n",
            "the uji gawa river fireworks display   nearest stop   uji station   is held in mid august   when the special schedule is introduced to secure sufficient transport capacity before 0 p m  to the end   after 0 p m    only the local trains run between kyoto station and uji station at intervals of approximately 0 minutes  \n",
            "kizu station is the arrival and departure station of some local trains  \n",
            "when the nara line was a single track   a special schedule was adopted during the new year holidays   according to which the rapid train stopped at every station between kyoto station and uji station   and the number of trains traveling to uji station was increased  \n",
            "during some new year holidays the train would arrive at and depart from momoyama station  \n",
            "in about 0   mainly on saturdays and holidays   a through train to the sakurai line bound for tenri station ran once an hour by extending its regular rapid service  \n",
            "occasionally   trains reserved for groups enter the nara line from around the country  \n",
            "during the tenrikyo autumn grand service held every october 0   particularly   the through train denoted as a   train reserved for groups   tenrikyo transportation train   runs from kyoto station to kizu station via the nara line   to nara station via the yamatoji line   and to tenri station via the sakurai line  \n",
            "all cars belong to the nara rail yard  \n",
            "jr suburban train series 0\n",
            "the nara line shares the jr suburban train series 0   comprising two   four or six cars   with the yamatoji line   the osaka loop line   etc  \n",
            "the jr suburban train series 0 comprising either four or six cars is used mainly for the   miyakoji rapid service   and the   rapid service     while the one comprising four cars mainly enters the nara line  \n",
            "the jr suburban train series 0   comprising two cars joining two other cars   runs according to the holiday schedule  \n",
            "jnr   jr commuter train series 0\n",
            "the jnr   jr commuter train series 0   comprising four cars   is used on the nara line for the local train   while the one comprising eight cars   four joining four other cars   is used on the osaka loop line and on the yamatoji line for the regional rapid service during the rush hours  \n",
            "part of the rolling stock is replaced with stock transferred from the osaka loop line  s morinomiya rail yard  \n",
            "the line connecting kyoto   kizu and nara was opened by the nara electric railway   and initially trains ran the same route as the current kintetsu kyoto line between kyoto and momoyama  \n",
            "in 0   the tokaido main line between baba   currently zeze   station and kyoto station was switched to the current line via the higashiyama tunnel   and on the same day the old tokaido main line between kyoto station and inari station and a new line between inari station and momoyama station came to serve as the nara line   whereby the line between kyoto station and fushimi station was abolished and the line between fushimi station and momoyama station was made a freight line  \n",
            "later   the land after the line between the kyoto station and the fushimi station was sold to the nara electric railway   a predecessor of the kintetsu kyoto line  \n",
            "august 0   0   the line between kyoto station   inari station and otani station   shiga prefecture   was opened as a governmental railway   later as the tokaido main line    \n",
            "inari station was opened  \n",
            "september 0   0   the line between kyoto station and fushimi station   0 miles 0 chains ≒ 0 0 km   was opened by nara railways  \n",
            "fushimi station was opened  \n",
            "kyoto station was shared with the governmental railway  \n",
            "november 0   the line between fushimi station and momoyama station   0m0c ≒ 0 0 km   was extended and opened  \n",
            "momoyama station was opened  \n",
            "january 0   0   the line between momoyama station and tamamizu station   0m0c ≒ 0 0 km   was extended and opened  \n",
            "kohata   uji   shinden   nagaike and tamamizu stations were opened  \n",
            "march 0   the line between tamamizu station and kizu station   0m0c ≒ 0 0 km   was extended and opened  \n",
            "tanakura and kizu stations were opened  \n",
            "april 0   the line between kizu station and nara station was extended and opened   thereby connecting kyoto station and nara station  \n",
            "april 0   to ji provisional train station was opened  \n",
            "april 0   0   the nara railways kyoto station was renamed as shichijo station  \n",
            "may 0   0   kamikoma station was opened  \n",
            "november 0   the operating distance was changed from miles and chains to miles   e g    from 0m0c to 0 0m    \n",
            "february 0   0   the track was transferred from nara railways to the kansei railway company  \n",
            "october 0   0   the kansei railway company was nationalized  \n",
            "august 0   0   shichijo station was merged into kyoto station  \n",
            "october 0   0   the line names were given and the line between kizu station and kyoto station was called the nara line  \n",
            "december 0   0   the ujigawa signal box was built for the line between uji station and kohata station  \n",
            "june 0   0   the hachijo signal box was built for the line between fushimi station and kyoto station  \n",
            "november 0   the ujigawa signal box was abolished  \n",
            "august 0   0   the hachijo signal box was abolished  \n",
            "march 0   0   ujigawa temporary signal box was built for the line between uji station and kohata station  \n",
            "august 0   the line between kyoto station and fushimi station   0 0m ≒ 0 0 km   on the nara line was abolished  \n",
            "the passenger service was abolished between fushimi station and momoyama station   0 0m ≒ 0 0 km    \n",
            "the tokaido main line between kyoto station and inari station   0 0m ≒ 0 0 km   was incorporated into the nara line  \n",
            "the new line between inari station and momoyama station （ 0 0m ≒ 0 0 km   was opened  \n",
            "april 0   0   the ujigawa temporary signal box   宇治川仮信号所   was changed to the ujigawa temporary signal station   宇治川仮信号場    \n",
            "february 0   0   aodani bairin temporary train station was opened  \n",
            "april 0   the ujigawa temporary signal station was abolished  \n",
            "september 0   0   the freight branch line between momoyama station and fushimi station was abolished  \n",
            "fushimi station was abolished  \n",
            "april 0   0   the operating distance was changed to be given in meters from miles   e g    from 0 0m to 0 0 km    \n",
            "december 0   0   aodani bairin temporary train station was upgraded to yamashiro aodani station  \n",
            "july 0   0   yamashiro taga station was opened  \n",
            "december 0   0   tofuku ji station was opened  \n",
            "july 0   0   joyo station was opened  \n",
            "april 0   0   obaku station was opened  \n",
            "march 0   0   centralized traffic control was introduced  \n",
            "october 0   0   the line between kyoto station and kizu station   to nara station   became electrified  \n",
            "the jnr   jr commuter train series 0 and the jnr   jr suburban train series 0 began operating  \n",
            "the limited express   kinokawa     between kyoto station and wakayama station via the nara   sakurai and wakayama lines   was abolished  \n",
            "april 0   0   the japan national railways was split up and privatized to form the jr group   as a result of which the jr nara line became part of the west japan railway company  \n",
            "the japan freight railway company became a railway business operator for every line  \n",
            "march 0   0   the rapid service comprising the jnr   jr suburban train series 0 began operating  \n",
            "october 0   0   rokujizo station was opened  \n",
            "the rapid service began making stops at joyo station  \n",
            "september 0   0   the jnr   jr commuter train series 0 was withdrawn  \n",
            "  the jnr   jr commuter train series 0 came to run only on the sakurai and wakayama lines    \n",
            "march 0   0   jr fujinomori station was opened  \n",
            "may 0   0   the rapid service began making stops at rokujizo station  \n",
            "march 0   0   the lines between kyoto station and jr fujinomori station   uji station and shinden station became double track  \n",
            "jr ogura station was opened  \n",
            "the   miyakoji rapid service   comprising the regional rapid service and the jr suburban train series 0 began operating  \n",
            "october 0   the rapid service and regional rapid service began making stops at tofuku ji station  \n",
            "march 0   0   the miyakoji rapid service and the rapid service began making stops at tofuku ji station and tamamizu station   and at tamamizu station   respectively  \n",
            "april 0   the japan freight railway company left the type ii railway business  \n",
            "april 0   0   the ats was introduced between yamashiro aodani station and kizu station  \n",
            "april 0   the ats p was introduced between obaku station and yamashiro aodani station  \n",
            "april 0   the ats p was introduced between kyoto station and obaku station  \n",
            "the single track sections between jr fujinomori and uji stations   and between shinden and kizu stations   remain under discussion   but no specific plan has been established  \n",
            "moreover   the acquisition of land along national route 0 between jr fujinomori station and momoyama station   and along the keihan uji line between kohata station and uji station   near obaku station   is particularly difficult  \n",
            "due to a shortage of budget   no further plan will proceed before the double tracking of the entire sagano line   between the sanin main line kyoto station and sonobe station   is completed  \n",
            "at the council for transportation in the kinki region   a plan for operation of the nara line through to the tokaido main line   jr kyoto line   in the future is under discussion   there was once an overnight service from the late evening of december 0 to the early morning of january 0    \n",
            "uji city is planning to build a new station between obaku station and uji station  \n",
            "there is no intermediate station under direct control   except for rokujizo station and uji station   jr west    \n",
            "the distance of operation   in kilometers   from kyoto station is placed in parentheses  \n",
            "kyoto station   0 0 km     hachijo temporary signal box   0 0 km     to ji temporary depot   0 0 km     fushimi station   0 0 km     momoyama station   0 0 km  \n",
            "the distance of operation   in kilometers   from kyoto station are placed in parentheses  \n",
            "ujigawa temporary signal station   from obaku station to uji station   approximately 0 0 km ）\n",
            "the keiji bypass is the bypass of a national highway that extends from kusatsu city in shiga prefecture to kumiyama town   kuse gun in kyoto prefecture  \n",
            "it is a planning route designated as a local high standard highway  \n",
            "the first description below refers to a toll road called the keiji bypass and the second refers to a bypass joining national highways 0 and 0 that is also called the keiji bypass  \n",
            "its toll road section is integrated in the meishin expressway to connect the ritto interchange   in the direction of nagoya city   and the suita interchange   in the direction of osaka city   of the meishin expressway by passing round the urban areas of otsu city and kyoto city  \n",
            "in terms of laws   the section from the seta higashi interchange to the kumiyamayodo interchange   being part of the national highways 0 and 0   is a regional high standard road   the kumiyamayodo interchange to oyamazaki junction   a part of national highway 0   is a high standard highway designated by the ministry of land   infrastructure and transportation   a national highway only for vehicles     kyoto daini soto kanjo doro belt line of the kyoto jukan jidoushdo expressway     and the section from oyamazaki junction to kumiyamyodo interchange is a national express highway   a section called the keiji bypass of the chuo jidosha do national expressway    \n",
            "to reduce congestion on the meishin expressway and respond to the need for a road network that could serve as a radial road and belt line in kyoto city   the construction of the kyoto daini soto kanjo doro belt line was subsequently planned to connect the kyoto tanba doro   formerly the bypass of national highway 0   to the keiji bypass  \n",
            "the kyoto jukan jidoshado expressway and the kyoto daini soto kanjo doro belt line were designated as national highway 0   and their construction has been continuously promoted  \n",
            "on traffic signs   the section between the seta higashi junction and the oyamazaki junction is identified as the keiji bypass so as to make it easier for drivers to understand the signs  \n",
            "whichever route is chosen  the meishin expressway or the keiji bypass  the same toll and etc commuter discount service apply  \n",
            "the keiji bypass here means the service line known as such   between seta higashi interchange and oyamazaki junction   unless otherwise specified  \n",
            "origin   seta oe cho   otsu city   shiga prefecture\n",
            "terminal   oyamazaki cho   otokuni gun   kyoto prefecture\n",
            "total length   0 0 km\n",
            "number of lines   four\n",
            "speed limit   0 km   h\n",
            "redemption date   april 0   0\n",
            "shiga prefecture\n",
            "kyoto prefecture\n",
            "uji city   kumiyama ｔown   kuse gun   yawata city   fushimi ku   kyoto city   oyamazaki cho otokuni gun\n",
            "national highway 0 keiji bypass   seta higashi ic   kumiyamayodo ic\n",
            "national highway 0   kyoto daini soto kanjo doro belt line   kumiyamayodo ic   oyamazaki jct\n",
            "chuo jidosha do national highway   oyamazaki jct   kumiyamayodo ic   revised name  \n",
            "sections of the interchange number column that are highlighted in green indicate that the roads are already in service  \n",
            "the facilities with a highlight in ash have not been established  \n",
            "roads are operated by local municipalities unless otherwise specified  \n",
            "west nippon expressway company limited   kansai branch   ibaraki office\n",
            "  roadways managed by west nippon expressway company limited   kansai branch   ibaraki office   include the kyoto higashi interchange   suita junction section of the meishin highway   the daini keihan doro bypass   and kyoto areas of the keinawa jidoshado expressway    \n",
            "august 0   0   the opening of seta higashi ic   jct   ogura ic took place  \n",
            "march 0   0   the opening of ogura ic   kumiyama jct took place  \n",
            "the toll system was integrated into that of the japan highway public corporation   the present nexco    \n",
            "august 0   0   the opening of kumiyama jct   oyamazaki jct made the entire line available  \n",
            "december 0   0   the oyamazaki ic opened  \n",
            "fiscal year 0 average weekday 0 hour traffic volume   numbers     road traffic census  \n",
            "oyamazaki ic   jct   kumiyamayodo ic   0 0\n",
            "the uji tunnel of this section is   at 0 0 meters   the longest on the national highway   lane changes are prohibited in the tunnel except between the area 0 meters short of the kasatori interchange and the ujihigashi interchange  \n",
            "a well known japanese movie called   odoru daisosasen the movie 0   block up the rainbow bridge   was filmed around the kumiyama junction before it was opened  \n",
            "because less distance is traveled between the seta higashi junction and the oyamazaki interchange   junction if drivers take the keiji bypass instead of the meishin highway   a driver driving at the speed limit on the keiji bypass will arrive at their destination more quickly than a driver driving at the speed limit on the meishin highway  \n",
            "the keiji bypass has no rest service areas or other such facilities   so it  s less congested than the meishin expressway   which has two rest areas   otsu rest area and katsuragawa rest area between setahigashi and oyamazaki    \n",
            "however   due to the design of both the seto higashia and oyamazaki junctions   which require ramps   in the highway lanes   to connect them to the meishin highway   in particular the very complicated design of the oyamazaki junction     drivers have to reduce their speed  \n",
            "another disadvantage is that drivers often encounter congestion as they attempt to merge into the left lane of the meishin highway from the oyamazaki junction toward suita or osaka  \n",
            "moreover   many drivers dislike the winding areas and slopes along the keiji bypass   so the keiji bypass doesn  t fully perform its function as a bypass  \n",
            "furthermore   it  s a bypass of national highway 0   a general national highway     which means it isn  t a expressway but a motor highway  \n",
            "when drivers want to get on the keiji bypass toward suita or osaka from the east side of the uji gawa river   near the ujihigashi interchange     they must go to the ujinishi interchange  \n",
            "this is because the ujihigashi interchange is a half interchange  \n",
            "as a result   local roads linking the ujihigashi and ujinishi interchanges are commonly congested during the rush hours  \n",
            "this area is outside the vics   vehicle information and communication system   link zone   which makes it difficult for drivers to use their congestion avoidance schemes  \n",
            "uji city   kyoto prefecture   ujinishi interchange     kumiyama cho   kuse gun   terminus  \n",
            "see   kyoto daini soto kanjo doro   for descriptions of the sections west of kumiyama cho  \n",
            "the upper is nearer to the origin   while the lower is nearer to the termination point  \n",
            "the left is nearer to tokyo   while the right is farther from tokyo  \n",
            "fiscal year 0 average weekday 0 hour traffic volume   numbers     road traffic census  \n",
            "higashi yagura 0 chome   kusatsu city   0 0\n",
            "mori aza ouchi   kumiyama town   kuse gun   0 0\n",
            "ministry of land   infrastructure   transport and tourism   shiga national highway office   kusatsu maintenance branch office\n",
            "ministry of land   infrastructure   transport and tourism   kyoto national highway office   kyoto first maintenance branch office   national highway 0   national highway 0 within kyoto city  \n",
            "ministry of land   infrastructure   transport and tourism   kyoto national highway office   kyoto second maintenance branch office   national highway 0  \n",
            "higashi hongan ji temple is a shin buddhist temple in shimogyo ward   kyoto city   kyoto  \n",
            "it is the head temple of the otani sect   which is part of the shinshu kyodan rengo   ten schools of shin buddhism     and also joins nishi hongan ji temple   officially hongan ji temple   head temple of the hongan ji sect of shin buddhism     as one of the main temples of shin buddhism  \n",
            "the official name of the temple was   hongan ji   until 0   since which time it has been   shinshu honbyo temple    \n",
            "to distinguish it from nishi hongan ji temple   also in shimogyo ward   it is often called   o higashi san    \n",
            "  for history before the east west split   refer to the article on the history of hongan ji temple    \n",
            "kyounyo   kouju     the 0th master of hongan ji temple   was given a domain for a temple to the east of hongan ji by ieyasu tokugawa   and founded higashi hongan ji temple when hongan ji temple split in 0  \n",
            "located in present day karasuma shichijo   shimogyo ward   kyoto   it is called higashi temple   eastern   hongan ji temple because it is to the east of nishi   western   hongan ji temple   which is located at horikawa shichijo  \n",
            "it was a religious corporation overseen by the otani sect until 0   at which time hongan ji temple was legally dissolved and absorbed by the sect  \n",
            "after that   higashi hongan ji temple  s official name became   shinshu honbyo temple       honbyo   means the original sanctuary where followers hear the teaching of the patriarch shinran   the resting place of shinran    \n",
            "it is also to distinguish it from the breakaway higashi hongan ji sect of shin buddhism and emphasize its propriety  \n",
            "cf o higashi rebellion\n",
            "having had 0 fires in the edo period   so many that it acquired the nickname     flaming hongan ji temple     the current buildings   though many burned and were rebuilt in the meiji period   maintain the style of the time in their construction and the paintings on the walls  \n",
            "the goei do   which houses the statue of shinran shonin   is among the largest wooden buildings in the world   currently covered by scaffolding for a major restoration    \n",
            "the goei do gate is also one of the three famous gates of kyoto  \n",
            "shinran  s   kyogyo shinsho     national treasure     bando edition   is in its possession  \n",
            "shosei en garden   on the neighboring grounds   is a national scenic area  \n",
            "the current master of the temple is choken otani   jonyo    \n",
            "national treasures\n",
            "important cultural assets\n",
            "take the kyoto city subway to gojo station\n",
            "universities\n",
            "otani university\n",
            "kyoto koka women  s university\n",
            "osaka ohtani university\n",
            "doho universiity\n",
            "nagoya zokei university of art   design\n",
            "nagoya college of music\n",
            "aichi bunkyo university\n",
            "aichi shinshiro otani university\n",
            "under the prewar school educational system   third high school was one of the high schools that later became a foundation of the present kyoto university  \n",
            "the school was commonly called   sanko    \n",
            "hikoichi orita was its first president  \n",
            "0   seimi kyoku was founded in osaka\n",
            "the school underwent continuous changes in structure and nomenclature  \n",
            "0   institute of chemistry   kagakusho   → institute of science   rigakusho   → kaiseijo →\n",
            "0   fourth university district no  0 junior high school   →\n",
            "0   third university district no  0 junior high school → kaimei gakko →\n",
            "0   osaka school of foreign languages → osaka english academy\n",
            "0   osaka technical college\n",
            "0   osaka middle school\n",
            "0   it was reorganized as a branch school of a university   after which the third higher middle school   dai san koto chugakko   was established in 0  \n",
            "0   okayama prefectural medical school became the third higher middle school medical faculty   sakae machi   okayama city    \n",
            "the main curriculum   university preparatory education   started  \n",
            "0   third higher middle school was relocated from higashi ku   present day chuo ku     osaka city to kamigyo ku   present day sakyo ku     kyoto city  \n",
            "the college of law was also founded in addition to the already established college of medicine  \n",
            "0   the third higher middle school was elevated to status as the third high school  \n",
            "the main curriculum   university preparatory education   and preparatory course were abolished  \n",
            "the college of engineering was established  \n",
            "a preparatory course was re established for the kyoto imperial university   which was to be founded in 0   thus restoring university preparatory education    \n",
            "the colleges of law and engineering closed     the ordinance was issued in 0    \n",
            "0   college of medicine was transformed into okayama university  \n",
            "0   along with the former kyoto university   called kyoto imperial university until 0   and the medical specialization division of kyoto university   third high school came to be included in the present kyoto university as its branch school   bunko   and provided education in the liberal arts  \n",
            "0   the third high school was abolished once the last of its students had graduated  \n",
            "0   the branch school was renamed the college of liberal arts and sciences  \n",
            "0   in the course of reforming the college of liberal arts and sciences   the graduate school of human and environmental studies was established  \n",
            "0   the college of liberal arts and sciences was reorganized as the faculty of integrated human studies  \n",
            "the college of liberal arts and sciences was abolished  \n",
            "0   the faculties of integrated human studies and human and environmental studies were merged  \n",
            "  see education system order\n",
            "amaterasu omikami   tensho daijin   is a humanized shinto god in japanese  \n",
            "in   engishiki   list of official shrines         amaterasu   of amaterasu omikami is cited as   amateru   when worshipped as the god of nature  \n",
            "amaterasu omikami is the god of the sun and one of the oyagami   parent deities   of the imperial family   the kososhin   imperial ancestor      \n",
            "in   kojiki   records of ancient matters       amaterasu omikami is referred to as 天照大御神 whereas in   nihonshoki   chronicles of japan     the god is cited as 天照大神  \n",
            "another name given to her is ohirumenomuchi no kami  \n",
            "in   nihonshoki   chronicles of japan       amaterasu omikami is referred to under different names   whereas in   kojiki   records of ancient matters     she is consistently cited as   天照大御神    \n",
            "amaterasu omikami is considered to be female   but some say the god is male  \n",
            "because the name of amaterasu omikami was used in the political scene as shown in   kotai jingu gishikisho     shoei mishima and tsutomu saijo even argue that takamimusubi was the kososhin   imperial ancestor       and that hirume was deified and became amaterasu omikami  \n",
            "at ise jingu shrine   amaterasu omikami is typically referred to as amaterasu sume omikami   sume omikami or amaterasimasu sume omikami   particularly when the name is uttered before the altar in ceremonies  \n",
            "in academic documents she is now customarily spelled   アマテラス   in katakana  \n",
            "some say that in old days she was cited as 天照太神  \n",
            "how amaterasu omikami is described in myths\n",
            "according to   kojiki   records of ancient matters       amaterasu omikami was born from izanagi as she rinsed her left eye when safely returning from yomi   the world after death   where izanami lived   in order to remove the dirt  \n",
            "at this time   tsukuyomi was born from her right eye and susanoo from her nose   and the three together have come to be called mihashira no uzuno miko   three noble children    \n",
            "izanagi then instructed amaterasu omikami to rule takamanohara  \n",
            "because susanoo — who had been instructed to rule unabara   the sea   — kept crying   saying he wanted to go to the land of the roots   the land of izanami     he was finally expelled by izanagi  \n",
            "although susanao ascended to takamanohara to meet amaterasu omikami   his elder sister   on the way to the land of the roots   amaterasu omikami   who assumed her brother was coming to deprive her of takamanohara   armed herself and waited for him  \n",
            "after susanoo made a pledge to prove that he had no such intention   five male gods were born from amaterasu omikami  s monozane   the media from which gods are given birth   and three female gods from that of susanoo   and susanoo declared victory  \n",
            "the following are the five gods considered to be born from amaterasu omikami  s monozane and are therefore her children  \n",
            "amenooshihomimi\n",
            "amenohohi\n",
            "amatsuhikone\n",
            "ikutsuhikone\n",
            "kumanokusubi\n",
            "susanoo   swept up by the event   became wild and violent at takamanohara   causing his sister to hide herself in ama no iwato   the cave of heaven    \n",
            "this threw the entire world into darkness   and various problems occurred  \n",
            "yao yorozu no kami like chichibu no kami   the god of wisdom     ameno yagokoroomoikane no mikoto and ameno koyane no mikoto eventually became successful in getting her out of ama no iwato   but she drove him out of takamanohara  \n",
            "amaterasu omikami — who had decided that she would have one of her sons   amenooshihomimi   descend to ashihara no nakatsukuni — dispatched amatsu kami   the god of heaven   there  \n",
            "by the time amenooshihomimi was ready to descend to ashihara no nakatsukuni   where peace and order had been established   ninigi — the grandchild of amaterasu omikami — was born and ordered to go down there by amaterasu omikami  \n",
            "interpretations\n",
            "because some myths say that amaterasu omikami sought opinions from her elder amatsu kami   the god of heaven   about her specific acts whether they were good or not and even followed his instructions   some consider that amaterasu omikami was a deified emperor who ruled the country by conducting religious services  \n",
            "others have viewed her as a deified miko   female shinto attendant    \n",
            "her alias name     hirume     literally means   the woman of the sun     as a miko   an attendant of the sun god  \n",
            "according to a theory   a miko who had served the god of the sun came to be regarded as identical to the god himself   this caused the god to be thought of as female   although the god of the sun is typically male  \n",
            "other theories consider that amaterasu omikami is female because her myth was established during the era of empress jito   or because himiko was her model  \n",
            "the idea that the god of the sun   amaterasu omikami   and the god of the moon   tsukuyomi   were born from the eyes of izanagi shows a strong influence of taoism   whose canons   reiho gobujo     completed by the sixth century   and   goho innenkyo     written at the beginning of the seventh century   respectively contain descriptions of   turning his eyes into the sun and the moon   and   turning his left eye into the sun and his right eye into the moon    \n",
            "some say there is a parallelism between shinto and taoism in some of the terms used for ise jingu shrine   which worships amaterasu omikami — such as okami   great god     jingu   itsuki no miya   itsuki no yakata   murasaki no mizo   kiyoginu   mitegura and usukinu — and in the idea behind dividing its shrine into kotai jingu and toyouke daijingu  \n",
            "in comparative mythology   the myth of the birth of amaterasu omikami is nearly identical to that of pangu   as recorded earlier   somewhere during the third or fourth century    \n",
            "because the story of turning the eyes into the sun and the moon is a common analogy found across asia   some consider that the myth was propagated as peoples migrated around the region during prehistory  \n",
            "some scholars point out that in   nihon shoki     chronicles of japan     the divinity of amaterasu omikami   was changed in three stages  \n",
            "the following describes the historical events and their details that triggered the changes  \n",
            "0   the taika era reforms took place  \n",
            "it was after these reforms that the imperial family began to worship amaterasu omikami  \n",
            "hi no kami   the god of the sun   or the sun itself  \n",
            "a male god called   amateru   was widely worshipped  \n",
            "  engishiki   list of official shrines     lists the shrines that contained within their names the word   amateru   — like the amateru jinja shrine of tsushima   the iiboniamateru jinja shrine of harima and the amateru mitama no mikoto jinja shrine of tanba  \n",
            "in fact   amateru   as the name of the male god   is the alias name of amaterasu omikami  s grandson amenohoakari   who is the first ancestor of the owari   tsumori and kaifu clans  \n",
            "additionally   amenohoakari  s younger brother was ninigi   the great grandfather of emperor jimmu  \n",
            "0   the jinsin war broke out  \n",
            "  ohirumenomuchi   a woman who worships the god of the sun     or   tanabatatsume    \n",
            "two interpretations exist   first   to consider that the god worshipped was ohirumenomuchi   a female god     and secondly   to think that the god honored was amateru   the male god in the form of a snake  \n",
            "0   kodai jingu was elected  \n",
            "  amateras     or the ancestor god of the imperial family\n",
            "a female god  \n",
            "using empress jito as a model   amaterasu omikami was changed from the god of nature into a humanized god  \n",
            "since then   the   amaterasu   portion of amaterasu omikami   if not meant to be the kososhin   the ancestor of the imperial family     has been pronounced   amateru    \n",
            "for example   in   sarashina nikki   sarashina diary       written around 0   the amaterasu omikami was cited as   天照御神   and the portion of   天照   was pronounced   amateru     while in   sendai kujihongi       天照孁   is provided with the furigana readings   アマテルヒルメノムチ       amateru hirumenomuchi      \n",
            "from 0 to 0   a fierce debate took place over the shinto doctrine concerning which gods should be enshrined in the temple of shinto jimukyoku   office   in hibiya   tokyo  \n",
            "shito jimukyoku   which had decided to enshrine in its temple four gods and zokasanshin   the three gods of creation    ameno minakanushi no kami   takamimusubi no kami and kamimusubi no kami and amaterasu omikami met with opposition from the izumo group   which stressed the idea of yuken ichinyo   the unity of this world and the world after death   and insisted on worshipping five gods   including okuni nushi no okami   the ruling god of yukai   the world after death    \n",
            "however   the ise group   which had played the central role in shito jimukyoku   argued that amaterasu omikami was the great god of heaven and earth   which ruled kenrei   this world and the world after death   and that other gods were simply her subordinates   thus creating a full scale confrontation between the two  \n",
            "this confrontation escalated to the point where various rumors circulated that the izumo group might be trying to settle a deep seated grudge against the ise group   which had accumulated since the era of gods   and that takatomi senge   who had been disloyal to the imperial family   had to be killed as punishment  \n",
            "this debate was eventually brought to an end by the decision of emperor meiji   resulting in the defeat of the izumo group   and amaterasu omikami was ranked at the highest level of all the gods  \n",
            "the government is said to have learned from this debate that it was impossible to establish a common shinto doctrine and to rule the people of the modern state directly through its restoration  \n",
            "for more information   see   debate regarding the gods to be enshrined     by shinto jimukyoku   kokka shinto   state shinto    \n",
            "in depicting amaterasu omikami in his novel   ryunosuke akutagawa used the alias   ohirumemuchi    \n",
            "this is because he wanted to depict amaterasu omikami as the god of the sun or nature   not the kososhin   imperial ancestor   associated with the name   amaterasu omikami    \n",
            "in fact   akutagawa had to make many revisions   additions and deletions to his novels due to the inspection conducted by the army  \n",
            "masahiro yasuoka   0 0     the founder of kinkei gakuin   0     said that the japanese people worshipped amaterasu omikami  \n",
            "amaterasu omikami represents the ideal of the japanese spirit to cover the world with the rays of the sun  \n",
            "the   izana   of   izanami   and   izanagi     which means   to lead     also represents an ideal of the japanese people who set an example for other peoples in the world as leaders   forerunners and originators  \n",
            "shrines that honor amaterasu omikami\n",
            "shrines that worship amaterasu omikami are called shinmei shrines and are scattered across the country   but their chief shrine is the naiku   inner shrine   of ise jingu shrine  \n",
            "kotai jingu is a shrine that houses yataka no kagami   mirror of yataka     one of the sanshu no jingi   three kinds of treasures     as the object of worship  \n",
            "nearly all the shrines in japan distribute the shinsatsu   talisman     jingu taima   shrine amulet     of kotai jingu shrine   tensyo kotai jingu shrine    \n",
            "daimyo is an expression that came from daimyoshu  \n",
            "it signifies a person whose name is very famous  \n",
            "as opposed to a shomyo  \n",
            "it originally meant a person who wields influence in a region  \n",
            "later   in samurai society   it came to mean a warrior who has a lot of territory or subordinates  \n",
            "in the muromachi period the shugo strengthened their control over their assigned territories and became shugo daimyo  \n",
            "in the warring states period   taishin ryoshu   noble land owners   appeared who established even stronger control over their territories   and were called daimyobun no kokujin or sengoku daimyo  \n",
            "in the edo period the word specified a lord who was granted a territory of 0 0 koku or more from the edo shogunate  \n",
            "among warriors with less than 0 0 koku   those who belonged to the shogunate were called jikisan  \n",
            "in fact   because the feudal lords that were the daimyo shared characteristics with chinese lords   shoko     they were also called daimyo shoko  \n",
            "this section describes the details of the edo period   recent daimyo  \n",
            "for shugo daimyo or sengoku daimyo   refer to the relevant articles  \n",
            "edo period daimyo\n",
            "the rank of edo period daimyo was determined by family status   official rank   size of stipend   office   and service  \n",
            "first   depending on his connection to the tokugawa shogun family   related families   shinpan   shinpan daimyo   were classified mainly as fudai daimyo   hereditary daimyo   that were vassals of the tokugawa family before the battle of sekigahara   and tozama daimyo   non hereditary daimyo   that became vassals around the time of the battle of sekigahara  \n",
            "the first shogun   ieyasu   in order to maintain the bloodline in the event that the shogun household died out   for supervision and control of the daimyo nationwide   and for support of the shogunate   established three branch households of the tokugawa family that were allowed to use the tokugawa name   and sent the ninth son   yoshinao tokugawa   to the owari domain   the 0th son yorinobu tokugawa to the kishu domain   and the 0th son yorifusa tokugawa to the mito domain  \n",
            "beginning with the elder brother of the second shogun   hidetada tokugawa   and second son of ieyasu   hideyasu yuki   was sent to the echizen domain   he placed tokugawa family members as daimyo all of the country  \n",
            "moreover   over the generations the hereditary vassals that supported the beginning of the tokugawa shogun family were placed as fudai daimyo   who secured the shogunate  s military power while serving as important officials from the shogunate  s chief minister to members of the council of elders   supported the shogunate  s government  \n",
            "the fudai daimyo had relatively small stipends   and aside from the prominent fudai hitto in the ii clan who received an unusual 0 0 koku in the hikone domain   the torii   sakakibara   honda and ogasawara clans received relatively large stipends   but throughout the edo period only a few fudai daimyo maintained more than 0 0 koku   starting with the sakai   abe   hotta   yanagisawa   and toda clans  \n",
            "this was for the separation of influence and military power  \n",
            "the tozama daimyo were the daimyo that joined after the battle of sekigahara   and many had opposed the tokugawa at sekigahara  \n",
            "the shogunate was very careful about that   actively carried out intelligence gathering activities using spies   and when they feared impropriety or insurrection   did not hesitate to revoke rank  \n",
            "representative of the tozama daimyo were the kaga domain of the maeda clan   famous for the million koku of kaga   the satsuma domain of the shimazu clan   a family famous from the kamakura period   the sendai domain of the date clan   the fukuoka domain of the kuroda clan   the hiroshima domain of the asano clan   the choshu domain of the mori clan   the yonezawa domain of the uesugi clan   the saga domain of the nabeshima clan   the kumamoto domain of the hosokawa clan   the okayama and tottori domains of the ikeda clan   the tokushima domain of the hachisuka clan   the tosa domain of the tosa yamauchi clan   and the akita domain of the satake clan  \n",
            "the status of a daimyo was defined as kokushu if their territory was one province or more   or an equivalent number of koku   as joshu daimyo   joshu class   if they had a castle   those without a castle were distinguished as jinya   and the rooms that they occupied at edo castle during their service there were differentiated based on the daimyo  s status  \n",
            "refer to shikoseki for details  \n",
            "aside from daimyo in charge of provinces   those with 0 0 koku were generally not allowed possession of a whole district   and usually held territory around the castle and in patches  \n",
            "in extreme cases   rule of a single village was divided between two land holders   aikyu    \n",
            "daimyo   as a rule   had 0 0 koku or more   but the kitsuregawa clan  s kitsuregawa domain was 0 0 koku  \n",
            "that was because the kitsuregawa clan were descendants of the ashikaga clan  \n",
            "daimyo were subject to control by the shogunate through the laws for the military houses and the sankin kotai   alternate attendance   system  \n",
            "other than that   there was a system of assignments called otetsudai   and at the end of the edo period some were ordered to defend the coast   so the daimyo were always in a difficult position financially  \n",
            "words associated with daimyo\n",
            "daimyo kazoku   among the nobility   those who had formerly been daimyo  \n",
            "daimyo yashiki   a daimyo  s mansion  \n",
            "out of several   the principle residence in edo was called kamiyashiki   and the others shimoyashiki  \n",
            "daimyozen   generous like a daimyo  \n",
            "daimyo gyoretsu   daimyo procession     the daimyo moving in a file when changing beginning a term in edo  \n",
            "also used to refer jokingly to a group moving in a cluster around an important person  \n",
            "daimyo hikeshi   edo firefighter brigades that the edo shogunate ordered each daimyo to create  \n",
            "made up of samurai from each domain  \n",
            "daimyogashi   when a major merchant lent money to a daimyo at high interest with rice as collateral  \n",
            "daimyoazuke   when the shogunate entrusted management of criminals to a daimyo  \n",
            "daimyo bushin   extravagant construction  \n",
            "daimyo ryoko   extravagant travel  \n",
            "daimyo hikyaku   a messenger that a daimyo employed to communicate between edo and the province  \n",
            "daimyo oroshi   cutting a fish into three slices so that a lot of meat stays around the spine  \n",
            "comes from the fact that the practice is extravagant  \n",
            "daimyogiri   cutting meat or fish into rough chunks  \n",
            "daimyogai   purchasing something just as the seller asks for  \n",
            "daimyo wan   a big bowl  \n",
            "daimyo kendon   refers to a kendon that was served in a container that has a lacquer painting of a daimyo  s family crest or boat  \n",
            "daimyojima   a design of thin   vertical pinstripes  \n",
            "the muromachi period dictionary   setsuyoshu   listed the two pronunciations   taimei   and   daimyo     the former meaning a shugo   a major feudal lord     and the latter meaning a wealthy person   the wealthy class    \n",
            "in the warring states period   the distinction by pronunciation weakened   and it is thought that   taimei   was common  \n",
            "the nippo jisho   japanese portuguese dictionary   from the beginning of the 0th century also listed the two pronunciations     daimyo   and   taimei     but there was no clear distinction in meaning   and both were used for major feudal lords  \n",
            "the pronunciation settled on   daimyo   after the beginning of the edo period   and by the kansei era they were solely called   daimyo    \n",
            "shugo daimyo   sengoku daimyo\n",
            "shinpan   fudai daimyo   tozama daimyo\n",
            "domains\n",
            "lords\n"
          ]
        }
      ],
      "source": [
        "!cat ./tokenized_data/test.en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b9NaMjQ5kMG"
      },
      "source": [
        "# 93"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bQN4brFX4iRh",
        "outputId": "f1456dd3-6b33-4a40-ca5f-14b85d7c7b6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(ignore_case=False, order=4, ref='./tokenized_data/test.en', sacrebleu=False, sentence_bleu=False, sys='92.out')\n",
            "BLEU4 = 2.21, 22.5/5.0/1.5/0.5 (BP=0.733, ratio=0.763, syslen=17326, reflen=22710)\n"
          ]
        }
      ],
      "source": [
        "!fairseq-score \\\n",
        "--sys 92.out \\\n",
        "--ref ./tokenized_data/test.en"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVcLFmeI54o0"
      },
      "source": [
        "# 94"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X0_nOtZn5iKx",
        "outputId": "7f61de6e-1d76-458f-d7c9-397430386111"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-07-19 10:34:11 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'result/train/checkpoint_last.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 1, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'result/preprocessing', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-07-19 10:34:11 | INFO | fairseq.tasks.translation | [ja] dictionary: 62704 types\n",
            "2022-07-19 10:34:11 | INFO | fairseq.tasks.translation | [en] dictionary: 59624 types\n",
            "2022-07-19 10:34:11 | INFO | fairseq_cli.interactive | loading model(s) from result/train/checkpoint_last.pt\n",
            "2022-07-19 10:34:18 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-07-19 10:34:18 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-07-19 10:37:02 | INFO | fairseq_cli.interactive | Total time: 170.809 seconds; translation time: 158.292\n",
            "2022-07-19 10:37:07 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'result/train/checkpoint_last.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 2, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'result/preprocessing', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-07-19 10:37:07 | INFO | fairseq.tasks.translation | [ja] dictionary: 62704 types\n",
            "2022-07-19 10:37:07 | INFO | fairseq.tasks.translation | [en] dictionary: 59624 types\n",
            "2022-07-19 10:37:07 | INFO | fairseq_cli.interactive | loading model(s) from result/train/checkpoint_last.pt\n",
            "2022-07-19 10:37:14 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-07-19 10:37:14 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-07-19 10:40:12 | INFO | fairseq_cli.interactive | Total time: 185.141 seconds; translation time: 172.589\n",
            "2022-07-19 10:40:17 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'result/train/checkpoint_last.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 3, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'result/preprocessing', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-07-19 10:40:17 | INFO | fairseq.tasks.translation | [ja] dictionary: 62704 types\n",
            "2022-07-19 10:40:17 | INFO | fairseq.tasks.translation | [en] dictionary: 59624 types\n",
            "2022-07-19 10:40:17 | INFO | fairseq_cli.interactive | loading model(s) from result/train/checkpoint_last.pt\n",
            "2022-07-19 10:40:24 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-07-19 10:40:24 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-07-19 10:43:22 | INFO | fairseq_cli.interactive | Total time: 185.241 seconds; translation time: 172.914\n",
            "2022-07-19 10:43:27 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'result/train/checkpoint_last.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 4, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'result/preprocessing', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-07-19 10:43:27 | INFO | fairseq.tasks.translation | [ja] dictionary: 62704 types\n",
            "2022-07-19 10:43:27 | INFO | fairseq.tasks.translation | [en] dictionary: 59624 types\n",
            "2022-07-19 10:43:27 | INFO | fairseq_cli.interactive | loading model(s) from result/train/checkpoint_last.pt\n",
            "2022-07-19 10:43:34 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-07-19 10:43:34 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-07-19 10:46:45 | INFO | fairseq_cli.interactive | Total time: 197.523 seconds; translation time: 185.122\n",
            "2022-07-19 10:46:50 | INFO | fairseq_cli.interactive | {'_name': None, 'common': {'_name': None, 'no_progress_bar': False, 'log_interval': 100, 'log_format': None, 'log_file': None, 'aim_repo': None, 'aim_run_hash': None, 'tensorboard_logdir': None, 'wandb_project': None, 'azureml_logging': False, 'seed': 1, 'cpu': False, 'tpu': False, 'bf16': False, 'memory_efficient_bf16': False, 'fp16': False, 'memory_efficient_fp16': False, 'fp16_no_flatten_grads': False, 'fp16_init_scale': 128, 'fp16_scale_window': None, 'fp16_scale_tolerance': 0.0, 'on_cpu_convert_precision': False, 'min_loss_scale': 0.0001, 'threshold_loss_scale': None, 'amp': False, 'amp_batch_retries': 2, 'amp_init_scale': 128, 'amp_scale_window': None, 'user_dir': None, 'empty_cache_freq': 0, 'all_gather_list_size': 16384, 'model_parallel_size': 1, 'quantization_config_path': None, 'profile': False, 'reset_logging': False, 'suppress_crashes': False, 'use_plasma_view': False, 'plasma_path': '/tmp/plasma'}, 'common_eval': {'_name': None, 'path': 'result/train/checkpoint_last.pt', 'post_process': None, 'quiet': False, 'model_overrides': '{}', 'results_path': None}, 'distributed_training': {'_name': None, 'distributed_world_size': 1, 'distributed_num_procs': 1, 'distributed_rank': 0, 'distributed_backend': 'nccl', 'distributed_init_method': None, 'distributed_port': -1, 'device_id': 0, 'distributed_no_spawn': False, 'ddp_backend': 'pytorch_ddp', 'ddp_comm_hook': 'none', 'bucket_cap_mb': 25, 'fix_batches_to_gpus': False, 'find_unused_parameters': False, 'gradient_as_bucket_view': False, 'fast_stat_sync': False, 'heartbeat_timeout': -1, 'broadcast_buffers': False, 'slowmo_momentum': None, 'slowmo_base_algorithm': 'localsgd', 'localsgd_frequency': 3, 'nprocs_per_node': 1, 'pipeline_model_parallel': False, 'pipeline_balance': None, 'pipeline_devices': None, 'pipeline_chunks': 0, 'pipeline_encoder_balance': None, 'pipeline_encoder_devices': None, 'pipeline_decoder_balance': None, 'pipeline_decoder_devices': None, 'pipeline_checkpoint': 'never', 'zero_sharding': 'none', 'fp16': False, 'memory_efficient_fp16': False, 'tpu': False, 'no_reshard_after_forward': False, 'fp32_reduce_scatter': False, 'cpu_offload': False, 'use_sharded_state': False, 'not_fsdp_flatten_parameters': False}, 'dataset': {'_name': None, 'num_workers': 1, 'skip_invalid_size_inputs_valid_test': False, 'max_tokens': None, 'batch_size': 1, 'required_batch_size_multiple': 8, 'required_seq_len_multiple': 1, 'dataset_impl': None, 'data_buffer_size': 10, 'train_subset': 'train', 'valid_subset': 'valid', 'combine_valid_subsets': None, 'ignore_unused_valid_subsets': False, 'validate_interval': 1, 'validate_interval_updates': 0, 'validate_after_updates': 0, 'fixed_validation_seed': None, 'disable_validation': False, 'max_tokens_valid': None, 'batch_size_valid': None, 'max_valid_steps': None, 'curriculum': 0, 'gen_subset': 'test', 'num_shards': 1, 'shard_id': 0, 'grouped_shuffling': False, 'update_epoch_batch_itr': False, 'update_ordered_indices_seed': False}, 'optimization': {'_name': None, 'max_epoch': 0, 'max_update': 0, 'stop_time_hours': 0.0, 'clip_norm': 0.0, 'sentence_avg': False, 'update_freq': [1], 'lr': [0.25], 'stop_min_lr': -1.0, 'use_bmuf': False, 'skip_remainder_batch': False}, 'checkpoint': {'_name': None, 'save_dir': 'checkpoints', 'restore_file': 'checkpoint_last.pt', 'continue_once': None, 'finetune_from_model': None, 'reset_dataloader': False, 'reset_lr_scheduler': False, 'reset_meters': False, 'reset_optimizer': False, 'optimizer_overrides': '{}', 'save_interval': 1, 'save_interval_updates': 0, 'keep_interval_updates': -1, 'keep_interval_updates_pattern': -1, 'keep_last_epochs': -1, 'keep_best_checkpoints': -1, 'no_save': False, 'no_epoch_checkpoints': False, 'no_last_checkpoints': False, 'no_save_optimizer_state': False, 'best_checkpoint_metric': 'loss', 'maximize_best_checkpoint_metric': False, 'patience': -1, 'checkpoint_suffix': '', 'checkpoint_shard_count': 1, 'load_checkpoint_on_all_dp_ranks': False, 'write_checkpoints_asynchronously': False, 'model_parallel_size': 1}, 'bmuf': {'_name': None, 'block_lr': 1.0, 'block_momentum': 0.875, 'global_sync_iter': 50, 'warmup_iterations': 500, 'use_nbm': False, 'average_sync': False, 'distributed_world_size': 1}, 'generation': {'_name': None, 'beam': 5, 'nbest': 1, 'max_len_a': 0.0, 'max_len_b': 200, 'min_len': 1, 'match_source_len': False, 'unnormalized': False, 'no_early_stop': False, 'no_beamable_mm': False, 'lenpen': 1.0, 'unkpen': 0.0, 'replace_unk': None, 'sacrebleu': False, 'score_reference': False, 'prefix_size': 0, 'no_repeat_ngram_size': 0, 'sampling': False, 'sampling_topk': -1, 'sampling_topp': -1.0, 'constraints': None, 'temperature': 1.0, 'diverse_beam_groups': -1, 'diverse_beam_strength': 0.5, 'diversity_rate': -1.0, 'print_alignment': None, 'print_step': False, 'lm_path': None, 'lm_weight': 0.0, 'iter_decode_eos_penalty': 0.0, 'iter_decode_max_iter': 10, 'iter_decode_force_max_iter': False, 'iter_decode_with_beam': 1, 'iter_decode_with_external_reranker': False, 'retain_iter_history': False, 'retain_dropout': False, 'retain_dropout_modules': None, 'decoding_format': None, 'no_seed_provided': False, 'eos_token': None}, 'eval_lm': {'_name': None, 'output_word_probs': False, 'output_word_stats': False, 'context_window': 0, 'softmax_batch': 9223372036854775807}, 'interactive': {'_name': None, 'buffer_size': 1, 'input': '-'}, 'model': None, 'task': {'_name': 'translation', 'data': 'result/preprocessing', 'source_lang': None, 'target_lang': None, 'load_alignments': False, 'left_pad_source': True, 'left_pad_target': False, 'max_source_positions': 1024, 'max_target_positions': 1024, 'upsample_primary': -1, 'truncate_source': False, 'num_batch_buckets': 0, 'train_subset': 'train', 'dataset_impl': None, 'required_seq_len_multiple': 1, 'eval_bleu': False, 'eval_bleu_args': '{}', 'eval_bleu_detok': 'space', 'eval_bleu_detok_args': '{}', 'eval_tokenized_bleu': False, 'eval_bleu_remove_bpe': None, 'eval_bleu_print_samples': False}, 'criterion': {'_name': 'cross_entropy', 'sentence_avg': True}, 'optimizer': None, 'lr_scheduler': {'_name': 'fixed', 'force_anneal': None, 'lr_shrink': 0.1, 'warmup_updates': 0, 'lr': [0.25]}, 'scoring': {'_name': 'bleu', 'pad': 1, 'eos': 2, 'unk': 3}, 'bpe': None, 'tokenizer': None, 'ema': {'_name': None, 'store_ema': False, 'ema_decay': 0.9999, 'ema_start_update': 0, 'ema_seed_model': None, 'ema_update_freq': 1, 'ema_fp32': False}, 'simul_type': None}\n",
            "2022-07-19 10:46:50 | INFO | fairseq.tasks.translation | [ja] dictionary: 62704 types\n",
            "2022-07-19 10:46:50 | INFO | fairseq.tasks.translation | [en] dictionary: 59624 types\n",
            "2022-07-19 10:46:50 | INFO | fairseq_cli.interactive | loading model(s) from result/train/checkpoint_last.pt\n",
            "2022-07-19 10:46:56 | INFO | fairseq_cli.interactive | NOTE: hypothesis and token scores are output in base 2\n",
            "2022-07-19 10:46:56 | INFO | fairseq_cli.interactive | Type the input sentence and press return:\n",
            "2022-07-19 10:50:08 | INFO | fairseq_cli.interactive | Total time: 198.800 seconds; translation time: 186.291\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "for N in `seq 1 5`\n",
        "do\n",
        "    fairseq-interactive --path result/train/checkpoint_last.pt --beam $N result/preprocessing < ./tokenized_data/test.ja | grep '^H' | cut -f3 > 94_$N.out\n",
        "done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uRt-ywrjt42L"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "for N in `seq 1 5`\n",
        "do\n",
        "    fairseq-score --sys 94_$N.out --ref ./tokenized_data/test.en > 94_$N.score\n",
        "done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7q6zaqwBEHP",
        "outputId": "58f6f106-4c11-45f2-aa11-415ff3c45ac2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Namespace(ignore_case=False, order=4, ref='./tokenized_data/test.en', sacrebleu=False, sentence_bleu=False, sys='94_2.out')\n",
            "BLEU4 = 2.17, 22.8/5.0/1.4/0.5 (BP=0.749, ratio=0.775, syslen=17609, reflen=22710)\n"
          ]
        }
      ],
      "source": [
        "!cat 94_2.score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CfHG5Jgn1ZrF"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "\n",
        "def read_score(filename):\n",
        "    with open(filename) as f:\n",
        "        x = f.readlines()[1]\n",
        "        x = re.search(r'(?<=BLEU4 = )\\d*\\.\\d*(?=,)', x)\n",
        "        return float(x.group())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "rtI4yW6zZHZ-",
        "outputId": "e13c2e91-2d89-4e27-aa59-8be1d6b9e7b1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import re\n",
        "\n",
        "\n",
        "# def read_score(filename):\n",
        "#     with open(filename) as f:\n",
        "#         x = f.readlines()[1]\n",
        "#         x = re.search(r'(?<=BLEU4 = )\\d*\\.\\d*(?=,)', x)\n",
        "#         return float(x.group())\n",
        "\n",
        "\n",
        "xs = range(1, 6)\n",
        "ys = [read_score(f'94_{x}.score') for x in xs]\n",
        "plt.plot(xs, ys)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "nlp100knock_chapter10.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "e63aa626c3c030918726870aae77b3d3193589f868c9f7283f19d8a6631c5547"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
