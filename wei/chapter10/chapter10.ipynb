{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOMLRwj0dcti"
   },
   "source": [
    "本章では，日本語と英語の翻訳コーパスである京都フリー翻訳タスク (KFTT)を用い，ニューラル機械翻訳モデルを構築する．ニューラル機械翻訳モデルの構築には，fairseq，Hugging Face Transformers，OpenNMT-pyなどの既存のツールを活用せよ．\n",
    "## knock90 データの準備\n",
    "機械翻訳のデータセットをダウンロードせよ．訓練データ，開発データ，評価データを整形し，必要に応じてトークン化などの前処理を行うこと．ただし，この段階ではトークンの単位として形態素（日本語）および単語（英語）を採用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNpI5tPrcInj"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CgCs_jOHKqCr"
   },
   "outputs": [],
   "source": [
    "# download KFTT data and unzip\n",
    "!wget http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\n",
    "!tar -zxvf kftt-data-1.0.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "huuMC6PGe789"
   },
   "outputs": [],
   "source": [
    "# install mecab \n",
    "!apt install mecab libmecab-dev mecab-ipadic-utf8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dkKKl1v4gSFV"
   },
   "outputs": [],
   "source": [
    "# install CRF++(実行に必要)\n",
    "FILE_ID = '0B4y35FiV1wh7QVR6VXJ5dWExSTQ'\n",
    "FILE_NAME = 'crfpp.tar.gz'\n",
    "!wget 'https://docs.google.com/uc?export=download&id=$FILE_ID' -O $FILE_NAME\n",
    "!tar xvf crfpp.tar.gz\n",
    "%cd CRF++-0.58\n",
    "!./configure && make && make install && ldconfig\n",
    "!pwd\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRIaTztYe3v-"
   },
   "outputs": [],
   "source": [
    "# use macab to tokenize japanese data\n",
    "!cat kftt-data-1.0/data/orig/kyoto-train.ja | mecab > train.mecab.ja\n",
    "!cat kftt-data-1.0/data/orig/kyoto-dev.ja | mecab > dev.mecab.ja\n",
    "!cat kftt-data-1.0/data/orig/kyoto-test.ja | mecab > test.mecab.ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5vYUcbl9ovLH"
   },
   "outputs": [],
   "source": [
    "with open('/content/kftt-data-1.0/data/tok/kyoto-train.cln.en') as f:\n",
    "  data = f.readlines()\n",
    "  for line in data[:9]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgqcrPOPg8pY"
   },
   "outputs": [],
   "source": [
    "for src, dst in [\n",
    "    ('train.mecab.ja', 'train.spacy.ja'),\n",
    "    ('dev.mecab.ja', 'dev.spacy.ja'),\n",
    "    ('test.mecab.ja', 'test.spacy.ja'),\n",
    "]:\n",
    "    with open(src) as f:\n",
    "        lst = []\n",
    "        tmp = []\n",
    "        for x in f:\n",
    "            x = x.strip()\n",
    "            if x == 'EOS':\n",
    "                lst.append(' '.join(tmp))\n",
    "                tmp = []\n",
    "            elif x != '':\n",
    "                tmp.append(x.split('\\t')[0])\n",
    "    with open(dst, 'w') as f:\n",
    "        for line in lst:\n",
    "            print(line, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0o0Kr8Ymg0sk"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "# tokenize data-en\n",
    "\n",
    "for src, dst in [\n",
    "    ('kftt-data-1.0/data/orig/kyoto-train.en', 'train.spacy.en'),\n",
    "    ('kftt-data-1.0/data/orig/kyoto-dev.en', 'dev.spacy.en'),\n",
    "    ('kftt-data-1.0/data/orig/kyoto-test.en', 'test.spacy.en'),   \n",
    "]:\n",
    "  with open(src) as f, open(dst, 'w') as g:\n",
    "    for x in f:\n",
    "      x=x.strip()\n",
    "      x = x.replace(\"(\", \"( \")\n",
    "      x = x.replace(\")\", \" )\")\n",
    "      x = x.replace(\",\", \" ,\")\n",
    "      x = x.replace(\".\", \" .\")\n",
    "      x = x.replace(\"\\\"\", \" \\\" \")\n",
    "      x = re.sub(r'\\s+', ' ', x)\n",
    "      print(x, file=g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wEnhiP2soCZ"
   },
   "outputs": [],
   "source": [
    "with open('train.spacy.ja') as f:\n",
    "  data = f.readlines()\n",
    "  for line in data[:9]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CWVqDP5On1nM"
   },
   "outputs": [],
   "source": [
    "!pip install fairseq\n",
    "# doc: https://github.com/facebookresearch/fairseq/tree/main/examples/translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bajqa5NxeF65"
   },
   "source": [
    "## knock91 機械翻訳モデルの訓練\n",
    "90で準備したデータを用いて，ニューラル機械翻訳のモデルを学習せよ（ニューラルネットワークのモデルはTransformerやLSTMなど適当に選んでよい）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EGSEnNyMe27H"
   },
   "outputs": [],
   "source": [
    "# preprocess/binarize the data\n",
    "# src:ja trg:en; preprocessed data is saved to data91\n",
    "!fairseq-preprocess -s ja -t en \\\n",
    "    --trainpref train.spacy \\\n",
    "    --validpref dev.spacy \\\n",
    "    --destdir data91  \\\n",
    "    --thresholdsrc 5 \\\n",
    "    --thresholdtgt 5 \\\n",
    "    --workers 20\n",
    "# runing time:3min28s\n",
    "# tokenizer=None,bpe=None,cpu=False, criterion='cross_entropy',lr_scheduler='fixed',min_loss_scale=0.0001,optimizer=None,scoring='bleu', seed=1\n",
    "# [ja] Dictionary: 49320 types\n",
    "# [ja] train.spacy.ja: 440288 sents, 11412336 tokens, 1.0% replaced (by <unk>)\n",
    "# [ja] dev.spacy.ja: 1166 sents, 26014 tokens, 1.04% replaced (by <unk>)\n",
    "# [en] Dictionary: 61944 types\n",
    "# [en] train.spacy.en: 440288 sents, 11763358 tokens, 2.68% replaced (by <unk>)\n",
    "# [en] dev.spacy.en: 1166 sents, 25042 tokens, 3.95% replaced (by <unk>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QInpmNjsxR1l"
   },
   "outputs": [],
   "source": [
    "# train a Transformer translation model\n",
    "# CUDA_VISIBLE_DEVICE=0 # specify which GPU to use\n",
    "# epochs=3\n",
    "!fairseq-train data91 \\\n",
    "    --fp16 \\\n",
    "    --save-dir save91 \\\n",
    "    --max-epoch 3 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --update-freq 1 \\\n",
    "    --dropout 0.2 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8000 > 91.log\n",
    "# epoch 001: 100% 1758/1759 [10:52<00:00,  2.65it/s, loss=7.646, nll_loss=6.351, ppl=81.61, wps=18200.4, ups=2.67, wpb=6827.2, bsz=233.9, num_updates=1700, lr=0.00085, gnorm=0.886, clip=22, loss_scale=8, train_wall=37, gb_free=5.8, wall=633]\n",
    "# epoch 002: 100% 1758/1759 [10:53<00:00,  2.71it/s, loss=6.818, nll_loss=5.399, ppl=42.21, wps=18216, ups=2.69, wpb=6772.2, bsz=242.2, num_updates=3500, lr=0.000755929, gnorm=0.583, clip=1, loss_scale=8, train_wall=37, gb_free=5.6, wall=1324]\n",
    "# epoch 003: 100% 1758/1759 [10:52<00:00,  2.76it/s, loss=6.398, nll_loss=4.913, ppl=30.12, wps=18330.8, ups=2.7, wpb=6782.8, bsz=279.4, num_updates=5200, lr=0.000620174, gnorm=0.624, clip=4, loss_scale=8, train_wall=37, gb_free=7.3, wall=1979]\n",
    "# runing time:34min10s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsKX8JdzeSpS"
   },
   "source": [
    "## knock92 機械翻訳モデルの適用\n",
    "91で学習したニューラル機械翻訳モデルを用い，与えられた（任意の）日本語の文を英語に翻訳するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zI9AxffHewZK"
   },
   "outputs": [],
   "source": [
    "!fairseq-interactive --path save91/checkpoint3.pt data91 < test.spacy.ja | grep '^H' | cut -f3 > 92.out\n",
    "# [ja] dictionary: 49320 types\n",
    "# [en] dictionary: 61944 types\n",
    "# Total time: 301.925 seconds; translation time: 289.058"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OcKkxHXu-xMp"
   },
   "outputs": [],
   "source": [
    "with open('92.out') as f:\n",
    "  data = f.readlines()\n",
    "  print(len(data))   # 1160\n",
    "  for line in data[:9]:\n",
    "    print(line)\n",
    "# <unk>\n",
    "# <unk> ( <unk> ) was a priest of the Rinzai sect in the late Kamakura period .\n",
    "# He was the founder of the Soto sect .\n",
    "# He was also known as <unk> .\n",
    "# He was the founder of the sect .\n",
    "# His posthumous Buddhist name was <unk> .\n",
    "# It is also called <unk> .\n",
    "# It is said to be the origin of the word <unk> or <unk> in Japan .\n",
    "# It is said that he was a disciple of <unk> ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Op0-95NFecMY"
   },
   "source": [
    "## knock93 BLEUスコアの計測\n",
    "91で学習したニューラル機械翻訳モデルの品質を調べるため，評価データにおけるBLEUスコアを測定せよ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Xq5PZZeAnD7"
   },
   "outputs": [],
   "source": [
    "!pip install tensorboardX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urdgkoRU9ISO"
   },
   "outputs": [],
   "source": [
    "!fairseq-score --sys 92.out --ref test.spacy.en > 93.bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onI5euQgBIlE"
   },
   "outputs": [],
   "source": [
    "with open('93.bleu') as f:\n",
    "  data = f.readlines()\n",
    "  print(len(data))   # 2\n",
    "  for line in data[:9]:\n",
    "    print(line)\n",
    "    \n",
    "# Namespace(ignore_case=False, order=4, ref='test.spacy.en', sacrebleu=False, sentence_bleu=False, sys='92.out')\n",
    "# BLEU4 = 5.15, 25.8/6.9/2.9/1.4 (BP=1.000, ratio=1.015, syslen=26553, reflen=26155)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FHrPHNvNekYN"
   },
   "source": [
    "## knock94 ビーム探索\n",
    "91で学習したニューラル機械翻訳モデルで翻訳文をデコードする際に，ビーム探索を導入せよ．ビーム幅を1から100くらいまで適当に変化させながら，開発セット上のBLEUスコアの変化をプロットせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQEbMwJWBiy6"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "for N in `seq 5 11`; do\n",
    "  fairseq-interactive --path save91/checkpoint_best.pt --beam $N data91 < test.spacy.ja | grep '^H' | cut -f3 > 94.$N.out\n",
    "  done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XIyaSzU2Dsx8"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "for N in `seq 5 11`; do\n",
    "  fairseq-score --sys 94.$N.out --ref test.spacy.en > 94.$N.score\n",
    "  done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-uIudumWB0kk"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def read_score(filename):\n",
    "  with open(filename) as f:\n",
    "    x = f.readlines()[1]\n",
    "    x = re.search(r'(?<=BLEU4 = )\\d*\\.\\d*(?=,)', x)\n",
    "  return float(x.group())\n",
    "\n",
    "xs = range(5, 12)\n",
    "ys = [read_score(f'94.{x}.score') for x in xs]\n",
    "plt.plot(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXzA0Xbk_f6N"
   },
   "source": [
    "## knock95 サブワード化\n",
    "トークンの単位を単語や形態素からサブワードに変更し，91-94の実験を再度実施せよ.\n",
    "## 1．訓練データを用いて、源言語と目標言語それぞれのsentencepieceモデルを構築し、サブワード化\n",
    "\n",
    "[参照記事](https://note.com/npaka/n/n90f97543ec4b)\n",
    "\n",
    "[SentecePiece](https://github.com/google/sentencepiece)\n",
    "\n",
    "言語を処理する時、テキストをまず「トークン」に分割して、それを「ベクトル表現」に変換する。\n",
    "* 形態素解析して得た「単語」は利用上に問題点ある:\n",
    "\n",
    "語彙数が膨大で、高頻度語彙のみに限定している。低頻度語彙が捨てられて未知語として扱われている。\n",
    "* SentencePieceの手順:\n",
    "\n",
    "まず、テキストを単語に分割し、各単語の頻度を求める。\n",
    "次は、低頻度単語は1語彙として扱い、より短い語彙に分割する。\n",
    "語彙数が事前に指定したサイズになるまで、分割を繰り返します。\n",
    "\n",
    "## 2．91-94の実験を再度実施\n",
    "* preprocessでバイナリデータにする\n",
    "* sentencepieceモデルを学習(教師なし学習)\n",
    "* 推論時のbeam_sizeを変えながら、BLUEスコアを計算し、可視化する "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GO5EfOHI7JWk"
   },
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJ6Er0NJEClI"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import re\n",
    "\n",
    "# 学習の実行\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    '--input=/content/kftt-data-1.0/data/orig/kyoto-train.ja --model_prefix=kyoto_ja --vocab_size=16000 --character_coverage=0.9995')\n",
    "\n",
    "# 学習済み単語分割モデルを用いて日本語をトークン化\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('kyoto_ja.model')\n",
    "\n",
    "for src, dst in [\n",
    "  ('kftt-data-1.0/data/orig/kyoto-train.ja', 'train.sub.ja'),\n",
    "  ('kftt-data-1.0/data/orig/kyoto-dev.ja', 'dev.sub.ja'),\n",
    "  ('kftt-data-1.0/data/orig/kyoto-test.ja', 'test.sub.ja')\n",
    "]:\n",
    "  with open(src, 'r') as rf, open(dst, 'w') as wf:\n",
    "    for x in rf:\n",
    "      x = x.strip()\n",
    "      x = re.sub(r'\\s+', ' ', x)\n",
    "      x = sp.encode_as_pieces(x)\n",
    "      x = ' '.join(x)\n",
    "      print(x, file=wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z8Qg-Ov3NZrr"
   },
   "outputs": [],
   "source": [
    "# 英語をトークン化\n",
    "!pip install subword-nmt\n",
    "\n",
    "!subword-nmt learn-bpe -s 16000 < kftt-data-1.0/data/orig/kyoto-train.en > kyoto_en.codes\n",
    "!subword-nmt apply-bpe -c kyoto_en.codes < kftt-data-1.0/data/orig/kyoto-train.en > train.sub.en\n",
    "!subword-nmt apply-bpe -c kyoto_en.codes < kftt-data-1.0/data/orig/kyoto-dev.en > dev.sub.en\n",
    "!subword-nmt apply-bpe -c kyoto_en.codes < kftt-data-1.0/data/orig/kyoto-test.en > test.sub.en\n",
    "\n",
    "# runing time:1min56sec  \n",
    "# vocab_size:16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V3TE1UI2LZ2O"
   },
   "outputs": [],
   "source": [
    "!fairseq-preprocess -s ja -t en\\\n",
    "    --trainpref train.sub \\\n",
    "    --validpref dev.sub \\\n",
    "    --testpref test.sub \\\n",
    "    --tokenizer space \\\n",
    "    --workers 20 \\\n",
    "    --thresholdsrc 3 \\\n",
    "    --thresholdtgt 3 \\\n",
    "    --task translation \\\n",
    "    --workers 20 \\\n",
    "    --destdir knock95_subwords_sp\n",
    "\n",
    "# Namespace(aim_repo=None, aim_run_hash=None, align_suffix=None, alignfile=None, all_gather_list_size=16384, amp=False, amp_batch_retries=2, amp_init_scale=128, amp_scale_window=None, azureml_logging=False, bf16=False, bpe=None, cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='knock95_subwords_sp', dict_only=False, empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_file=None, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, on_cpu_convert_precision=False, only_source=False, optimizer=None, padding_factor=8, plasma_path='/tmp/plasma', profile=False, quantization_config_path=None, reset_logging=False, scoring='bleu', seed=1, source_lang='ja', srcdict=None, suppress_crashes=False, target_lang='en', task='translation', tensorboard_logdir=None, testpref='test.sub', tgtdict=None, threshold_loss_scale=None, thresholdsrc=3, thresholdtgt=3, tokenizer='space', tpu=False, trainpref='train.sub', use_plasma_view=False, user_dir=None, validpref='dev.sub', wandb_project=None, workers=20)\n",
    "# [ja] Dictionary: 17048 types\n",
    "# [ja] train.sub.ja: 440288 sents, 10462018 tokens, 0.0147% replaced (by <unk>)\n",
    "# [ja] Dictionary: 17048 types\n",
    "# [ja] dev.sub.ja: 1166 sents, 24223 tokens, 0.0206% replaced (by <unk>)\n",
    "# [ja] Dictionary: 17048 types\n",
    "# [ja] test.sub.ja: 1160 sents, 26130 tokens, 0.0153% replaced (by <unk>)\n",
    "# [en] Dictionary: 18656 types\n",
    "# [en] train.sub.en: 440288 sents, 13280091 tokens, 0.022% replaced (by <unk>)\n",
    "# [en] Dictionary: 18656 types\n",
    "# [en] dev.sub.en: 1166 sents, 29011 tokens, 0.0103% replaced (by <unk>)\n",
    "# [en] Dictionary: 18656 types\n",
    "# [en] test.sub.en: 1160 sents, 31468 tokens, 0.0254% replaced (by <unk>)\n",
    "# Wrote preprocessed data to knock95_subwords_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVvMKiinMMt5"
   },
   "outputs": [],
   "source": [
    "!fairseq-train knock95_subwords_sp \\\n",
    "    --fp16 \\\n",
    "    --save-dir save95 \\\n",
    "    --max-epoch 3 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --update-freq 1 \\\n",
    "    --dropout 0.2 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8000 > 95.log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gbjNYn5tN28J"
   },
   "outputs": [],
   "source": [
    "!fairseq-interactive --path save95/checkpoint_best.pt knock95_subwords_sp < test.spacy.ja | grep '^H' | cut -f3 > 95.out\n",
    "\n",
    "# [ja] dictionary: 17048 types\n",
    "# [en] dictionary: 18656 types\n",
    "# Total time: 379.424 seconds; translation time: 367.011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeIUd9wGS19Z"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "def spacy_tokenize(src, dst):\n",
    "    with open(src) as f, open(dst, 'w') as g:\n",
    "        for x in f:\n",
    "            x = x.strip()\n",
    "            x = ' '.join([doc.text for doc in nlp(x)])\n",
    "            print(x, file=g)\n",
    "spacy_tokenize('95.out', '95.out.spacy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Bi1RA4xRo26"
   },
   "outputs": [],
   "source": [
    "!fairseq-score --sys 95.out.spacy --ref test.spacy.en\n",
    "\n",
    "# Namespace(ignore_case=False, order=4, ref='test.spacy.en', sacrebleu=False, sentence_bleu=False, sys='95.out.spacy')\n",
    "# BLEU4 = 1.70, 16.7/2.8/0.7/0.3 (BP=1.000, ratio=1.350, syslen=35302, reflen=26155)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnIQkIAo_s5O"
   },
   "source": [
    "## knock96 学習過程の可視化\n",
    "Tensorboardなどのツールを用い，ニューラル機械翻訳モデルが学習されていく過程を可視化せよ．可視化する項目としては，学習データにおける損失関数の値とBLEUスコア，開発データにおける損失関数の値とBLEUスコアなどを採用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vyPpGBzyZN-K"
   },
   "outputs": [],
   "source": [
    "!fairseq-train knock95_subwords_sp \\\n",
    "    --fp16 \\\n",
    "    --tensorboard-logdir log96 \\\n",
    "    --save-dir save96 \\\n",
    "    --max-epoch 5 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --dropout 0.2 --weight-decay 0.0001 \\\n",
    "    --update-freq 1 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8000 > 96.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvsmOlz5C4IK"
   },
   "source": [
    "# knock97 ハイパー・パラメータの調整\n",
    "\n",
    "ニューラルネットワークのモデルや，そのハイパーパラメータを変更しつつ，開発データにおけるBLEUスコアが最大となるモデルとハイパーパラメータを求めよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-RBNEG1KDFlB"
   },
   "outputs": [],
   "source": [
    "#学習\n",
    "%%bash\n",
    "# training with different dropout rate\n",
    "for N in `seq 0.1 0.2 0.5`; do\n",
    "  fairseq-train knock95_subwords_sp \\\n",
    "        --save-dir checkpoints/train.sub.dropout_$N \\\n",
    "        --arch transformer --share-decoder-input-output-embed \\\n",
    "        --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "        --lr 5e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "        --dropout $N --weight-decay 0.0001 \\\n",
    "        --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "        --max-tokens 4096 \\\n",
    "        --max-epoch 5\n",
    "  done\n",
    "\n",
    "#推論\n",
    "for N in `seq 0.1 0.2 0.5` ; do\n",
    "  fairseq-interactive knock95_subwords_sp \\\n",
    "    --path checkpoints/train.sub.dropout_$N/checkpoint_best.pt \\\n",
    "    < test.sub.ja | grep '^H' | cut -f3 | sed -r 's/(@@ )|(@@ ?$)//g' > out97/dropout_$N.out\n",
    "  done\n",
    "\n",
    "#BLEUスコアを計算\n",
    "for N in `seq 0.1 0.2 0.5` ; do\n",
    "    echo dropout=$N >> out97/score97.out \n",
    "    fairseq-score --sys out97/dropout_$N.out --ref test.spacy.en >> out97/score97.out\n",
    "  done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXR29cewDIL7"
   },
   "source": [
    "# 98.ドメイン適応\n",
    "Japanese-English Subtitle Corpus (JESC)やJParaCrawlなどの翻訳データを活用し，KFTTのテストデータの性能向上を試みよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fm57V4Uniris"
   },
   "outputs": [],
   "source": [
    "!wget http://www.kecl.ntt.co.jp/icl/lirg/jparacrawl/release/3.0/bitext/en-ja.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pnw2YRShFfq"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open('en-ja.tar.gz') as tar:\n",
    "    for f in tar.getmembers():\n",
    "        if f.name.endswith('txt'):\n",
    "            text = tar.extractfile(f).read().decode('utf-8')\n",
    "            break\n",
    "\n",
    "data = text.splitlines()\n",
    "data = [x.split('\\t') for x in data]\n",
    "data = [x for x in data if len(x) == 4]\n",
    "data = [[x[3], x[2]] for x in data]\n",
    "\n",
    "with open('jparacrawl.ja', 'w') as f, open('jparacrawl.en', 'w') as g:\n",
    "    for j, e in data:\n",
    "        print(j, file=f)\n",
    "        print(e, file=g)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68noorj1h2iY"
   },
   "outputs": [],
   "source": [
    "with open('jparacrawl.ja') as f, open('train.jparacrawl.ja', 'w') as g:\n",
    "    for x in f:\n",
    "        x = x.strip()\n",
    "        x = re.sub(r'\\s+', ' ', x)\n",
    "        x = sp.encode_as_pieces(x)\n",
    "        x = ' '.join(x)\n",
    "        print(x, file=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiNcjuT4i9RE"
   },
   "outputs": [],
   "source": [
    "# execute subword\n",
    "!subword-nmt apply-bpe -c kyoto_en.codes < jparacrawl.en > train.jparacrawl.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bb_qcK9M9HX-"
   },
   "outputs": [],
   "source": [
    "!fairseq-preprocess -s ja -t en \\\n",
    "    --trainpref train.jparacrawl \\\n",
    "    --validpref dev.sub \\\n",
    "    --destdir data98  \\\n",
    "    --workers 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUtC9xoL9mUA"
   },
   "outputs": [],
   "source": [
    "#学習\n",
    "%%bash\n",
    "# training with different dropout rate\n",
    "for N in `seq 0.001 0.0005`; do\n",
    "  fairseq-train data98 \\\n",
    "        --fp16\n",
    "        --save-dir checkpoints/train.jparacrawl.lr_$N \\\n",
    "        --arch transformer --share-decoder-input-output-embed \\\n",
    "        --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0 \\\n",
    "        --lr $N --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "        --dropout $N --weight-decay 0.0001 \\\n",
    "        --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "        --max-tokens 4096 \\\n",
    "        --max-epoch 5\n",
    "  done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UURVR6d6_Cy5"
   },
   "outputs": [],
   "source": [
    "#推論\n",
    "%%bash\n",
    "for N in `seq 0.001 0.0005` ; do\n",
    "  fairseq-interactive data98 \\\n",
    "    --path checkpoints/train.jparacrawl.lr_$N/checkpoint_best.pt \\\n",
    "    < test.sub.ja | grep '^H' | cut -f3 | sed -r 's/(@@ )|(@@ ?$)//g' > out98/lr_$N.out\n",
    "  done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Eg0m9_N6AKOn"
   },
   "outputs": [],
   "source": [
    "#BLEUスコアを計算\n",
    "for N in `seq 0.001 0.0005` ; do\n",
    "    echo lr=$N >> out98/score98.out \n",
    "    fairseq-score --sys out98/lr_$N.out --ref test.spacy.en >> out98/score98.out\n",
    "  done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwRu96qgDQCO"
   },
   "source": [
    "# 99．翻訳サーバの構築\n",
    "ユーザが翻訳したい文を入力すると，その翻訳結果がウェブブラウザ上で表示されるデモシステムを構築せよ．"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "chapter10.ipynb",
   "private_outputs": true,
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
