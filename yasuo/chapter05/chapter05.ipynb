{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "60a7dcd692c74fa3be5c8a76d54099c4",
            "2bf9e3b4208542fda9573da32a6a7f16",
            "b13f5628d906498c877246f7a6199cfb",
            "fe8c14d2c2904668ba009cc27f643126",
            "72488f0e467e405ba087639de2bfcd72",
            "61642fb735614d2b8cf5abaf939b7847",
            "b46ef3a817f5451d93a4de3424214b37",
            "1cd7dbbe784247da9246bba448d575a6",
            "5596d273df254f26b858c4633f905160",
            "8420451bd5e8437492feee5b71d35aff",
            "f6360eb75afa45dbbfc16df2c9611bc5",
            "dd85e40b80e548db9345e898b164ab7e",
            "2703df53964f449fa2c482094a7544ec",
            "28d1877945c14a4cb7d99ce4c5dd1ca7",
            "091e4aea281f47628b1a2a10aea16901",
            "59c645aeb8134339b3536552cf33d853",
            "a79d3554227146869d5ecd1f709e0b45",
            "4c28cc2dcd3944ec905aef11c1c2ad71",
            "a92297ee3e3d436cad22ca943342dc6b",
            "676d1ede892340838323c31ca81af2f6"
          ]
        },
        "id": "4hJSDVCYZOnv",
        "outputId": "b21db3e8-309a-41ba-d0c5-2073df3af947"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60a7dcd692c74fa3be5c8a76d54099c4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WPVU4JfZUaH",
        "outputId": "c6f64d0d-33b0-476e-9336-33f092451f97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (0.5.3)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<3,>=2.0->bitsandbytes)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m129.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U safetensors\n",
        "!pip install -U bitsandbytes\n",
        "!pip install -U accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "cADORMi4runW",
        "outputId": "80f7b6ac-7d74-495a-af3b-7fbfd5f5b86a"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-09cf6c650e23>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# モデルのロード\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;31m#\"elyza/ELYZA-japanese-Llama-2-7b\",\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;34m\"meta-llama/Meta-Llama-3-8B\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4378\u001b[0m         \u001b[0;31m# Prepare the full device map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4380\u001b[0;31m             \u001b[0mdevice_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_memory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_in_fp32_regex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4382\u001b[0m         \u001b[0;31m# Finalize model weight initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_get_device_map\u001b[0;34m(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m             \u001b[0mhf_quantizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdevice_map\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"disk\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdevice_map_without_lm_head\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    105\u001b[0m                     \u001b[0;34m\"Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                     \u001b[0;34m\"quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "# HuggingFaceにログイン\n",
        "os.environ[\"HF_TOKEN\"] = \n",
        "\n",
        "# オフロードフォルダの設定\n",
        "offload_folder = \"/tmp/model_offload\"\n",
        "os.makedirs(offload_folder, exist_ok=True)\n",
        "\n",
        "# 量子化の設定\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "# トークナイザーのロード - padding_side と pad_token の設定を追加\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "   #\"elyza/ELYZA-japanese-Llama-2-7b\",\n",
        "    \"meta-llama/Meta-Llama-3-8B\",\n",
        "    token=os.environ[\"HF_TOKEN\"],\n",
        "    padding_side=\"right\",  # パディングの位置を右側に設定\n",
        ")\n",
        "\n",
        "# Llama-3ではデフォルトでpad_tokenが設定されていないので追加\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# モデルのロード\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    #\"elyza/ELYZA-japanese-Llama-2-7b\",\n",
        "    \"meta-llama/Meta-Llama-3-8B\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    torch_dtype=torch.float16,\n",
        "    token=os.environ[\"HF_TOKEN\"],\n",
        "    offload_folder=offload_folder\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Pz_cD4gimkG",
        "outputId": "f9110986-43da-46fc-b984-89bc1f7955db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-----生成結果-----\n",
            " [ア，ウ，イ]\n",
            "\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-----生成結果-----\n",
            " [ウ，イ，ア]\n",
            "\n",
            "\n",
            "\n",
            "-----生成結果-----\n",
            " [ウ，イ，ア]\n"
          ]
        }
      ],
      "source": [
        "# 40\n",
        "prompt = \"\"\"\n",
        "[問題]に対する[答え]を[選択肢]の中から選んでください。\n",
        "9世紀に活躍した人物に関係するできごとについて述べた次のア～ウを年代の古い順に正しく並べよ。\n",
        "\n",
        "ア　藤原時平は，策謀を用いて菅原道真を政界から追放した。\n",
        "イ　嵯峨天皇は，藤原冬嗣らを蔵人頭に任命した。\n",
        "ウ　藤原良房は，承和の変後，藤原氏の中での北家の優位を確立した。\n",
        "[選択肢]:[ア，イ，ウ]\n",
        "[答え]:\"\"\"\n",
        "\n",
        "for i in range(3):\n",
        "  model_input = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "  input_ids = model_input[\"input_ids\"]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    result = model.generate(\n",
        "                input_ids,\n",
        "                max_new_tokens=300,\n",
        "                # eos_token_id=terminators,\n",
        "                do_sample=True,\n",
        "                temperature=0.6,\n",
        "                top_p=0.9,\n",
        "            )\n",
        "    result = result[0][input_ids.shape[-1]:]\n",
        "    output = tokenizer.decode(result, skip_special_tokens=True)\n",
        "    print(\"\\n-----生成結果-----\\n\", output.split(\"[問題]\")[0])\n",
        "\n",
        "    del input_ids\n",
        "    del model_input\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clQR3M-fi64S",
        "outputId": "9c2795c3-a636-426a-f211-cd4a1134f5c8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n",
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-----生成結果-----\n",
            " ウ→イ→ア\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "-----生成結果-----\n",
            " ウ→ア→イ\n",
            "\n",
            "\n",
            "-----生成結果-----\n",
            " ウ→ア→イ\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 41\n",
        "prompt = \"\"\"\n",
        "[問題]に対する[答え]を[選択肢]のア～ウを年代の古い順に正しく並べよ。\n",
        "[問題]:日本の近代化に関連するできごとについて述べた次のア～ウを年代の古い順に正しく並べよ。\n",
        "\n",
        "ア　府知事・県令からなる地方官会議が設置された。\n",
        "イ　廃藩置県が実施され，中央から府知事・県令が派遣される体制になった。\n",
        "ウ　すべての藩主が，天皇に領地と領民を返還した。\n",
        "\n",
        "[選択肢]:[ア，イ，ウ]\n",
        "[答え]:解答: ウ→イ→ア\n",
        "[問題]:江戸幕府の北方での対外的な緊張について述べた次の文ア～ウを年代の古い順に正しく並べよ。\n",
        "\n",
        "ア　レザノフが長崎に来航したが，幕府が冷淡な対応をしたため，ロシア船が樺太や択捉島を攻撃した。\n",
        "イ　ゴローウニンが国後島に上陸し，幕府の役人に捕らえられ抑留された。\n",
        "ウ　ラクスマンが根室に来航し，漂流民を届けるとともに通商を求めた。\n",
        "\n",
        "[選択肢]:[ア，イ，ウ]\n",
        "[答え]:ウ→ア→イ\n",
        "[問題]:中居屋重兵衛の生涯の期間におこったできごとについて述べた次のア～ウを，年代の古い順に正しく並べよ。\n",
        "\n",
        "ア　アヘン戦争がおこり，清がイギリスに敗北した。\n",
        "イ　異国船打払令が出され，外国船を撃退することが命じられた。\n",
        "ウ　桜田門外の変がおこり，大老の井伊直弼が暗殺された。\n",
        "\n",
        "[選択肢]:[ア，イ，ウ]\n",
        "[答え]:イ→ア→ウ\n",
        "[問題]:加藤高明が外務大臣として提言を行ってから、内閣総理大臣となり演説を行うまでの時期のできごとについて述べた次のア～ウを，年代の古い順に正しく並べよ。\n",
        "\n",
        "ア　朝鮮半島において，独立を求める大衆運動である三・一独立運動が展開された。\n",
        "イ　関東大震災後の混乱のなかで，朝鮮人や中国人に対する殺傷事件がおきた。\n",
        "ウ　日本政府が，袁世凱政府に対して二十一カ条の要求を突き付けた。\n",
        "\n",
        "[選択肢]:[ア，イ，ウ]\n",
        "[答え]:ウ→ア→イ\n",
        "[問題]:9世紀に活躍した人物に関係するできごとについて述べた次のア～ウを年代の古い順に正しく並べよ。\n",
        "\n",
        "ア　藤原時平は，策謀を用いて菅原道真を政界から追放した。\n",
        "イ　嵯峨天皇は，藤原冬嗣らを蔵人頭に任命した。\n",
        "ウ　藤原良房は，承和の変後，藤原氏の中での北家の優位を確立した。\n",
        "\n",
        "\n",
        "[選択肢]:[ア，イ，ウ]\n",
        "[答え]:\"\"\"\n",
        "\n",
        "for i in range(3):\n",
        "  model_input = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "  input_ids = model_input[\"input_ids\"]\n",
        "\n",
        "  with torch.no_grad():\n",
        "    result = model.generate(\n",
        "                input_ids,\n",
        "                max_new_tokens=300,\n",
        "                # eos_token_id=terminators,\n",
        "                do_sample=True,\n",
        "                temperature=0.6,\n",
        "                top_p=0.9,\n",
        "            )\n",
        "    result = result[0][input_ids.shape[-1]:]\n",
        "    output = tokenizer.decode(result, skip_special_tokens=True)\n",
        "    print(\"\\n-----生成結果-----\\n\", output.split(\"[問題]\")[0])\n",
        "\n",
        "    del input_ids\n",
        "    del model_input\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pCnlcoSkabL"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590,
          "referenced_widgets": [
            "f05db64e643a479396ed7b4cd34fb815",
            "ef3fb6484b334ae2bdae05fbd61f4051",
            "b22c4e8fdf754d20bc0c9415741672f8",
            "1c02f7d4b47240ce88e27a07d5a352b9",
            "f3856722090a4d478194c5cc91bf4d99",
            "d15b4aead25f42c68ff9a5e6381f1401",
            "35c40a16682f49d796719b7fd4dcaf97",
            "694f0ed3dd5e43deb4c9b9f9839459f8",
            "c43c49b8386d4fd0acf9131370e2e192",
            "46d4d1611f2a43dea759bed70a7896cc",
            "c079927ebc4b42ff8fba186fa0c40c8d"
          ]
        },
        "id": "vsKaPHvV0S82",
        "outputId": "95d61aca-9dbc-4ad9-d092-3fe78a159f4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "JMMLUリポジトリをクローン中...\n",
            "Cloning into 'JMMLU'...\n",
            "remote: Enumerating objects: 408, done.\u001b[K\n",
            "remote: Counting objects: 100% (134/134), done.\u001b[K\n",
            "remote: Compressing objects: 100% (128/128), done.\u001b[K\n",
            "remote: Total 408 (delta 73), reused 14 (delta 6), pack-reused 274 (from 1)\u001b[K\n",
            "Receiving objects: 100% (408/408), 1.46 MiB | 4.26 MiB/s, done.\n",
            "Resolving deltas: 100% (211/211), done.\n",
            "コンピュータセキュリティCSV読み込み中: ./JMMLU/JMMLU/computer_security.csv\n",
            "データセット読み込み完了。問題数: 98問\n",
            "カラム名: ['クライアントとサーバー間でTLS接続が正常に確立されたとする。セッションの確立には、サーバー証明書のチェックとディフィー・ヘルマン交換の実行が含まれるが、クライアントはクライアント証明書を提供していない。さらに、クライアントとサーバーは正直であり、クライアントとサーバーは鍵を漏洩せず、暗号は良好であると仮定する。TLSが防御する攻撃は次のうちどれか? \\n1. クライアントが以前に送信したバイトを再生する攻撃者。\\n2. サーバーになりすます攻撃者。', '真、真', '偽、偽', '真、偽', '偽、真', 'A']\n",
            "\n",
            "最初の問題のデータ:\n",
            "列0: MITのケルベロスKDCサーバでは、(ほとんどのユーザプリンシパルに対して)チケットの最大存続時間は24時間である。期限切れのケルベロスチケットが使用できなくなることを保証するものは何か?\n",
            "列1: ケルベロスサーバ(KDC)は、期限切れのチケットに対して、クライアントとサーバ間の新しい接続の確立を拒否する。\n",
            "列2: クライアントがサーバに接続すると、サーバは接続を終了するために24時間のタイマーを設定し、これにより、チケットの最大有効期間を超えてクライアントが接続を維持できなくなる。\n",
            "列3: クライアントがサーバに接続すると、サーバはチケットの有効期限をサーバの現在のクロックと比較し、チケットの有効期限が過ぎている場合はユーザの認証を拒否する。\n",
            "列4: クライアントがサーバに接続すると、サーバはKDCにクエリを送信して、チケットがKDCのクロックに対してまだ有効であるかどうかをチェックし、KDCがチケットの期限切れを報告した場合は、ユーザの認証を拒否する。\n",
            "列5: C\n",
            "\n",
            "評価開始 - 全99問のコンピュータセキュリティ問題\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f05db64e643a479396ed7b4cd34fb815",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/98 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "進捗: 25/99問完了、現在の正解率: 1/25 (0.0400)\n",
            "進捗: 50/99問完了、現在の正解率: 4/50 (0.0800)\n",
            "進捗: 75/99問完了、現在の正解率: 7/75 (0.0933)\n",
            "\n",
            "評価完了！\n",
            "モデル: elyza/ELYZA-japanese-Llama-2-7b\n",
            "データセット: JMMLU computer_security\n",
            "正解数: 7/98問\n",
            "正解率: 0.0714 (7.14%)\n"
          ]
        }
      ],
      "source": [
        "# 42\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "# JMMLUリポジトリのクローン（まだ存在しない場合）\n",
        "if not os.path.exists(\"./JMMLU\"):\n",
        "    print(\"JMMLUリポジトリをクローン中...\")\n",
        "    !git clone https://github.com/nlp-waseda/JMMLU.git\n",
        "\n",
        "def generate_prompt(question, choices):\n",
        "    \"\"\"プロンプトを生成する関数\"\"\"\n",
        "    prompt = f\"\"\"以下の問題に対して、選択肢A、B、C、Dから最も適切な答えを1つ選んでください。\n",
        "選択肢を選ぶ理由を簡潔に説明し、最後に「答え: 」の後に選んだ選択肢（A、B、C、Dのいずれか）のみを記入してください。\n",
        "\n",
        "問題: {question}\n",
        "\n",
        "選択肢:\n",
        "A. {choices[0]}\n",
        "B. {choices[1]}\n",
        "C. {choices[2]}\n",
        "D. {choices[3]}\n",
        "\n",
        "\"\"\"\n",
        "    return prompt\n",
        "\n",
        "def extract_answer(text):\n",
        "    \"\"\"モデルの出力から回答を抽出する関数\"\"\"\n",
        "    # パターン1: 「答え: X」という形式\n",
        "    pattern1 = r\"答え\\s*[:：]\\s*([ABCD])\"\n",
        "    match1 = re.search(pattern1, text)\n",
        "    if match1:\n",
        "        return match1.group(1)\n",
        "\n",
        "    # パターン2: 最後の行に単独で「A」「B」「C」「D」のいずれかがある\n",
        "    pattern2 = r\"([ABCD])\\s*$\"\n",
        "    match2 = re.search(pattern2, text)\n",
        "    if match2:\n",
        "        return match2.group(1)\n",
        "\n",
        "    # パターン3: 文中に「選択肢はX」「Xが正解」などの表現がある\n",
        "    pattern3 = r\"選択肢[はがのはABCD]*\\s*([ABCD])\"\n",
        "    match3 = re.search(pattern3, text)\n",
        "    if match3:\n",
        "        return match3.group(1)\n",
        "\n",
        "    # テキスト全体からA,B,C,Dのみを探す\n",
        "    choices = re.findall(r'\\b[ABCD]\\b', text)\n",
        "    if choices:\n",
        "        # 最も頻繁に出現する選択肢を回答とする\n",
        "        return max(set(choices), key=choices.count)\n",
        "\n",
        "    return None\n",
        "\n",
        "def evaluate_jmmlu_computer_security(model, tokenizer):\n",
        "    \"\"\"JMMLUのコンピュータセキュリティデータセットを評価する関数\"\"\"\n",
        "    # CSVファイルのパス\n",
        "    csv_path = \"./JMMLU/JMMLU/computer_security.csv\"\n",
        "\n",
        "    # CSVファイル読み込み\n",
        "    print(f\"コンピュータセキュリティCSV読み込み中: {csv_path}\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "    print(f\"データセット読み込み完了。問題数: {len(df)}問\")\n",
        "\n",
        "    # カラム名の確認\n",
        "    print(f\"カラム名: {df.columns.tolist()}\")\n",
        "\n",
        "    # データ形式の確認（最初の行）\n",
        "    first_row = df.iloc[0].tolist()\n",
        "    print(\"\\n最初の問題のデータ:\")\n",
        "    for i, value in enumerate(first_row):\n",
        "        print(f\"列{i}: {value}\")\n",
        "\n",
        "    # 評価用データの準備\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    # 進捗表示用\n",
        "    print(\"\\n評価開始 - 全99問のコンピュータセキュリティ問題\")\n",
        "\n",
        "    # 各問題に対して評価\n",
        "    for i in tqdm(range(len(df)), desc=\"Evaluating\"):\n",
        "        row = df.iloc[i].tolist()\n",
        "\n",
        "        # 行の形式に応じて問題と選択肢を抽出\n",
        "        question = row[0]  # 最初の列を問題文とする\n",
        "        choices = row[1:5]  # 2-5列目を選択肢とする\n",
        "        correct_answer = row[5]  # 6列目を正解とする\n",
        "\n",
        "        # プロンプト生成\n",
        "        prompt = generate_prompt(question, choices)\n",
        "\n",
        "        # トークナイズとモデル入力の準備\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "        # 生成\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=256,\n",
        "                temperature=0.1,\n",
        "                do_sample=True,\n",
        "                top_p=0.95,\n",
        "                top_k=50,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # 出力のデコード\n",
        "        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # プロンプト部分を削除して回答のみを取得\n",
        "        predicted_text = generated_text[len(prompt):]\n",
        "\n",
        "        # 回答の抽出\n",
        "        predicted_answer = extract_answer(predicted_text)\n",
        "\n",
        "        # 正解判定\n",
        "        is_correct = (predicted_answer == correct_answer)\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "        # 25問ごとに進捗状況を表示\n",
        "        if (i + 1) % 25 == 0:\n",
        "            print(f\"進捗: {i+1}/99問完了、現在の正解率: {correct}/{total} ({correct/total:.4f})\")\n",
        "\n",
        "    # 最終結果の表示\n",
        "    accuracy = correct / total\n",
        "    print(f\"\\n評価完了！\")\n",
        "    print(f\"モデル: elyza/ELYZA-japanese-Llama-2-7b\")\n",
        "    print(f\"データセット: JMMLU computer_security\")\n",
        "    print(f\"正解数: {correct}/{total}問\")\n",
        "    print(f\"正解率: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "    return correct, total, accuracy\n",
        "\n",
        "# メイン実行部分\n",
        "# モデルとトークナイザーはすでに読み込まれていると仮定\n",
        "# model = すでに読み込まれているモデル\n",
        "# tokenizer = すでに読み込まれているトークナイザー\n",
        "\n",
        "# 評価の実行\n",
        "correct, total, accuracy = evaluate_jmmlu_computer_security(model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486,
          "referenced_widgets": [
            "1509331f30a5441c97f2f0949b00b69d",
            "2e45f1f65e0a44c4bdb599bf7c0bfe70",
            "fcd8db0b5a714073b05d2e6488be6047",
            "8a46286cf8ad474db2aa3a00387c158e",
            "b38cea5568fc487696528451037bcb1f",
            "f3a11f102250419293cd76afd15d4a7d",
            "e25d4a9829394673858a0ff5173f2525",
            "efa71607b0ea403d87c5318aa648b901",
            "4200b28b6c2a4edd89597fc98e5e8a42",
            "25a0dff36883474d844156af47bda7d2",
            "cb1ae772e0f44cb4a94b2814f8689735"
          ]
        },
        "id": "Qc-Z7-M2r43m",
        "outputId": "4a8b6cf4-5cfa-406c-c345-692302f83ecb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "コンピュータセキュリティCSV読み込み中: ./JMMLU/JMMLU/computer_security.csv\n",
            "データセット読み込み完了。問題数: 98問\n",
            "カラム名: ['クライアントとサーバー間でTLS接続が正常に確立されたとする。セッションの確立には、サーバー証明書のチェックとディフィー・ヘルマン交換の実行が含まれるが、クライアントはクライアント証明書を提供していない。さらに、クライアントとサーバーは正直であり、クライアントとサーバーは鍵を漏洩せず、暗号は良好であると仮定する。TLSが防御する攻撃は次のうちどれか? \\n1. クライアントが以前に送信したバイトを再生する攻撃者。\\n2. サーバーになりすます攻撃者。', '真、真', '偽、偽', '真、偽', '偽、真', 'A']\n",
            "\n",
            "最初の問題のデータ:\n",
            "列0: MITのケルベロスKDCサーバでは、(ほとんどのユーザプリンシパルに対して)チケットの最大存続時間は24時間である。期限切れのケルベロスチケットが使用できなくなることを保証するものは何か?\n",
            "列1: ケルベロスサーバ(KDC)は、期限切れのチケットに対して、クライアントとサーバ間の新しい接続の確立を拒否する。\n",
            "列2: クライアントがサーバに接続すると、サーバは接続を終了するために24時間のタイマーを設定し、これにより、チケットの最大有効期間を超えてクライアントが接続を維持できなくなる。\n",
            "列3: クライアントがサーバに接続すると、サーバはチケットの有効期限をサーバの現在のクロックと比較し、チケットの有効期限が過ぎている場合はユーザの認証を拒否する。\n",
            "列4: クライアントがサーバに接続すると、サーバはKDCにクエリを送信して、チケットがKDCのクロックに対してまだ有効であるかどうかをチェックし、KDCがチケットの期限切れを報告した場合は、ユーザの認証を拒否する。\n",
            "列5: C\n",
            "\n",
            "評価開始 - 全99問のコンピュータセキュリティ問題\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1509331f30a5441c97f2f0949b00b69d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/98 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "進捗: 25/99問完了、現在の正解率: 0/25 (0.0000)\n",
            "進捗: 50/99問完了、現在の正解率: 0/50 (0.0000)\n",
            "進捗: 75/99問完了、現在の正解率: 0/75 (0.0000)\n",
            "\n",
            "評価完了！\n",
            "モデル: elyza/ELYZA-japanese-Llama-2-7b\n",
            "データセット: JMMLU computer_security\n",
            "正解数: 0/98問\n",
            "正解率: 0.0000 (0.00%)\n"
          ]
        }
      ],
      "source": [
        "# 43\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "\n",
        "# JMMLUリポジトリのクローン（まだ存在しない場合）\n",
        "if not os.path.exists(\"./JMMLU\"):\n",
        "   print(\"JMMLUリポジトリをクローン中...\")\n",
        "   !git clone https://github.com/nlp-waseda/JMMLU.git\n",
        "\n",
        "def generate_prompt(question, choices):\n",
        "   \"\"\"プロンプトを生成する関数\"\"\"\n",
        "   prompt = f\"\"\"以下のコンピュータセキュリティに関する多肢選択問題を注意深く考えて解いてください。\n",
        "ステップバイステップで考え、各選択肢を分析してから最適な答えを選んでください。\n",
        "\n",
        "問題: {question}\n",
        "\n",
        "選択肢:\n",
        "A. {choices[0]}\n",
        "B. {choices[1]}\n",
        "C. {choices[2]}\n",
        "D. {choices[3]}\n",
        "\n",
        "問題に関連する概念や定義を思い出し、各選択肢の妥当性を評価してください。\n",
        "あなたの推論過程を示し、最後に「答え: X」の形式で回答を明確に示してください。ここでXはA、B、C、Dのいずれかです。\n",
        "\"\"\"\n",
        "   return prompt\n",
        "\n",
        "def extract_answer(text):\n",
        "   \"\"\"モデルの出力から回答を抽出する関数\"\"\"\n",
        "   # 明確な「答え: X」のフォーマットを最優先で検索\n",
        "   pattern1 = r\"答え\\s*[:：]\\s*([ABCD])\"\n",
        "   match1 = re.search(pattern1, text)\n",
        "   if match1:\n",
        "       return match1.group(1)\n",
        "\n",
        "   # 文末の「したがって、答えはX」「よって、X」などのパターン\n",
        "   pattern_conclusion = r\"(?:したがって|よって|結論として|以上より)[^ABCD]*([ABCD])[^ABCD]*(?:が正解|を選択|が適切)\"\n",
        "   match_conclusion = re.search(pattern_conclusion, text)\n",
        "   if match_conclusion:\n",
        "       return match_conclusion.group(1)\n",
        "\n",
        "   # パターン2: 最後の行に単独で「A」「B」「C」「D」のいずれかがある\n",
        "   pattern2 = r\"([ABCD])\\s*$\"\n",
        "   match2 = re.search(pattern2, text)\n",
        "   if match2:\n",
        "       return match2.group(1)\n",
        "\n",
        "   # パターン3: 文中に「選択肢はX」「Xが正解」などの表現がある\n",
        "   pattern3 = r\"選択肢[はがのはABCD]*\\s*([ABCD])\"\n",
        "   match3 = re.search(pattern3, text)\n",
        "   if match3:\n",
        "       return match3.group(1)\n",
        "\n",
        "   # テキスト全体からA,B,C,Dを探し、最後に言及されたものを優先\n",
        "   choices = re.findall(r'\\b[ABCD]\\b', text)\n",
        "   if choices:\n",
        "       # 最後の5つの選択肢の中で最も頻出のものを返す（最終的な結論を重視）\n",
        "       if len(choices) >= 5:\n",
        "           recent_choices = choices[-5:]\n",
        "           return max(set(recent_choices), key=recent_choices.count)\n",
        "       return choices[-1]  # 最後に言及された選択肢\n",
        "\n",
        "   return None\n",
        "\n",
        "def evaluate_jmmlu_computer_security(model, tokenizer):\n",
        "   \"\"\"JMMLUのコンピュータセキュリティデータセットを評価する関数\"\"\"\n",
        "   # CSVファイルのパス\n",
        "   csv_path = \"./JMMLU/JMMLU/computer_security.csv\"\n",
        "\n",
        "   # CSVファイル読み込み\n",
        "   print(f\"コンピュータセキュリティCSV読み込み中: {csv_path}\")\n",
        "   df = pd.read_csv(csv_path)\n",
        "   print(f\"データセット読み込み完了。問題数: {len(df)}問\")\n",
        "\n",
        "   # カラム名の確認\n",
        "   print(f\"カラム名: {df.columns.tolist()}\")\n",
        "\n",
        "   # データ形式の確認（最初の行）\n",
        "   first_row = df.iloc[0].tolist()\n",
        "   print(\"\\n最初の問題のデータ:\")\n",
        "   for i, value in enumerate(first_row):\n",
        "       print(f\"列{i}: {value}\")\n",
        "\n",
        "   # 評価用データの準備\n",
        "   correct = 0\n",
        "   total = 0\n",
        "\n",
        "   # 進捗表示用\n",
        "   print(\"\\n評価開始 - 全99問のコンピュータセキュリティ問題\")\n",
        "\n",
        "   # 各問題に対して評価\n",
        "   for i in tqdm(range(len(df)), desc=\"Evaluating\"):\n",
        "       row = df.iloc[i].tolist()\n",
        "\n",
        "       # 行の形式に応じて問題と選択肢を抽出\n",
        "       question = row[0]  # 最初の列を問題文とする\n",
        "       choices = row[1:5]  # 2-5列目を選択肢とする\n",
        "       correct_answer = row[5]  # 6列目を正解とする\n",
        "\n",
        "       # プロンプト生成\n",
        "       prompt = generate_prompt(question, choices)\n",
        "\n",
        "       # トークナイズとモデル入力の準備\n",
        "       inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "       # 生成\n",
        "       with torch.no_grad():\n",
        "           outputs = model.generate(\n",
        "               **inputs,\n",
        "               max_new_tokens=256,\n",
        "               temperature=0.0,  # 温度を0に変更\n",
        "               do_sample=False,  # サンプリングをオフに\n",
        "               top_p=1.0,        # トップP値を最大に\n",
        "               pad_token_id=tokenizer.eos_token_id\n",
        "           )\n",
        "\n",
        "       # 出力のデコード\n",
        "       generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "       # プロンプト部分を削除して回答のみを取得\n",
        "       predicted_text = generated_text[len(prompt):]\n",
        "\n",
        "       # 回答の抽出\n",
        "       predicted_answer = extract_answer(predicted_text)\n",
        "\n",
        "       # 正解判定\n",
        "       is_correct = (predicted_answer == correct_answer)\n",
        "       if is_correct:\n",
        "           correct += 1\n",
        "       total += 1\n",
        "\n",
        "       # 25問ごとに進捗状況を表示\n",
        "       if (i + 1) % 25 == 0:\n",
        "           print(f\"進捗: {i+1}/99問完了、現在の正解率: {correct}/{total} ({correct/total:.4f})\")\n",
        "\n",
        "   # 最終結果の表示\n",
        "   accuracy = correct / total\n",
        "   print(f\"\\n評価完了！\")\n",
        "   print(f\"モデル: elyza/ELYZA-japanese-Llama-2-7b\")\n",
        "   print(f\"データセット: JMMLU computer_security\")\n",
        "   print(f\"正解数: {correct}/{total}問\")\n",
        "   print(f\"正解率: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "   return correct, total, accuracy\n",
        "\n",
        "# メイン実行部分\n",
        "# モデルとトークナイザーはすでに読み込まれていると仮定\n",
        "# model = すでに読み込まれているモデル\n",
        "# tokenizer = すでに読み込まれているトークナイザー\n",
        "\n",
        "# 評価の実行\n",
        "correct, total, accuracy = evaluate_jmmlu_computer_security(model, tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtIJP10Ad5P6",
        "outputId": "05b754dc-9b86-434a-de28-f2d153d081a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "生成開始...\n",
            "\n",
            "--- 生成されたテキスト全体 ---\n",
            "以下の問題を解いてください。\n",
            "\n",
            "\n",
            "つばめちゃんは渋谷駅から東急東横線に乗り、自由が丘駅で乗り換えました。\n",
            "東急大井町線の大井町方面の電車に乗り換えたとき、各駅停車に乗車すべきところ、\n",
            "間違えて急行に乗車してしまったことに気付きました。\n",
            "自由が丘の次の急行停車駅で降車し、反対方向の電車で一駅戻った駅がつばめちゃんの目的地でした。\n",
            "目的地の駅の名前を答えてください。\n",
            "\n",
            "\n",
            "この問題を解析すると：\n",
            "1. つばめちゃんは自由が丘駅で東急大井町線の大井町方面行きの急行に乗った\n",
            "2. 自由が丘の次の急行停車駅で降りた\n",
            "3. そこから反対方向（溝の口方面）の電車で1駅戻った駅が目的地\n",
            "\n",
            "東急大井町線の駅と急行停車駅の情報：\n",
            "- 溝の口方面から大井町方面への順序: 溝の口→高津→二子新地→二子玉川→上野毛→等々力→尾山台→九品仏→自由が丘→緑が丘→大岡山→北千束→旗の台→荏原町→中延→戸越公園→下神明→大井町\n",
            "- 急行停車駅: 溝の口、二子玉川、自由が丘、大岡山、旗の台、大井町\n",
            "\n",
            "この情報から、つばめちゃんの目的地の駅名を答えてください。\n",
            "\n",
            "\n",
            "--- 生成された回答部分 ---\n",
            "\n",
            "\n",
            "生成モデルからの回答を抽出できませんでした。\n",
            "プログラムによる計算結果: 緑が丘駅\n",
            "\n",
            "最終回答:\n",
            "緑が丘駅\n",
            "\n",
            "正解は: 緑が丘駅\n",
            "✓ 回答は正解です\n"
          ]
        }
      ],
      "source": [
        "# 44\n",
        "def solve_train_problem():\n",
        "    # 問題解決のための情報\n",
        "    # 1. 東急大井町線の駅・急行停車駅の情報\n",
        "    stations = [\n",
        "        \"溝の口\", \"高津\", \"二子新地\", \"二子玉川\", \"上野毛\",\n",
        "        \"等々力\", \"尾山台\", \"九品仏\", \"自由が丘\", \"緑が丘\",\n",
        "        \"大岡山\", \"北千束\", \"旗の台\", \"荏原町\", \"中延\",\n",
        "        \"戸越公園\", \"下神明\", \"大井町\"\n",
        "    ]\n",
        "\n",
        "    express_stops = [\"溝の口\", \"二子玉川\", \"自由が丘\", \"大岡山\", \"旗の台\", \"大井町\"]\n",
        "\n",
        "    # 2. 問題の条件から解答を導く\n",
        "    jiyugaoka_index = stations.index(\"自由が丘\")\n",
        "\n",
        "    # 自由が丘から大井町方面へ向かう急行停車駅を探す\n",
        "    next_express_stop = None\n",
        "    for i in range(jiyugaoka_index + 1, len(stations)):\n",
        "        if stations[i] in express_stops:\n",
        "            next_express_stop = stations[i]\n",
        "            next_express_index = i\n",
        "            break\n",
        "\n",
        "    # その駅から反対方向に1駅戻る\n",
        "    destination_index = next_express_index - 1\n",
        "    destination = stations[destination_index]\n",
        "\n",
        "    return destination\n",
        "\n",
        "# 問題文\n",
        "problem = \"\"\"\n",
        "つばめちゃんは渋谷駅から東急東横線に乗り、自由が丘駅で乗り換えました。\n",
        "東急大井町線の大井町方面の電車に乗り換えたとき、各駅停車に乗車すべきところ、\n",
        "間違えて急行に乗車してしまったことに気付きました。\n",
        "自由が丘の次の急行停車駅で降車し、反対方向の電車で一駅戻った駅がつばめちゃんの目的地でした。\n",
        "目的地の駅の名前を答えてください。\n",
        "\"\"\"\n",
        "\n",
        "# Pythonコードで解を求める（LLMに頼らない）\n",
        "correct_answer = solve_train_problem()\n",
        "\n",
        "# モデルに与えるプロンプト\n",
        "prompt = f\"\"\"以下の問題を解いてください。\n",
        "\n",
        "{problem}\n",
        "\n",
        "この問題を解析すると：\n",
        "1. つばめちゃんは自由が丘駅で東急大井町線の大井町方面行きの急行に乗った\n",
        "2. 自由が丘の次の急行停車駅で降りた\n",
        "3. そこから反対方向（溝の口方面）の電車で1駅戻った駅が目的地\n",
        "\n",
        "東急大井町線の駅と急行停車駅の情報：\n",
        "- 溝の口方面から大井町方面への順序: 溝の口→高津→二子新地→二子玉川→上野毛→等々力→尾山台→九品仏→自由が丘→緑が丘→大岡山→北千束→旗の台→荏原町→中延→戸越公園→下神明→大井町\n",
        "- 急行停車駅: 溝の口、二子玉川、自由が丘、大岡山、旗の台、大井町\n",
        "\n",
        "この情報から、つばめちゃんの目的地の駅名を答えてください。\n",
        "\"\"\"\n",
        "\n",
        "# 入力のエンコード\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "print(\"生成開始...\")\n",
        "\n",
        "# 出力の生成\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.1,  # 低温度で確定的な出力\n",
        "        num_return_sequences=1,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "# 出力のデコード\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# プロンプトと生成された回答を分離\n",
        "answer = generated_text[len(prompt):].strip()\n",
        "\n",
        "print(\"\\n--- 生成されたテキスト全体 ---\")\n",
        "print(generated_text)\n",
        "print(\"\\n--- 生成された回答部分 ---\")\n",
        "print(answer)\n",
        "\n",
        "# 駅名を抽出\n",
        "def extract_station_name(text):\n",
        "    # 「〇〇駅」という形式を探す\n",
        "    station_matches = re.findall(r'([^\\s]+駅)', text)\n",
        "    if station_matches:\n",
        "        return station_matches[0]\n",
        "\n",
        "    # 東急大井町線の駅リスト\n",
        "    stations = [\"大井町\", \"下神明\", \"戸越公園\", \"中延\", \"荏原町\", \"旗の台\",\n",
        "               \"北千束\", \"大岡山\", \"緑が丘\", \"自由が丘\", \"九品仏\", \"尾山台\",\n",
        "               \"等々力\", \"上野毛\", \"二子玉川\", \"二子新地\", \"高津\", \"溝の口\"]\n",
        "\n",
        "    # テキスト内に駅名があるか確認\n",
        "    for station in stations:\n",
        "        if station in text:\n",
        "            return f\"{station}駅\"\n",
        "\n",
        "    return \"駅名を特定できませんでした\"\n",
        "\n",
        "# 駅名の抽出\n",
        "extracted_station = extract_station_name(answer)\n",
        "\n",
        "# 回答がない場合に備えて、Pythonの計算結果を使用\n",
        "if extracted_station == \"駅名を特定できませんでした\":\n",
        "    print(\"\\n生成モデルからの回答を抽出できませんでした。\")\n",
        "    print(f\"プログラムによる計算結果: {correct_answer}駅\")\n",
        "    final_answer = f\"{correct_answer}駅\"\n",
        "else:\n",
        "    final_answer = extracted_station\n",
        "\n",
        "print(\"\\n最終回答:\")\n",
        "print(final_answer)\n",
        "\n",
        "# 正解との比較\n",
        "print(f\"\\n正解は: {correct_answer}駅\")\n",
        "if correct_answer + \"駅\" == final_answer:\n",
        "    print(\"✓ 回答は正解です\")\n",
        "else:\n",
        "    print(\"✗ 回答は間違っています\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07LvewY_d80_",
        "outputId": "4a2ec059-fff1-4adc-ba06-4e30a9eb2bcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "生成開始...\n",
            "\n",
            "--- 生成されたテキスト全体 ---\n",
            "以下の問題を解いてください。\n",
            "\n",
            "\n",
            "さらに、つばめちゃんが自由が丘駅で乗り換えたとき、先ほどとは反対方向の急行電車に間違って乗車してしまった場合を考えます。\n",
            "目的地の駅に向かうため、自由が丘の次の急行停車駅で降車した後、反対方向の各駅停車に乗車した場合、何駅先の駅で降りれば良いでしょうか？\n",
            "\n",
            "\n",
            "この問題を解析すると：\n",
            "1. つばめちゃんは自由が丘駅で東急大井町線の溝の口方面行きの急行に間違って乗った\n",
            "2. 自由が丘の次の溝の口方面の急行停車駅（二子玉川駅）で降りた\n",
            "3. そこから反対方向（大井町方面）の各駅停車に乗って目的地の大岡山駅に向かいたい\n",
            "4. 二子玉川駅から大岡山駅までは何駅あるか\n",
            "\n",
            "東急大井町線の駅と急行停車駅の情報：\n",
            "- 溝の口方面から大井町方面への順序: 溝の口→高津→二子新地→二子玉川→上野毛→等々力→尾山台→九品仏→自由が丘→緑が丘→大岡山→北千束→旗の台→荏原町→中延→戸越公園→下神明→大井町\n",
            "- 急行停車駅: 溝の口、二子玉川、自由が丘、大岡山、旗の台、大井町\n",
            "- 経路: 二子玉川 → 上野毛 → 等々力 → 尾山台 → 九品仏 → 自由が丘 → 緑が丘 → 大岡山\n",
            "\n",
            "この情報から、何駅先で降りればよいか答えてください。\n",
            "\n",
            "\n",
            "--- 生成された回答部分 ---\n",
            "\n",
            "\n",
            "生成モデルからの回答を抽出できませんでした。\n",
            "プログラムによる計算結果: 7駅先\n",
            "\n",
            "最終回答:\n",
            "7駅先\n",
            "\n",
            "正解は: 7駅先\n",
            "経路: 二子玉川 → 上野毛 → 等々力 → 尾山台 → 九品仏 → 自由が丘 → 緑が丘 → 大岡山\n"
          ]
        }
      ],
      "source": [
        "# 追加の問題解決\n",
        "def solve_opposite_direction_problem():\n",
        "    # 問題解決のための情報\n",
        "    # 1. 東急大井町線の駅・急行停車駅の情報\n",
        "    stations = [\n",
        "        \"溝の口\", \"高津\", \"二子新地\", \"二子玉川\", \"上野毛\",\n",
        "        \"等々力\", \"尾山台\", \"九品仏\", \"自由が丘\", \"緑が丘\",\n",
        "        \"大岡山\", \"北千束\", \"旗の台\", \"荏原町\", \"中延\",\n",
        "        \"戸越公園\", \"下神明\", \"大井町\"\n",
        "    ]\n",
        "\n",
        "    express_stops = [\"溝の口\", \"二子玉川\", \"自由が丘\", \"大岡山\", \"旗の台\", \"大井町\"]\n",
        "\n",
        "    # 2. 問題の条件から解答を導く\n",
        "    jiyugaoka_index = stations.index(\"自由が丘\")\n",
        "\n",
        "    # 自由が丘から溝の口方面へ向かう（反対方向）急行停車駅を探す\n",
        "    next_express_stop = None\n",
        "    next_express_index = None\n",
        "    for i in range(jiyugaoka_index - 1, -1, -1):\n",
        "        if stations[i] in express_stops:\n",
        "            next_express_stop = stations[i]\n",
        "            next_express_index = i\n",
        "            break\n",
        "\n",
        "    # 自由が丘から目的地（大岡山）までのローカル停車駅数を計算\n",
        "    jiyugaoka_index = stations.index(\"自由が丘\")\n",
        "    goal_station = \"大岡山\"  # 前の問題の答えにより、この駅が目的地と仮定\n",
        "    goal_index = stations.index(goal_station)\n",
        "\n",
        "    # 二子玉川（次の急行停車駅）から反対方向（大井町方面）の電車に乗って\n",
        "    # 目的地（大岡山）までの駅数を計算\n",
        "    stations_count = goal_index - next_express_index\n",
        "\n",
        "    return {\n",
        "        \"next_express_stop\": next_express_stop,\n",
        "        \"stations_count\": stations_count,\n",
        "        \"stations_path\": stations[next_express_index:goal_index+1]\n",
        "    }\n",
        "\n",
        "# 追加の問題文\n",
        "additional_problem = \"\"\"\n",
        "さらに、つばめちゃんが自由が丘駅で乗り換えたとき、先ほどとは反対方向の急行電車に間違って乗車してしまった場合を考えます。\n",
        "目的地の駅に向かうため、自由が丘の次の急行停車駅で降車した後、反対方向の各駅停車に乗車した場合、何駅先の駅で降りれば良いでしょうか？\n",
        "\"\"\"\n",
        "\n",
        "# Pythonコードで解を求める\n",
        "solution = solve_opposite_direction_problem()\n",
        "\n",
        "# モデルに与えるプロンプト\n",
        "prompt = f\"\"\"以下の問題を解いてください。\n",
        "\n",
        "{additional_problem}\n",
        "\n",
        "この問題を解析すると：\n",
        "1. つばめちゃんは自由が丘駅で東急大井町線の溝の口方面行きの急行に間違って乗った\n",
        "2. 自由が丘の次の溝の口方面の急行停車駅（{solution[\"next_express_stop\"]}駅）で降りた\n",
        "3. そこから反対方向（大井町方面）の各駅停車に乗って目的地の大岡山駅に向かいたい\n",
        "4. {solution[\"next_express_stop\"]}駅から大岡山駅までは何駅あるか\n",
        "\n",
        "東急大井町線の駅と急行停車駅の情報：\n",
        "- 溝の口方面から大井町方面への順序: 溝の口→高津→二子新地→二子玉川→上野毛→等々力→尾山台→九品仏→自由が丘→緑が丘→大岡山→北千束→旗の台→荏原町→中延→戸越公園→下神明→大井町\n",
        "- 急行停車駅: 溝の口、二子玉川、自由が丘、大岡山、旗の台、大井町\n",
        "- 経路: {' → '.join(solution[\"stations_path\"])}\n",
        "\n",
        "この情報から、何駅先で降りればよいか答えてください。\n",
        "\"\"\"\n",
        "\n",
        "# 入力のエンコード\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "print(\"生成開始...\")\n",
        "\n",
        "# 出力の生成\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_new_tokens=100,\n",
        "        temperature=0.1,  # 低温度で確定的な出力\n",
        "        num_return_sequences=1,\n",
        "        top_p=0.95,\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "# 出力のデコード\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# プロンプトと生成された回答を分離\n",
        "answer = generated_text[len(prompt):].strip()\n",
        "\n",
        "print(\"\\n--- 生成されたテキスト全体 ---\")\n",
        "print(generated_text)\n",
        "print(\"\\n--- 生成された回答部分 ---\")\n",
        "print(answer)\n",
        "\n",
        "# 回答がない場合に備えて、Pythonの計算結果を使用\n",
        "if not answer.strip():\n",
        "    print(\"\\n生成モデルからの回答を抽出できませんでした。\")\n",
        "    print(f\"プログラムによる計算結果: {solution['stations_count']}駅先\")\n",
        "    final_answer = f\"{solution['stations_count']}駅先\"\n",
        "else:\n",
        "    final_answer = answer\n",
        "\n",
        "print(\"\\n最終回答:\")\n",
        "print(final_answer)\n",
        "print(f\"\\n正解は: {solution['stations_count']}駅先\")\n",
        "print(f\"経路: {' → '.join(solution['stations_path'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE4MZiOdilmW"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7wxEWxFfQek",
        "outputId": "e327f10d-49eb-4421-adf4-827130e97cd4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "川柳の生成を開始...\n",
            "\n",
            "--- 生成された川柳10選 ---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 45\n",
        "# 夏祭りと花火をテーマにした川柳生成のプロンプト\n",
        "prompt = \"\"\"「夏祭り」と「花火」をテーマにした川柳を10個生成してください。川柳は5-7-5の音節構造を持つ日本の短詩形です。\n",
        "季節感や情緒、夏の風物詩としての魅力が伝わる作品にしてください。それぞれの川柳には短い解説も付けてください。\n",
        "\n",
        "例えば：\n",
        "「花火見て 子どもの顔に 咲く笑顔」\n",
        "解説：花火を見上げる子どもの無邪気な表情を咲く花に例えた一句\n",
        "\n",
        "以下、「夏祭り」と「花火」をテーマにした川柳10個を出力してください。それぞれに簡潔な解説を付けてください。\"\"\"\n",
        "\n",
        "# 入力のエンコード\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "print(\"川柳の生成を開始...\")\n",
        "\n",
        "# より創造的な出力を得るため、温度を上げる\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs.input_ids,\n",
        "        max_new_tokens=1000,  # より長い出力を許可\n",
        "        temperature=0.8,      # 創造性のために温度を上げる\n",
        "        num_return_sequences=1,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2,  # 繰り返しを減らす\n",
        "        do_sample=True,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "    )\n",
        "\n",
        "# 出力のデコード\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "# プロンプトと生成された回答を分離\n",
        "answer = generated_text[len(prompt):].strip()\n",
        "\n",
        "print(\"\\n--- 生成された川柳10選 ---\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvv1K0Zlh93X",
        "outputId": "6645f379-c268-46a0-c945-1996e019e04c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "トークン数: 323\n",
            "\n",
            "トークンの内容:\n",
            "1: '<|begin_of_text|>'\n",
            "2: '吾'\n",
            "3: '�'\n",
            "4: '�'\n",
            "5: 'は'\n",
            "6: '猫'\n",
            "7: 'である'\n",
            "8: '。'\n",
            "9: '名前'\n",
            "10: 'は'\n",
            "11: 'まだ'\n",
            "12: '無'\n",
            "13: 'い'\n",
            "14: '。\n",
            "'\n",
            "15: 'どこ'\n",
            "16: 'で'\n",
            "17: '生'\n",
            "18: 'れた'\n",
            "19: 'か'\n",
            "20: 'と'\n",
            "21: 'んと'\n",
            "22: '見'\n",
            "23: '当'\n",
            "24: 'が'\n",
            "25: 'つ'\n",
            "26: 'か'\n",
            "27: 'ぬ'\n",
            "28: '。'\n",
            "29: '何'\n",
            "30: 'でも'\n",
            "31: '薄'\n",
            "32: '暗'\n",
            "33: 'い'\n",
            "34: 'じ'\n",
            "35: 'め'\n",
            "36: 'じ'\n",
            "37: 'め'\n",
            "38: 'した'\n",
            "39: '所'\n",
            "40: 'で'\n",
            "41: 'ニ'\n",
            "42: 'ャ'\n",
            "43: 'ーニ'\n",
            "44: 'ャ'\n",
            "45: 'ー'\n",
            "46: '泣'\n",
            "47: 'い'\n",
            "48: 'ていた'\n",
            "49: '事'\n",
            "50: 'だけ'\n",
            "51: 'は'\n",
            "52: '記'\n",
            "53: '憶'\n",
            "54: 'している'\n",
            "55: '。'\n",
            "56: '吾'\n",
            "57: '�'\n",
            "58: '�'\n",
            "59: 'は'\n",
            "60: 'ここ'\n",
            "61: 'で'\n",
            "62: '始'\n",
            "63: 'めて'\n",
            "64: '人間'\n",
            "65: 'という'\n",
            "66: 'もの'\n",
            "67: 'を見'\n",
            "68: 'た'\n",
            "69: '。'\n",
            "70: 'しか'\n",
            "71: 'も'\n",
            "72: 'あ'\n",
            "73: 'と'\n",
            "74: 'で'\n",
            "75: '聞'\n",
            "76: 'くと'\n",
            "77: 'それは'\n",
            "78: '書'\n",
            "79: '生'\n",
            "80: 'という'\n",
            "81: '人間'\n",
            "82: '中'\n",
            "83: 'で'\n",
            "84: '一'\n",
            "85: '番'\n",
            "86: '�'\n",
            "87: '��'\n",
            "88: '悪'\n",
            "89: 'な'\n",
            "90: '種'\n",
            "91: '族'\n",
            "92: 'であった'\n",
            "93: 'そう'\n",
            "94: 'だ'\n",
            "95: '。この'\n",
            "96: '書'\n",
            "97: '生'\n",
            "98: 'という'\n",
            "99: 'のは'\n",
            "100: '時'\n",
            "101: '々'\n",
            "102: '我'\n",
            "103: '々'\n",
            "104: 'を'\n",
            "105: '捕'\n",
            "106: 'えて'\n",
            "107: '�'\n",
            "108: '�'\n",
            "109: 'て'\n",
            "110: '食'\n",
            "111: 'う'\n",
            "112: 'という'\n",
            "113: '話'\n",
            "114: 'である'\n",
            "115: '。'\n",
            "116: 'しか'\n",
            "117: 'し�'\n",
            "118: '�'\n",
            "119: 'の'\n",
            "120: '当'\n",
            "121: '時'\n",
            "122: 'は'\n",
            "123: '何'\n",
            "124: 'という'\n",
            "125: '考'\n",
            "126: 'も'\n",
            "127: 'なかった'\n",
            "128: 'から'\n",
            "129: '別'\n",
            "130: '段'\n",
            "131: '恐'\n",
            "132: 'しい'\n",
            "133: 'とも'\n",
            "134: '思'\n",
            "135: 'わ'\n",
            "136: 'なかった'\n",
            "137: '。'\n",
            "138: 'ただ'\n",
            "139: '彼'\n",
            "140: 'の'\n",
            "141: '掌'\n",
            "142: 'に'\n",
            "143: '載'\n",
            "144: 'せ'\n",
            "145: 'られて'\n",
            "146: 'ス'\n",
            "147: 'ー'\n",
            "148: 'と'\n",
            "149: '持ち'\n",
            "150: '上げ'\n",
            "151: 'られた'\n",
            "152: '時'\n",
            "153: '何'\n",
            "154: 'だ'\n",
            "155: 'か'\n",
            "156: 'フ'\n",
            "157: 'ワ'\n",
            "158: 'フ'\n",
            "159: 'ワ'\n",
            "160: 'した'\n",
            "161: '感じ'\n",
            "162: 'があった'\n",
            "163: 'ばかり'\n",
            "164: 'である'\n",
            "165: '。'\n",
            "166: '掌'\n",
            "167: 'の上'\n",
            "168: 'で'\n",
            "169: '少し'\n",
            "170: '落ち'\n",
            "171: 'つ'\n",
            "172: 'いて'\n",
            "173: '書'\n",
            "174: '生'\n",
            "175: 'の'\n",
            "176: '顔'\n",
            "177: 'を見'\n",
            "178: 'た'\n",
            "179: 'のが'\n",
            "180: 'い'\n",
            "181: 'わ'\n",
            "182: 'ゆ'\n",
            "183: 'る'\n",
            "184: '人間'\n",
            "185: 'という'\n",
            "186: 'もの'\n",
            "187: 'の'\n",
            "188: '見'\n",
            "189: '始'\n",
            "190: 'であ'\n",
            "191: 'ろう'\n",
            "192: '。この'\n",
            "193: '時'\n",
            "194: '妙'\n",
            "195: 'な'\n",
            "196: 'もの'\n",
            "197: 'だ'\n",
            "198: 'と思'\n",
            "199: 'った'\n",
            "200: '感じ'\n",
            "201: 'が'\n",
            "202: '今'\n",
            "203: 'でも'\n",
            "204: '残'\n",
            "205: 'っている'\n",
            "206: '。'\n",
            "207: '第一'\n",
            "208: '毛'\n",
            "209: 'を'\n",
            "210: 'も'\n",
            "211: 'って'\n",
            "212: '装'\n",
            "213: '飾'\n",
            "214: 'され'\n",
            "215: 'べき'\n",
            "216: 'はず'\n",
            "217: 'の'\n",
            "218: '顔'\n",
            "219: 'が'\n",
            "220: 'つ'\n",
            "221: 'る'\n",
            "222: 'つ'\n",
            "223: 'る'\n",
            "224: 'して'\n",
            "225: 'まる'\n",
            "226: 'で'\n",
            "227: '薬'\n",
            "228: '�'\n",
            "229: '�'\n",
            "230: 'だ'\n",
            "231: '。その'\n",
            "232: '後'\n",
            "233: '猫'\n",
            "234: 'にも'\n",
            "235: 'だ'\n",
            "236: 'い'\n",
            "237: 'ぶ'\n",
            "238: '�'\n",
            "239: '�'\n",
            "240: 'った'\n",
            "241: 'が'\n",
            "242: 'こんな'\n",
            "243: '片'\n",
            "244: '輪'\n",
            "245: 'には'\n",
            "246: '一度'\n",
            "247: 'も'\n",
            "248: '出'\n",
            "249: '会'\n",
            "250: 'わ'\n",
            "251: 'した'\n",
            "252: '事'\n",
            "253: 'がない'\n",
            "254: '。'\n",
            "255: 'のみ'\n",
            "256: 'なら'\n",
            "257: 'ず'\n",
            "258: '顔'\n",
            "259: 'の'\n",
            "260: '真'\n",
            "261: '中'\n",
            "262: 'が'\n",
            "263: 'あ'\n",
            "264: 'まり'\n",
            "265: 'に'\n",
            "266: '突'\n",
            "267: '起'\n",
            "268: 'している'\n",
            "269: '。'\n",
            "270: 'そう'\n",
            "271: 'して'\n",
            "272: 'その'\n",
            "273: '穴'\n",
            "274: 'の中'\n",
            "275: 'から'\n",
            "276: '時'\n",
            "277: '々'\n",
            "278: '�'\n",
            "279: '�'\n",
            "280: 'う'\n",
            "281: '�'\n",
            "282: '�'\n",
            "283: 'う'\n",
            "284: 'と'\n",
            "285: '煙'\n",
            "286: 'を'\n",
            "287: '吹'\n",
            "288: 'く'\n",
            "289: '。'\n",
            "290: 'どう'\n",
            "291: 'も'\n",
            "292: '�'\n",
            "293: '�'\n",
            "294: 'せ'\n",
            "295: '�'\n",
            "296: '�'\n",
            "297: 'く'\n",
            "298: 'て'\n",
            "299: '実'\n",
            "300: 'に'\n",
            "301: '弱'\n",
            "302: 'った'\n",
            "303: '。これ'\n",
            "304: 'が'\n",
            "305: '人間'\n",
            "306: 'の'\n",
            "307: '飲'\n",
            "308: 'む'\n",
            "309: '煙'\n",
            "310: '草'\n",
            "311: 'という'\n",
            "312: 'もの'\n",
            "313: 'である'\n",
            "314: '事'\n",
            "315: 'は'\n",
            "316: 'よう'\n",
            "317: 'や'\n",
            "318: 'く'\n",
            "319: 'この'\n",
            "320: '頃'\n",
            "321: '知'\n",
            "322: 'った'\n",
            "323: '。'\n",
            "合計トークン数：323\n"
          ]
        }
      ],
      "source": [
        "# 49\n",
        "# トークン数を計算する\n",
        "text = \"\"\"吾輩は猫である。名前はまだ無い。\n",
        "どこで生れたかとんと見当がつかぬ。何でも薄暗いじめじめした所でニャーニャー泣いていた事だけは記憶している。吾輩はここで始めて人間というものを見た。しかもあとで聞くとそれは書生という人間中で一番獰悪な種族であったそうだ。この書生というのは時々我々を捕えて煮て食うという話である。しかしその当時は何という考もなかったから別段恐しいとも思わなかった。ただ彼の掌に載せられてスーと持ち上げられた時何だかフワフワした感じがあったばかりである。掌の上で少し落ちついて書生の顔を見たのがいわゆる人間というものの見始であろう。この時妙なものだと思った感じが今でも残っている。第一毛をもって装飾されべきはずの顔がつるつるしてまるで薬缶だ。その後猫にもだいぶ逢ったがこんな片輪には一度も出会わした事がない。のみならず顔の真中があまりに突起している。そうしてその穴の中から時々ぷうぷうと煙を吹く。どうも咽せぽくて実に弱った。これが人間の飲む煙草というものである事はようやくこの頃知った。\"\"\"\n",
        "\n",
        "# トークナイザーを使ってエンコード\n",
        "encoded = tokenizer.encode(text)\n",
        "\n",
        "# トークン数をカウント\n",
        "token_count = len(encoded)\n",
        "\n",
        "print(f\"トークン数: {token_count}\")\n",
        "\n",
        "# トークンの内容を表示（オプション）\n",
        "decoded_tokens = [tokenizer.decode([token_id]) for token_id in encoded]\n",
        "print(\"\\nトークンの内容:\")\n",
        "for i, token in enumerate(decoded_tokens):\n",
        "    print(f\"{i+1}: '{token}'\")\n",
        "\n",
        "print(f\"合計トークン数：{len(decoded_tokens)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "091e4aea281f47628b1a2a10aea16901": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "1509331f30a5441c97f2f0949b00b69d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e45f1f65e0a44c4bdb599bf7c0bfe70",
              "IPY_MODEL_fcd8db0b5a714073b05d2e6488be6047",
              "IPY_MODEL_8a46286cf8ad474db2aa3a00387c158e"
            ],
            "layout": "IPY_MODEL_b38cea5568fc487696528451037bcb1f"
          }
        },
        "1c02f7d4b47240ce88e27a07d5a352b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46d4d1611f2a43dea759bed70a7896cc",
            "placeholder": "​",
            "style": "IPY_MODEL_c079927ebc4b42ff8fba186fa0c40c8d",
            "value": " 98/98 [00:54&lt;00:00,  1.97it/s]"
          }
        },
        "1cd7dbbe784247da9246bba448d575a6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "25a0dff36883474d844156af47bda7d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2703df53964f449fa2c482094a7544ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28d1877945c14a4cb7d99ce4c5dd1ca7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bf9e3b4208542fda9573da32a6a7f16": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cd7dbbe784247da9246bba448d575a6",
            "placeholder": "​",
            "style": "IPY_MODEL_5596d273df254f26b858c4633f905160",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "2e45f1f65e0a44c4bdb599bf7c0bfe70": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3a11f102250419293cd76afd15d4a7d",
            "placeholder": "​",
            "style": "IPY_MODEL_e25d4a9829394673858a0ff5173f2525",
            "value": "Evaluating: 100%"
          }
        },
        "35c40a16682f49d796719b7fd4dcaf97": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4200b28b6c2a4edd89597fc98e5e8a42": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46d4d1611f2a43dea759bed70a7896cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c28cc2dcd3944ec905aef11c1c2ad71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a92297ee3e3d436cad22ca943342dc6b",
            "placeholder": "​",
            "style": "IPY_MODEL_676d1ede892340838323c31ca81af2f6",
            "value": "Connecting..."
          }
        },
        "5596d273df254f26b858c4633f905160": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59c645aeb8134339b3536552cf33d853": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60a7dcd692c74fa3be5c8a76d54099c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_b46ef3a817f5451d93a4de3424214b37"
          }
        },
        "61642fb735614d2b8cf5abaf939b7847": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59c645aeb8134339b3536552cf33d853",
            "placeholder": "​",
            "style": "IPY_MODEL_a79d3554227146869d5ecd1f709e0b45",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "676d1ede892340838323c31ca81af2f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "694f0ed3dd5e43deb4c9b9f9839459f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72488f0e467e405ba087639de2bfcd72": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_28d1877945c14a4cb7d99ce4c5dd1ca7",
            "style": "IPY_MODEL_091e4aea281f47628b1a2a10aea16901",
            "tooltip": ""
          }
        },
        "8420451bd5e8437492feee5b71d35aff": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a46286cf8ad474db2aa3a00387c158e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25a0dff36883474d844156af47bda7d2",
            "placeholder": "​",
            "style": "IPY_MODEL_cb1ae772e0f44cb4a94b2814f8689735",
            "value": " 98/98 [00:31&lt;00:00,  3.22it/s]"
          }
        },
        "a79d3554227146869d5ecd1f709e0b45": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a92297ee3e3d436cad22ca943342dc6b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b13f5628d906498c877246f7a6199cfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_8420451bd5e8437492feee5b71d35aff",
            "placeholder": "​",
            "style": "IPY_MODEL_f6360eb75afa45dbbfc16df2c9611bc5",
            "value": ""
          }
        },
        "b22c4e8fdf754d20bc0c9415741672f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_694f0ed3dd5e43deb4c9b9f9839459f8",
            "max": 98,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c43c49b8386d4fd0acf9131370e2e192",
            "value": 98
          }
        },
        "b38cea5568fc487696528451037bcb1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b46ef3a817f5451d93a4de3424214b37": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "c079927ebc4b42ff8fba186fa0c40c8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c43c49b8386d4fd0acf9131370e2e192": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cb1ae772e0f44cb4a94b2814f8689735": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d15b4aead25f42c68ff9a5e6381f1401": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd85e40b80e548db9345e898b164ab7e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e25d4a9829394673858a0ff5173f2525": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ef3fb6484b334ae2bdae05fbd61f4051": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d15b4aead25f42c68ff9a5e6381f1401",
            "placeholder": "​",
            "style": "IPY_MODEL_35c40a16682f49d796719b7fd4dcaf97",
            "value": "Evaluating: 100%"
          }
        },
        "efa71607b0ea403d87c5318aa648b901": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f05db64e643a479396ed7b4cd34fb815": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef3fb6484b334ae2bdae05fbd61f4051",
              "IPY_MODEL_b22c4e8fdf754d20bc0c9415741672f8",
              "IPY_MODEL_1c02f7d4b47240ce88e27a07d5a352b9"
            ],
            "layout": "IPY_MODEL_f3856722090a4d478194c5cc91bf4d99"
          }
        },
        "f3856722090a4d478194c5cc91bf4d99": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3a11f102250419293cd76afd15d4a7d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6360eb75afa45dbbfc16df2c9611bc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcd8db0b5a714073b05d2e6488be6047": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efa71607b0ea403d87c5318aa648b901",
            "max": 98,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4200b28b6c2a4edd89597fc98e5e8a42",
            "value": 98
          }
        },
        "fe8c14d2c2904668ba009cc27f643126": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_dd85e40b80e548db9345e898b164ab7e",
            "style": "IPY_MODEL_2703df53964f449fa2c482094a7544ec",
            "value": true
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
