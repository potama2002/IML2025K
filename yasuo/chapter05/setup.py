# -*- coding: utf-8 -*-
"""chapter05.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zK5jiORcTh1lltWAMfZ8i5buD5ml2UDZ
"""

from huggingface_hub import notebook_login

notebook_login()

!pip install -U safetensors
!pip install -U bitsandbytes
!pip install -U accelerate

import torch
import os
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# HuggingFaceにログイン
os.environ["HF_TOKEN"] = ""

# オフロードフォルダの設定
offload_folder = "/tmp/model_offload"
os.makedirs(offload_folder, exist_ok=True)

# 量子化の設定
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=False,
)

# トークナイザーのロード - padding_side と pad_token の設定を追加
tokenizer = AutoTokenizer.from_pretrained(
   #"elyza/ELYZA-japanese-Llama-2-7b",
    "meta-llama/Meta-Llama-3-8B",
    token=os.environ["HF_TOKEN"],
    padding_side="right",  # パディングの位置を右側に設定
)

# Llama-3ではデフォルトでpad_tokenが設定されていないので追加
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# モデルのロード
model = AutoModelForCausalLM.from_pretrained(
    #"elyza/ELYZA-japanese-Llama-2-7b",
    "meta-llama/Meta-Llama-3-8B",
    device_map="auto",
    quantization_config=bnb_config,
    torch_dtype=torch.float16,
    token=os.environ["HF_TOKEN"],
    offload_folder=offload_folder
)