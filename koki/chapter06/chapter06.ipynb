{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter06 機械学習\n",
    "\n",
    "News Aggregator Data Setを用い、ニュース記事の見出しをビジネス、科学技術、エンターテイメント、健康のカテゴリに分類するタスクに取り組む"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  422937 ./newsCorpora.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l ./newsCorpora.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\tFed official says weak data caused by weather, should not slow taper\thttp://www.latimes.com/business/money/la-fi-mo-federal-reserve-plosser-stimulus-economy-20140310,0,1312750.story\\?track=rss\tLos Angeles Times\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.latimes.com\t1394470370698\n",
      "2\tFed's Charles Plosser sees high bar for change in pace of tapering\thttp://www.livemint.com/Politics/H2EvwJSK2VE6OF7iK1g3PP/Feds-Charles-Plosser-sees-high-bar-for-change-in-pace-of-ta.html\tLivemint\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.livemint.com\t1394470371207\n",
      "3\tUS open: Stocks fall after Fed official hints at accelerated tapering\thttp://www.ifamagazine.com/news/us-open-stocks-fall-after-fed-official-hints-at-accelerated-tapering-294436\tIFA Magazine\tb\tddUyU0VZz0BRneMioxUPQVP6sIxvM\twww.ifamagazine.com\t1394470371550\n"
     ]
    }
   ],
   "source": [
    "!head -3 ./newsCorpora.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 50. データの入手・整形(2)\n",
    "\n",
    "News Aggregator Data Setをダウンロードし、以下の要領で学習データ（train.txt），検証データ（valid.txt），評価データ（test.txt）を作成せよ．\n",
    "\n",
    "\n",
    "1. ダウンロードしたzipファイルを解凍し，readme.txtの説明を読む．\n",
    "2. 情報源（publisher）が”Reuters”, “Huffington Post”, “Businessweek”, “Contactmusic.com”, “Daily Mail”の事例（記事）のみを抽出する．\n",
    "3. 抽出された事例をランダムに並び替える．\n",
    "4. 抽出された事例の80%を学習データ，残りの10%ずつを検証データと評価データに分割し，それぞれtrain.txt，valid.txt，test.txtというファイル名で保存する．ファイルには，１行に１事例を書き出すこととし，カテゴリ名と記事見出しのタブ区切り形式とせよ（このファイルは後に問題70で再利用する）．\n",
    "\n",
    "\n",
    "学習データと評価データを作成したら，各カテゴリの事例数を確認せよ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データ\n",
      "b    4496\n",
      "e    4213\n",
      "t    1225\n",
      "m     738\n",
      "Name: CATEGORY, dtype: int64\n",
      "\n",
      "検証データ\n",
      "b    573\n",
      "e    539\n",
      "t    147\n",
      "m     75\n",
      "Name: CATEGORY, dtype: int64\n",
      "\n",
      "テストデータ\n",
      "b    558\n",
      "e    527\n",
      "t    152\n",
      "m     97\n",
      "Name: CATEGORY, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#%%writefile knock50.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('newsCorpora.csv', sep = '\\t', header=None, names = ['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
    "\n",
    "#要領2 出版元の抽出\n",
    "flag = df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail'])  #df[].isin(hoge) <- hogeが含まれていたらTrueを返す\n",
    "df = df[flag]\n",
    "#まとめて書いた場合が以下の通り\n",
    "#df = df.loc[df['PUBLISHER'].isin(['Reuters', 'Huffington Post', 'Businessweek', 'Contactmusic.com', 'Daily Mail'])]\n",
    "#len(df) #13340\n",
    "\n",
    "#要領3 データのシャッフル\n",
    "df = df.sample(frac=1, random_state=0)\n",
    "#sampleメソッド...dfをランダムサンプリング\n",
    "#frac引数...抽出する行・列の割合。1だと100%指定、再現性確保のためシード値は0に設定\n",
    "\n",
    "#要領4 データの分割\n",
    "train_data, other_data = train_test_split(df, test_size=0.2)  # 訓練データ8, その他2\n",
    "valid_data, test_data = train_test_split(other_data, test_size=0.5)  # 検証データ1, テストデータ1\n",
    "\n",
    "#テキストに書き出し\n",
    "#train_data.to_csv('./train.txt', sep = '\\t', index = False)\n",
    "#valid_data.to_csv('./valid.txt', sep = '\\t', index = False)\n",
    "#test_data.to_csv('./test.txt', sep = '\\t', index = False)\n",
    "\n",
    "#各カテゴリの事例数の確認\n",
    "print('学習データ')\n",
    "print(train_data['CATEGORY'].value_counts())\n",
    "print('\\n検証データ')\n",
    "print(valid_data['CATEGORY'].value_counts())\n",
    "print('\\nテストデータ')\n",
    "print(test_data['CATEGORY'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <span style=\"color: red; \">51.特徴量抽出(7)</span>\n",
    "\n",
    "学習データ，検証データ，評価データから特徴量を抽出し，それぞれtrain.feature.txt，valid.feature.txt，test.feature.txtというファイル名で保存せよ． なお，カテゴリ分類に有用そうな特徴量は各自で自由に設計せよ．記事の見出しを単語列に変換したものが最低限のベースラインとなるであろう．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TfidfVectorizer\n",
    "TfidfVectorizerは文書群を与えると，各文書をtf-Idfの値を元にしたベクトルに変換するもの．\n",
    "\n",
    "- tf ... term frequency 単語の出現頻度\n",
    "- idf ... inverse document frequency \n",
    "\n",
    "\n",
    "$idf_{wj} = log{\\frac{文書数+1}{w_{i}が出現する文書数+1}}+1$\n",
    "\n",
    "\n",
    "特定の文書に出現する単語ほど、ある話題に特化した意味のある単語という考え方\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データ\n",
      "b    4495\n",
      "e    4223\n",
      "t    1222\n",
      "m     732\n",
      "Name: CATEGORY, dtype: int64\n",
      "\n",
      "検証データ\n",
      "b    564\n",
      "e    521\n",
      "t    157\n",
      "m     92\n",
      "Name: CATEGORY, dtype: int64\n",
      "\n",
      "テストデータ\n",
      "b    568\n",
      "e    535\n",
      "t    145\n",
      "m     86\n",
      "Name: CATEGORY, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#%%writefile knock51.py\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from knock50 import train_data, valid_data, test_data\n",
    "\n",
    "def preprosessing(text):\n",
    "    '''前処理'''\n",
    "    table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "    text = text.translate(table)\n",
    "    text = text.lower()\n",
    "    pattern = re.compile('[0-9]+')\n",
    "    text = re.sub(pattern, '0', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "#データの連結、前処理\n",
    "df = pd.concat([train_data, valid_data, test_data], axis = 0)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df['TITLE'] = df['TITLE'].map(lambda x: preprosessing(x)) #map関数を使ってSeriesの各要素に前処理の関数を適用\n",
    "\n",
    "#単語のベクトル化\n",
    "vec_tfidf = TfidfVectorizer() #TfidfVectorizerのインスタンス生成\n",
    "data = vec_tfidf.fit_transform(df['TITLE'])\n",
    "data = pd.DataFrame(data.toarray(), columns = vec_tfidf.get_feature_names_out())\n",
    "\n",
    "#分割幅の指定\n",
    "split_point1 = int(len(data)//3)\n",
    "split_point2 = int(split_point1 * 2)\n",
    "\n",
    "#学習、検証、評価データ\n",
    "x_train = data[:split_point1]\n",
    "x_valid = data[split_point1:split_point2]\n",
    "x_test = data[split_point2:]\n",
    "\n",
    "#学習、検証、評価等別\n",
    "y_data = df['CATEGORY']\n",
    "y_train = y_data[:split_point1]\n",
    "y_valid = y_data[split_point1:split_point2]\n",
    "y_test = y_data[split_point2:]\n",
    "\n",
    "#特徴量の書き出し\n",
    "#x_train.to_csv('train.feature.txt', sep = '\\t', index = False)\n",
    "#x_valid.to_csv('valid.feature.txt', sep = '\\t', index = False)\n",
    "#x_test.to_csv('test.feature.txt', sep = '\\t', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 52. 学習 (2)\n",
    "51で構築した学習データを用いて，ロジスティック回帰モデルを学習せよ．    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%writefile knock52.py\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()  # ロジスティック回帰モデルのインスタンス生成\n",
    "model.fit(x_train, y_train)  # ロジスティック回帰モデルの重みを学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 53. 予測(2)\n",
    "52で学習したロジスティック回帰モデルを用い，与えられた記事見出しからカテゴリとその予測確率を計算するプログラムを実装せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "各記事のカテゴリ(ラベル) :  ['t' 'e' 'e' ... 'e' 'b' 'e']\n",
      "各記事のカテゴリ予測 :  ['b' 'e' 'e' ... 'e' 'b' 'e']\n",
      "カテゴリの予測確率:\n",
      " [[0.40230371 0.17059931 0.07384669 0.3532503 ]\n",
      " [0.15600929 0.72786042 0.06116336 0.05496693]\n",
      " [0.1397064  0.66967009 0.08611403 0.10450948]\n",
      " ...\n",
      " [0.29278451 0.47931844 0.05808103 0.16981602]\n",
      " [0.60337228 0.2747074  0.0550837  0.06683662]\n",
      " [0.16470895 0.58320611 0.04261545 0.20946949]]\n"
     ]
    }
   ],
   "source": [
    "#%%writefile knock53.py\n",
    "y_pred = model.predict(x_valid)  # 検証データで予測\n",
    "print('各記事のカテゴリ(ラベル) : ', y_valid.values)\n",
    "print('各記事のカテゴリ予測 : ', y_pred)\n",
    "\n",
    "# predict_proba(X)各データがそれぞれのクラスに所属する確率を返す\n",
    "y_pred = model.predict_proba(x_valid)  \n",
    "print('カテゴリの予測確率:\\n', y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 54. 正解率の計算(3)\n",
    "\n",
    "52で学習したロジスティック回帰モデルの正解率を，学習データおよび評価データ上で計測せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (train) : 0.9322986954565902\n",
      "Accuracy (test) : 0.8421762589928058\n"
     ]
    }
   ],
   "source": [
    "#%%writefile knock54.py\n",
    "from sklearn.metrics import accuracy_score  # 正解率計算用のメソッド\n",
    "\n",
    "y_train_pred = model.predict(x_train)  #予測\n",
    "y_test_pred = model.predict(x_test)\n",
    "\n",
    "print(f'Accuracy (train) : {accuracy_score(y_train, y_train_pred)}')\n",
    "print(f'Accuracy (test) : {accuracy_score(y_test, y_test_pred)}')\n",
    "# Accuracyは正解してたらsum+=1とかしてデータ数で割れば出せる　面倒なのでメソッド使った"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 55. 混同行列の作成(3)\n",
    "\n",
    "52で学習したロジスティック回帰モデルの混同行列（confusion matrix）を，学習データおよび評価データ上で作成せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix (train)\n",
      ": [[1819   24    1    2]\n",
      " [   4 1758    0    2]\n",
      " [  41   77  208    2]\n",
      " [  79   68    1  360]]\n",
      "Confusion matrix (test)\n",
      " : [[1773  129    3   14]\n",
      " [  32 1707    1    6]\n",
      " [  72  121  103    2]\n",
      " [ 155  167    0  163]]\n"
     ]
    }
   ],
   "source": [
    "#%%writefile knock55.py\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# 対角成分の数だけ正解\n",
    "\n",
    "print(f'Confusion matrix (train)\\n: {confusion_matrix(y_train, y_train_pred)}')\n",
    "print(f'Confusion matrix (test)\\n : {confusion_matrix(y_test, y_test_pred)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 56. 適合率，再現率，F1スコアの計測(5 主に評価指標の概念)\n",
    "52で学習したロジスティック回帰モデルの適合率，再現率，F1スコアを，評価データ上で計測せよ．カテゴリごとに適合率，再現率，F1スコアを求め，カテゴリごとの性能をマイクロ平均（micro-average）とマクロ平均（macro-average）で統合せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TP, FP, FN, TN\n",
    "- TP(True Positive)  \n",
    "- FP(False Positive)\n",
    "- FN(False Negative)\n",
    "- TN(True Negative)\n",
    "\n",
    "Trueは予測正解、Falseは予測不正解\n",
    "Positiveは予測が正、Negativeは予測が負"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【カテゴリ順】['b' 'e' 'm' 't']\n",
      "適合率：[0.87253937 0.80367232 0.96261682 0.88108108]\n",
      "再現率 : [0.92391871 0.97766323 0.34563758 0.33608247]\n",
      "F1 : [0.89749431 0.88217054 0.50864198 0.48656716]\n",
      "\n",
      "【マクロ平均】\n",
      " 適合率：0.8799773974934771\n",
      "再現率 : 0.6458254990050528\n",
      "F1 : 0.6937184968406463\n",
      "\n",
      "【マイクロ平均】\n",
      " 適合率：0.8421762589928058\n",
      "再現率 : 0.8421762589928058\n",
      "F1 : 0.8421762589928058\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%writefile knock56.py\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "# precision_score()  適合率 TP / (TP + FP)\n",
    "# recall_score() 再現率 TP / (TP + FN)\n",
    "# f1_score()  F1スコア、適合率と再現率の調和平均\n",
    "\n",
    "def metrics(y_data, y_pred, ave=None):\n",
    "  precision_sco = precision_score(y_data, y_pred, average=ave)\n",
    "  recall_sco = recall_score(y_data, y_pred, average=ave)\n",
    "  f1_sco = f1_score(y_data, y_pred, average=ave)\n",
    "  form = \"適合率：{}\\n再現率 : {}\\nF1 : {}\\n\".format(precision_sco, recall_sco, f1_sco)\n",
    "  return form\n",
    "\n",
    "print(f\"【カテゴリ順】{model.classes_}\\n{metrics(y_test, y_test_pred)}\")\n",
    "print(\"【マクロ平均】\\n\", metrics(y_test, y_test_pred, \"macro\"))\n",
    "print(\"【マイクロ平均】\\n\", metrics(y_test, y_test_pred, \"micro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 57. 特徴量の重みの確認 (5, 主にargsort, model)\n",
    "52で学習したロジスティック回帰モデルの中で，重みの高い特徴量トップ10と，重みの低い特徴量トップ10を確認せよ．"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LogisticRegression.classes_ \n",
    "\n",
    "    分類したカテゴリのラベル\n",
    "\n",
    "- LogisticRegression.coef_\n",
    "\n",
    "    特徴量ごとの重みを取得\n",
    "\n",
    "- numpy.argsort()\n",
    "\n",
    "    特定の行(列)をソート、値ではなくインデックスのndarrayを返す\n",
    "\n",
    "    argsortはデフォルトで昇順の結果でしか受け取ることができない。\n",
    "\n",
    "        -> 降順で受け取る場合、引数のnumpy配列をマイナス倍\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>重みの高い特徴量トップ10（クラス名: b）</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>update</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>stocks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>as</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>euro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>on</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>china</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ecb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>fed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>dollar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   重みの高い特徴量トップ10（クラス名: b）\n",
       "1                      us\n",
       "2                  update\n",
       "3                  stocks\n",
       "4                      as\n",
       "5                    euro\n",
       "6                      on\n",
       "7                   china\n",
       "8                     ecb\n",
       "9                     fed\n",
       "10                 dollar"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%%writefile knock57.py\n",
    "import numpy as np\n",
    "\n",
    "features = x_train.columns.values  # 学習データの特徴量\n",
    "\n",
    "class_name, coef = model.classes_[0], model.coef_[0]\n",
    "index = [i for i in range(1, 11)]\n",
    "\n",
    "top_10 = features[np.argsort(-coef)[:10]]  # 降順\n",
    "worst_10 = features[np.argsort(coef)[:10]]\n",
    "\n",
    "df_top_10 = pd.DataFrame(top_10, columns=[f'重みの高い特徴量トップ10（クラス名: {class_name}）'], index = index)\n",
    "df_worst_10 = pd.DataFrame(worst_10, columns=[f'重みの低い特徴量トップ10（クラス名: {class_name}）'], index= index)\n",
    "\n",
    "'''\n",
    "for class_name, coef in zip(model.classes_, model.coef_):\n",
    "    index = [i for i in range(1, 11)]\n",
    "    top_10 = features[np.argsort(-coef)[:10]]\n",
    "    worst_10 = features[np.argsort(coef)[:10]]\n",
    "    df_top_10 = pd.DataFrame(top_10, columns=[f'重みの高い特徴量トップ10（クラス名: {class_name}）'], index = index)\n",
    "    df_worst_10 = pd.DataFrame(worst_10, columns=[f'重みの低い特徴量トップ10（クラス名: {class_name}）'], index= index)\n",
    "'''\n",
    "\n",
    "df_top_10.to_csv('./results/output_top10.csv')\n",
    "df_worst_10.to_csv('./results/output_worst10.csv')\n",
    "\n",
    "df_top_10     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 58. 正則化パラメータの変更(3)\n",
    "\n",
    "ロジスティック回帰モデルを学習するとき，正則化パラメータを調整することで，学習時の過学習（overfitting）の度合いを制御できる．異なる正則化パラメータでロジスティック回帰モデルを学習し，学習データ，検証データ，および評価データ上の正解率を求めよ．実験の結果は，正則化パラメータを横軸，正解率を縦軸としたグラフにまとめよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing knock58.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile knock58.py\n",
    "import matplotlib.pyplot as plt\n",
    "import japanize_matplotlib\n",
    "\n",
    "# cごとの正解率を格納(縦軸)\n",
    "accuracies_train = []\n",
    "accuracies_valid = []\n",
    "accuracies_test = []\n",
    "\n",
    "# cは正則化パラメータlambdaの逆数\n",
    "c_list = [0.001, 0.01, 0.1, 1, 10]  # np.linsapce(start, stop, 要素数)\n",
    "\n",
    "for c in c_list:\n",
    "    '''モデルの構築, フィッティング'''\n",
    "    model = LogisticRegression(C = c, random_state = 0)\n",
    "    model.fit(x_train, y_train)\n",
    "\n",
    "    '''予測'''\n",
    "    # 訓練データ\n",
    "    y_pred_train = model.predict(x_train) \n",
    "    accuracy_train = accuracy_score(y_pred_train, y_train)  \n",
    "    accuracies_train.append(accuracy_train)\n",
    "\n",
    "    #検証データ\n",
    "    y_pred_valid = model.predict(x_valid)\n",
    "    accuracy_valid = accuracy_score(y_pred_valid, y_valid)  \n",
    "    accuracies_valid.append(accuracy_valid) \n",
    "\n",
    "    #テストデータ\n",
    "    y_pred_test  = model.predict(x_test)\n",
    "    accuracy_test = accuracy_score(y_pred_test, y_test)  \n",
    "    accuracies_test.append(accuracy_test) \n",
    "\n",
    "    print(f'正則化パラメータ: {c}')\n",
    "    print(f'正解率(訓練データ): {accuracy_train}')\n",
    "    print(f'正解率(検証データ): {accuracy_valid}')\n",
    "    print(f'正解率(テストデータ): {accuracy_test}')\n",
    "    print('-'*40)\n",
    "\n",
    "plt.plot(c_list, accuracies_train, label = 'tarin', marker = 'o')\n",
    "plt.plot(c_list, accuracies_valid, label = 'valid', marker = 'o')\n",
    "plt.plot(c_list, accuracies_test, label = 'test', marker = 'o')\n",
    "plt.xlabel('正則化パラメータc')\n",
    "plt.ylabel('正解率')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 59. ハイパーパラメータの探索(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing knock59.py\n"
     ]
    }
   ],
   "source": [
    "#%%writefile knock59.py\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'C':[0.01, 0.005, 10]}\n",
    "\n",
    "gs_model = GridSearchCV(LogisticRegression(\n",
    "    max_iter=1500),params, cv=5, verbose=1)\n",
    "gs_model.fit(x_train, y_train)\n",
    "\n",
    "best_gs_model = gs_model.best_estimator_\n",
    "print(\"\\ntrain_score: {:.2%}\".format(best_gs_model.score(x_train, y_train)))\n",
    "print(\"valid_score: {:.2%}\".format(best_gs_model.score(x_valid, y_valid)))\n",
    "print(\"test_score: {:.2%}\".format(best_gs_model.score(x_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
